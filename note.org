#+title:Note
#+startup: num latexpreview indent overview
#+options: tex:dvisvgm toc:2
#+archive: ~/org/archive.org::* head
#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{amssymb}
#+latex_header: \usepackage{tikz}\usetikzlibrary{cd}
#+latex_header: \usepackage{bussproofs}
#+latex_header: \usepackage{prftree}
#+latex_header: \newcommand{\semicolon}{\mathbin{;}}

* little logic

** chapter 3: LK

*** part 1: starting up, rules and more

**** the problem with Modus Ponens

There are actually problems with Modus Ponens. Modus Ponens is actually very like cut rule.  Why?  Because in the view of proof finding, we start from nothing: if we want to proof \(B\), then probably we have to image such \(A\),
\[
A \to B,
\]
holds.  "We have to image such \(A\)".  And this is the problem.

**** generalization

From \(A[x]\) to \(\forall xA[x]\). And this is called generalization.  What is wrong with it?

**** problems with hilbert's formalism and system

Anyway, the two rules make the Hilbert's logic system non-usable, because it has no good structural property.
And thus even the slightest automated deduction is impossible.

It is said that hilbert's system is a garbage, in the sense of equivalence, it does no more than other system but hinder our understanding.

**** deduction theorem

if \(B\) is provable in system \(\mathcal F + A\), with \(A\) closed, then \(A \Rightarrow B\) is provable in \(\mathcal F\).

The proof is not provided somehow.

**** seqeunt and hypothesis

I don't really know the intuition behind the idea that LK is better than Hilbert's, but it seems that it is the introduction of hypothesis.

Anyway, the problem of "image a premise \(A\)" can be solved via
1. LK
2. cut-elimination.

**** sequents

A sequent is expression of \(\Gamma \vdash \Delta\), where \(\Gamma\) and \(\Delta\) are all finite sequences of formulas.

**** the notion of context

We use more notion of context, in order to make a difference from other stuff.  The reason we are doing this?  I don't know.

But anyway, \(\Gamma \vdash A, \Delta\) for example, the context of it is \(\Gamma, \Delta\), one can write as \(\Gamma \vdash {-}, \Delta\)

**** the intuitive "semantic" of LK

\[
\Gamma \vdash \Delta
\]
for this expression, it means "if all the formulas in \(\Gamma\) are correct", then "one of the formula in \(\Delta\) is correct". By the way, the symbol \(\vdash\) is called "turnsile".

The symbols used in LK are:
1. left commas: means «and»
2. right commas: «or»
3. turnsiles: «implies»

**** special sequent in LK

\begin{gather}
A \vdash\\
\vdash \\
\vdash A
\end{gather}
the first one means \(A\) leads to nothing, which means \(A\) is contradictory, and 2 means an expression like \(v \to f\), which is bad, 3 means \(A\) is true without hypothesis, which is to say \(A\) is tautology.

The proof of second is important, given by Gentzen, saying that LK is *consistent*.

**** negate of \(A\)

I think we have this \(A \vdash\) then \(\vdash \neg A\). The precise definition is not here tho.

**** rules for LK

The content of LK is the rules. The rules are divided into three groups, we can actually only introduce those important ones, because introducing those about the connectives is always not necessary.

***** the identity group

There are two rules in this group they are called identity and cut rule
\[
\frac{}{A \vdash A}\ (\mathrm{id}) \quad \quad \quad
\frac{\Gamma \vdash A, \Delta \quad \Lambda, A \vdash \Pi}{\Gamma, \Lambda\vdash \Delta, \Pi}\ (\mathrm{cut})
\]
There are special cases for cut rules, because of the context. They are 1. modus ponens 2. transitivity
\[
\frac{\vdash A\quad A \vdash B}{\vdash B}\ (\mathrm{modus ponens})
\]
\[
\frac{A\vdash B\quad B\vdash C}{A\vdash C} \ (\mathrm{transitivity})
\]
cut rule is most important one.

***** Hauptsatz of Gentzen

Hauptsatz is called the cut-elimination. So basically it says that cut rule is redundant. Hauptsatz means big in Germany. So it can be called the main theorem.

***** structural group

\begin{align}
&&\frac{\Gamma\vdash\Delta}{\sigma(\Gamma)\vdash\tau(\Delta)}\tag{exchange}\\
&\frac{\Gamma \vdash \Delta}{\Gamma, A \vdash \Delta}
&&\frac{\Gamma \vdash \Delta}{\Gamma \vdash A, \Delta}\tag{weak} \\
&{\Gamma \vdash A, A, \Delta \over \Gamma \vdash A, \Delta}
&&{\Gamma, A, A \vdash \Delta \over \Gamma, A \vdash \Delta}\tag{contract}
\end{align}

***** contraction and weakening

The rules seem intuitive at first, but there are spaces for we to discuss.

See page 61.

***** logical group

The logical group is about those connectives like \(\forall\) \(\exists\) and so on. Here is the deal

\[
\frac{\Gamma\vdash A, \Delta}{\Gamma \vdash \forall x A, \Delta}(\vdash \forall) \quad \quad
\frac{\Gamma, A[t/x]\vdash \Delta}{\Gamma, \forall x A\vdash \Delta}
(\forall\vdash)
\]

there are important constrains for the rules:

1. for \(\forall \vdash\) and \(\vdash \exists\), \(t\) should not use \(x\).
2. \(\vdash \forall\) and \(\exists\vdash\), as \(x\) is introduced, there should be no appearance of \(x\) in context of \(A\), that is \(\Gamma \vdash {-}, \Delta\).

And of course, there are some other rules for \(\Rightarrow\)

\[
\frac{\Gamma, A \vdash B, \Delta}{\Gamma \vdash A\Rightarrow B, \Delta}\quad\quad
\frac{\Gamma \vdash A, \Delta\quad\Lambda, B \vdash \Pi}{\Gamma,\Lambda, A\Rightarrow B\vdash\Delta,\Pi}
\]

You may need some time to process this one.

\[
\frac{\Gamma \vdash A[t], \Delta}{\Gamma \vdash \exists xA[x], \Delta}\ (\vdash \exists)
\quad\quad
\frac{\Gamma, A\vdash \Delta}{\Gamma, \exists x A\vdash \Delta}\ (\exists\vdash)
\]

here \(A[t]\) is the same as \(A[t/x]\), the latter is more precise. Why are there two symbols because those are symbols used separately in book « proof theory and logic complexity » and « The Blind Spot ».

***** eigenvariable

The notion eigenvariable is not unambiguous.

Since we introduce a variable \(x\) in \(\vdash \forall\), we don't want the premise has \(x\) in it. Thus the premise is written as \(\Gamma \vdash A[v/x],\Delta\).

The thing is to avoid the notion of "bound variable".

***** laxist notation of eigenvariable

Here we have the laxist notation when confronting the variable \(x\) in the rule \(\vdash \forall\). This is because "bound variable" is sometimes confusing.

We say that when introducing \(x\), the premise has no occurrence of \(x\) in it. \(A\) becomes \(A[v/x]\) using a placeholder variable \(v\) that does not appear anywhere.

Significant details really.

**** right hand calculus of LK

The symmetry of LK makes it possible to decrease the size of rules significantly.

\[\begin{aligned}
&\frac{}{\vdash \neg A, A}\ (\mathrm{id})
& %
& \frac{\vdash \Delta, A\quad \vdash \neg A, \Pi}{\vdash \Delta, \Pi} (\mathrm{cut})
\\
\\
& \frac{\vdash \Delta}{\vdash \tau (\Delta)}\ (X)
& \frac{\vdash \Delta}{\vdash A, \Delta}\ (W)\quad\quad
& \frac{\vdash A,A,\Delta}{\vdash A, \Delta}\ (\mathrm{con}) %
\\
\\
& \frac{\vdash A, \Delta}{\vdash A \lor B, \Delta}\ (\vdash \lor)
& %
& \frac{\vdash A, \Delta\quad \vdash B, \Delta}{\vdash A\land B, \Delta}\ (\vdash \land)
\\
\\
& \frac{\vdash A, \Delta}{\vdash \forall x A, \Delta}
&
& \frac{\vdash A[t/x], \Delta}{\vdash \exists x A, \Delta}
\end{aligned}\]

*** part 1.5: some exercises

**** some basic

Check for [[*logical group]]

***** \(A \to (B \to A)\)

\begin{prooftree}
\AxiomC{\(A\vdash A\)}
\UnaryInfC{\(A, B\vdash A\)}
\UnaryInfC{\(A\vdash B \to A\)}
\UnaryInfC{\(\vdash A\to (B \to A)\)}
\end{prooftree}

***** \((A\to (B \to C)) \to (A\to B) \to A \to C\)

\begin{prooftree}
\AxiomC{\(A \vdash A\)}
   \AxiomC{\(A \vdash A\)}  \AxiomC{\(B \vdash B\)}
   \BinaryInfC{\(A \to B, A \vdash B\)}
                                    \AxiomC{\(C\vdash C\)}
                    \BinaryInfC{\(B \to C, A\to B, A \vdash C\)}
\BinaryInfC{\(A\to (B \to C), A, A\to B \vdash C\)}
\end{prooftree}

***** \(A \to A\lor B\)

apparent

***** \((\neg A\to\neg B) \to (\neg A\to B)\to A\)

I don't want to prove it.

***** \(\forall xA[x] \to A[t/x]\)

\begin{prooftree}
\AxiomC{\(A[t]\vdash A[t]\)}
\UnaryInfC{\(\forall xA[x]\vdash A[t]\)}
\UnaryInfC{\(\vdash \forall x A[x]\to A[t]\)}
\end{prooftree}

***** \(A[t/x]\to \exists xA[x]\)

\begin{prooftree}
\AxiomC{\(A[t]\vdash A[t]\)}
\UnaryInfC{\(A[t]\vdash \exists xA[x]\)}
\UnaryInfC{\(\vdash A[t]\to \exists xA[x]\)}
\end{prooftree}

**** prove that it is legit to restrict the id axiom on atom formula

that is, if axioms \(\dfrac{}{A \vdash A}\) holds only when \(A\) is atomic, the system is still legit.

Proof is simple. We use induction on formula \(A\)

***** if \(A\) is of form \(B \to C\)

\begin{prooftree}
\AxiomC{\(C\vdash C\)}
\AxiomC{\(B\vdash B\)}
\BinaryInfC{\(B\to C, B \vdash C\)}
\UnaryInfC{\(B\to C\vdash B\to C\)}
\end{prooftree}

***** if \(A\) is of form \(\neg B\)

\begin{prooftree}
\AxiomC{\(B \vdash B\)}
\UnaryInfC{\(\vdash B, \neg B\)}
\UnaryInfC{\(\neg B \vdash \neg B\)}
\end{prooftree}

***** if \(A\) is of form \(B \lor C\)

\begin{prooftree}
\AxiomC{\(B\vdash B\)}
\UnaryInfC{\(B \vdash B \lor C\)}
  \AxiomC{\(C\vdash C\)}
  \UnaryInfC{\(C\vdash B \lor C\)}
\BinaryInfC{\(B\lor C\vdash B \lor C\)}
\end{prooftree}

***** if \(A\) is of form \(B\land C\)

\begin{prooftree}
\AxiomC{\(B\vdash B\)}
\UnaryInfC{\(B\land C \vdash B\)}
  \AxiomC{\(C\vdash C\)}
  \UnaryInfC{\(B\land C\vdash C\)}
\BinaryInfC{\(B\land C\vdash B\land C\)}
\end{prooftree}

***** if \(A\) is of form \(\forall x B\)

***** if \(A\) is of form \(\exists xB\)

**** without structural rules, set-based sequent calculus

Structural rules are exchange, weakening and contraction. See [[*structural group]]

If we exclude those rules, we have set-based sequent calculus, where we treat \(\Gamma\) as set! This can be viewed as a variation of sequent calculus.

How can I prove this?

**** signature

Here is the def of signature of an occurrence of formula \(P\) in \(A\). \(\pi\) is an occurrence of \(P\).

1. if \(A\) is atomic, \(P\) is pos.
2. if \(\pi\) is pos in \(A\), then it is so in \(A\lor B\), \(A\land B\), \(B\to A\), \(\forall xA\), \(\exists xA\).
3. if \(\pi\) is pos in \(A\), then it is neg in \(A \to B\) and \(\neg A\).

We need also def signature of \(\pi\) of \(P\) in a sequent \(\Gamma \vdash \Delta\). as one can imagine, if \(\pi\) is pos in \(\Gamma\), then \(\pi\) is neg in sequent. if \(\pi\) is pos in \(\Delta\), then \(\pi\) is still pos in the sequent.

Prove that cut-free proofs preserve the signature.

Proof. Obvious.

**** \(A\sb{1}\dots A\sb{n}\vdash B\sb{1}\dots B\sb{m}\) leads to \(A\sb{1}\land,\dots,\land,A\sb{n}\vdash B\sb{1}\lor,\dots,\lor B\sb{m}\)

The proof is actually obvious. I think.

*** part 2: cut free and subformulas properties

**** Hauptsatz of Gentzen

The theorem can be stated as follow:

Every theorem in *LK* or *LJ* has a cut-free proof.

**** the proof given by Gentzen

The proof of Hauptsatz is not worth trusting. It is tedious and hard to follow. The idea is that there are many key cases he can list, where the cut can be push upward. In this process of pushing and potentially expanding the proof size, we can prove that the process is decisive.

And thus we can conclude that we can construct a cut-free proof.

**** subformula properties

***** subformulas

if \(A\) is atomic, then the subformula of \(A\) is \(A\) itself and nothing else.

if \(A\) is of form \(B * C\), then the subformula of \(A\) is \(A\) itself and subformulas of \(B\) and \(C\).

if \(A\) is of form \(\forall x B\) or \(\exists x B\), then the subformula of \(A\) is \(A\) itself and the subformulas of \(B[t]\) for some term \(t\).

***** the theorem

The cut-free proofs of a sequent \(\Gamma \vdash \Delta\) are consisted of sequents made out of the subformulas in \(\Gamma\) and \(\Delta\).

It is obvious that if there is quantifiers (\(\forall\), \(\exists\)) in the \(\Gamma\vdash \Delta\), there are infinite number of subformulas. The proof searching seems to be indecisive. However, ...

*** part 3: more

**** second order

***** the notation

We use \(X\) as variable for predicate. But we have some different notations.

- If \(X\) is arity 1, then it is supposed to be \(X(t)\) for term \(t\). But we wrote \(t \in X\), just like \(X\) is a set.
- If \(A\) is a formula, and we use it to create a predicate \(\{x; A\}\) (arity 1). Just like a set, whose elements are all \(x\) that satisfied \(A\).

I don't really know why we do this. I think this implies an relation with another expression of second order predicate logic. That is we treat \(X\) like a set. For example, we say that \(\mathbf{N}\) is the set of natural number, and thus we have:
\[
x \in \mathbf{N} := \mathsf{N}(x)
\]
where we say \(\mathsf{N}\) is a 1 arity predicate, and \(\mathsf{N}(x)\) means «\(x\) is natural number».

***** rules in second order

\[
\frac{\Gamma\vdash A,\Delta}{\Gamma\vdash\forall XA,\Delta}\ (\vdash \forall\sb{2})\quad\quad
\frac{\Gamma, A[T/X]\vdash\Delta}{\Gamma, \forall XA\vdash\Delta}\ %
(\forall\sb{2}\vdash)
\]

\[
\frac{\Gamma\vdash A[T/X],\Delta}{\Gamma\vdash \exists XA,\Delta}
\ (\vdash \exists\sb{2})
\quad\quad
\frac{\Gamma, A\vdash\Delta}{\Gamma,\exists A\vdash\Delta}
\ (\exists\sb{2}\vdash)
\]

where \(T\) should have the same arity of \(X\), in \(A[T/X]\).

**** \(\mathrm{PA}\sb{2}\) second order peano arithmetic

We write a Dedekind integers for example:
\[
N := \{x ; \forall X(0 \in X \land \forall z(z \in X \Rightarrow Sz \in X) \Rightarrow x \in X) \}
\]
How to understand this one, we treat \(X\) as a simple propostion for example \(A\). \(x \in X\) is \(A\), and \(z\in X\) is \(A[z/x]\).
Then we may have
\[
x \in N \vdash A[0] \land \forall z(A[z/x]\Rightarrow A[Sz / x]) \Rightarrow A[x/x]
\]

**** comprehension schema in second order

This is something that I don't understand, about the terms that Girard was using.

We start with \(\vdash \forall x (A \Leftrightarrow A)\), how can we get
\[
\vdash \exists X \forall x ( x \in X \Leftrightarrow A)
\]
What he said is we use a rule \(\vdash \exists\sb{2}\), on the abstract term \(T := \{x ; A\}\)

Okay, if \(T\) is \(\{x; A\}\) then \(x\in T\) means actually \(A\). And here we should look at \(\forall x  A\). We assume that there is a \(x\) in \(A\). Then \(\forall x A \equiv \forall x (x \in T)\) seems very legit.
The overall process is \(\vdash \forall x (A \Leftrightarrow A)\) rewrite as \(\vdash\forall x (x \in T \Leftrightarrow A)\), and then we apply \(\vdash \exists\sb{2}\), with result \(\vdash \exists X \forall x(x \in X \Leftrightarrow A)\).

Okay, what the fuck is this schema used for?

**** LJ is a subsystem of LK

LJ, where all the sequent \(\Gamma\vdash\Delta\) where \(\Delta\) consists of at most one formula, is a subsystem of LK.

LJ actually enjoys Hauptsatz and subformula properties.

**** decisiveness of LJ

The introduction of LJ is owing to an obvious reason, the LJ is relatively less expressive (where law of middle excluded can be derived), but LJ is fucking decisive.

** chapter 4: LJ

*** intuitionistic sequent

A intuitionistic sequent is of form \(\Gamma \vdash A\), where \(A\) is a formula.

That is LJ is a subset of LK, with the restriction of there is one and only one formula one the right hand side.

*** 0 in the LJ

The right hand side can not be empty but can be \(0\). There is rule of the introduction \(0\):

\[
\frac{}{\Gamma, 0 \vdash A}\ (0\vdash)
\]

*** rules

id

\[
\frac{}{A\vdash A}
\quad\quad
\frac{\Gamma \vdash A \quad \Lambda, A \vdash B}{\Gamma, \Lambda \vdash B}
\]

structural

\[
\frac{\Gamma\vdash A}{\sigma(\Gamma)\vdash A}
\]

\[
\frac{\Gamma\vdash B}{\Gamma, A\vdash B}
\]

\[
\frac{\Gamma, A, A\vdash B}{\Gamma, A\vdash B}
\]
logical group is 略

\[
\frac{\Gamma \vdash A}{\Gamma \vdash \forall x A}\ (\vdash \forall)
\quad\quad
\frac{\Gamma, A\vdash B}{\Gamma, \forall xA\vdash B}\ (\forall\vdash)
\]

\[
\frac{\Gamma \vdash A[t/x]}{\Gamma\vdash \exists x A}\ (\vdash \exists)
\quad\quad
\frac{\Gamma, A\vdash B}{\Gamma, \exists x A\vdash B}\ (\exists\vdash)
\]
\[
\frac{}{\Gamma, \mathbf{0}\vdash A}
\]

*** Gödel's translation

**** use \(A \to B\) induce \(\neg B \to \neg A\)

This is trivial in LK, but not in LJ.

\begin{prooftree}
\AxiomC{\(A \vdash B\)}
   \AxiomC{ }
   \UnaryInfC{\(\textbf{0} \vdash \textbf{0}\)}
\BinaryInfC{\(A, \neg B \vdash \textbf{0}\)}
\UnaryInfC{\(\neg B \vdash \neg A\)}
\end{prooftree}

**** the intuition of «hole» in LJ

in the introduction of \(\neg B \vdash \neg A\), we notice that when migrating \(B\) to the left side, we leave a «hole» one right side that is \(\textbf{0}\). And respectively, when migrating \(A\) to the right side, it occupies the «hole».

**** double migration to prove \(A \to \neg\neg A\)

A migration of a formula \(A\) will add a \(\neg\) to it. So a double migration on right hand side of \(A \vdash A\) will naturally lead to \(A \vdash \neg \neg A\).

\begin{prooftree}
\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\end{prooftree}

in the classic logic we can prove \(\neg\neg A \vdash A\), this is because we can do a double migration on the left hand side of \(A\vdash A\).

Although it is not possible to prove \(\neg\neg A \vdash A\), \(\neg\neg\neg A \Leftrightarrow \neg A\) is provable.

**** the Gödel's theorem

\(A\) is classically provable (provable in LK) iff \(A^{g}\) is intuitionistically provable (provable in LJ).

where we get \(A^{g}\) by adding \(\neg\neg\) to the front of every atomic formulas, quantifiers and connectives:

- \(A^{g} := \neg\neg A\),
- \((A \land B)^{g} := \neg\neg(A^{g} \land B^{g})\)
- \((\forall x A)^{g} := \neg\neg \forall x A^{g}\)

Proof is simple. And a sort of completeness is achieved in LJ.

*** Hauptsatz in LJ

**** the decidability of LJ

there is a remark that I don't understand in page 74.

**** intuitionistic existence and disjunction

LJ has a remarkable property:

\textsc{Theorem} if \(\vdash A\lor B\) is provable, then either \(\vdash A\) or \(\vdash B\) is provable.

\textsc{Theorem} if \(\vdash \exists xA\) is provable, then there is an appropriate \(t\), such that \(\vdash A[t/x]\) is provable.

**** the myth around the fine property of LJ

there is some misunderstanding about the fine property above: if we can \(\vdash A\) is provable, why bother to prove \(A\lor B\)?

From \(A \lor B\) to either \(A\) or \(B\), this is explicit only if you use cut-free proof system, but a cut-free proof is so tedious and something unnecessary. We may implicitly prove (with cut) \(A\lor B\) without knowing which one is true. We know only after cut-elimination!

Keep in mind that the propery is the corollary of Hauptsatz.

**** explicitable logic

LJ is not an explicit logic (it is if cut-free), but we can say it is explicitable.

*** NJ

**** conclusion and hypothesis

The begin of structure like
\[
\prfsummary{\Gamma}{A}
\]
The structure needs more clarification. It is more like a tree but upside down. The root is \(A\), and there are multiple leaves. The leaves are called hypothesis. The \(\Gamma\) here is the set of leaves in the proof tree.

\[
A
\]

is the proof of \(A\vdash A\), where the hypothesis \(A\) and the conclusion \(A\) itself.

**** introduction of \(\Rightarrow\)

\[
\prftree[r]{$(I \Rightarrow)$}
{\prfsummary{[A]}{B}}
{A\Rightarrow B}
\]

The introduction of \(\Rightarrow\) introduce \([A]\) means the \(A\) is marked as discarded. So one of the subproof (whose conclusion is \(A\)) is marked as used.

Let us say the proof of \(A\) use hypothesis \(\Gamma_{1}\), and the proof of \(B\) use hypothesis \(\Gamma_{1} * \Gamma_{2}\), here the proof of \(A\Rightarrow B\) use hypothesis \(\Gamma_{2}\), because the proof of \(A\) is abandoned. This becomes clear when you translate NJ to LJ.

** chapter 5: function interpretation

*** TODO

** chapter 6: system F
** chapter 7: CCC
*** pole and polar
**** def

Given a binary operation \(+\colon A \times B \to C\), and a subset of \(C\), namely \(P\) (the pole), we can give the polar set \(X^{p}\) with the respect to a subset of \(A\), namely \(X\):

\[
X^{p} := \{y \in B \semicolon \forall x \in X, x + y \in P\}
\]

**** some properties

We write \(\langle a, b\rangle\) as \(a * b\). Don't want to bother to type langle and rangle.

***** 1. \(X \subset X^{pp}\)

Proof. if \(x \in X\) we need to prove that \(\forall y \in X^{p}\), \(x * y \in P\).

Let us look at \(y \in X^{p}\), \(\forall x' \in X, x' * y \in P\). So of course \(x * y \in P\) is valid.

***** 2. \(X^{pp}\) is the smallest polar set that containing \(X\).

Proof. we need to prove that if \(Z\) is polar set that containing \(X\), then \(Z\) also containing a polar set \(X^{pp}\).

Let us say that \(Z = (Z^{-p})^{p}\), here \(Z^{-p}\subset B\). All we have is that if \(x \in X\) then \(x \in Z\), which is \(X \subset Z\). We need to prove that if \(x \in X ^{pp}\) then \(x \in Z\). Or we can prove that if \(x \notin Z\) then \(x \notin X^{pp}\). Let us go with the latter:

If \(x \notin Z\), then \(\exists z \in Z^{-p}\), that \(x * z \notin P\).

How can we prove that \(x \notin X ^{pp}\)? Let us say that if \(x \in X^{pp}\), then therefore \(\forall y \in X^{p}\), \(x * y \in P\). So we know that \(z\) in \(Z^{-p}\) is =not= in \(X^{p}\).

Consequently, \(z\) suited that there is a \(\exists x' \in X\), \(x' * z \notin P\). However, at the same time, \(z \in Z^{-p}\), then for  \(\forall x'' \in Z\) also include those \(\forall x'' \in X\), that \(x'' * z \in P\). A contradiction is derived from \(x \in X^{pp}\).

The formula is that if \(x \notin Z\) and if \(x\in X^{pp}\) there would be a contradiction.

So \(x\in X^{pp}\Rightarrow x\in Z\) which implies \(X^{pp}\subset Z\). And because we don't have pre-set condition of \(Z\), \(\forall Z \supset X, Z \supset X^{pp}\)

The smallest polar set is \(\bigcap Z = X^{pp}\). The equation holds because \(X\) is finite, the number of \(Z\) is limited.

***** 3. \(X^{p} = X^{ppp}\)

Proof. \(X\subset X^{pp}\) is enough to prove \(X^{p}\subset X^{ppp}\). And then we need \(X^{ppp}\subset X^{p}\).

We say like this \(X\) is a polar set, then \(X^{pp} \subset X\).

Let us say \(x \in X^{pp}\) we need \(x \in X\). Or we can say \(x \notin X\), then we need \(x\notin X^{pp}\).

\(x\notin X\), then \(\exists y \in X^{-p}, x * y \notin P\), thus (because \(X^{-p} \subset X^{p}\)) \(\exists y \in X^{p}, x * y \notin P\), which \(x \notin X^{pp}\).

**** connection

- \(A \Rightarrow \neg\neg A\)
- \(A \Rightarrow \neg B \vdash \neg\neg A \Rightarrow \neg B\)
- \(\neg A \Leftrightarrow \neg\neg\neg A\)


1. \(A \vdash \neg\neg A\)
\begin{prooftree}
\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\end{prooftree}
2. \(A \Rightarrow \neg B \vdash \neg\neg A \Rightarrow \neg B\)
\begin{prooftree}
\AxiomC{\(A \vdash A\)}
  \AxiomC{\(B \vdash B\)}  \AxiomC{\(0 \vdash 0\)}
  \BinaryInfC{\(\neg B, B \vdash 0\)}
\BinaryInfC{\(A\Rightarrow \neg B, A, B \vdash 0\)}
\doubleLine
\UnaryInfC{\(A\Rightarrow \neg B, \neg\neg A, B \vdash 0\)}
\UnaryInfC{\(A\Rightarrow \neg B, \neg\neg A \vdash \neg B\)}
\UnaryInfC{\(A\Rightarrow \neg B \vdash \neg\neg A\Rightarrow\neg B\)}
\end{prooftree}
3. \(\vdash \neg A \Leftrightarrow \neg\neg\neg A\)
\begin{prooftree}
  \AxiomC{ }
  \UnaryInfC{\(\neg A\vdash\neg A\)}
    \AxiomC{ }
    \UnaryInfC{\(0 \vdash 0\)}
  \BinaryInfC{\(\neg A, \neg \neg A \vdash 0\)}
  \UnaryInfC{\(\neg A\vdash \neg \neg\neg A\)}
  \UnaryInfC{\(\vdash \neg A \Rightarrow \neg\neg\neg A\)}

\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\doubleLine
\UnaryInfC{\(A, \neg\neg\neg A \vdash 0\)}
\UnaryInfC{\(\neg\neg\neg A\vdash \neg A\)}
\UnaryInfC{\(\vdash \neg\neg\neg A \Rightarrow \neg A\)}

\BinaryInfC{\(\vdash \neg A \Leftrightarrow \neg \neg \neg A\)}
\end{prooftree}

**** translation!

- \(A\) is \(X\)
- \(\neg A\) is \(X^{p}\)
- \(0\) is \(P\)
- \(\vdash\) is "we can find a way to use the left hand side variable to get one right hand side variable"

Examples of translation

- \(A \vdash A\) to \(x \in X \vdash x' \in X\)

We can find a way to use \(x \in X\) to get a \(x' \in X\): we just use identity function.

- \(A, \neg A \vdash 0\) to \(x \in X, y \in X^{p} \vdash c \in P\)

We can find a way to use \(x\) and \(y\) to get a \(c \in P\): we just use \(x * y\)

- \(\neg A:= A \Rightarrow 0\) to \(X^{p}\) defined as set of all function that maps \(X\) to \(P\)

The element \(f_{y}\) in \(X^{p}\) is a way to map \(x\in X\) to \(P\), by

\[
f_{y}\colon X \to P, f_{y}(x):= x * y
\]
*** TODO three layers

what the hell is this?

**** first layer of truth
**** second layer of algebric semantic or modeling
**** category layer
*** category
**** Ł
*** CC
**** categories

\(\mathsf{C}\) is a cat.

- Obj: \(\text{Obj}_{\mathsf C}\) or \(|\mathbf{C}|\) (we prefer the former)
- Hom: \(\text{Hom}_{\mathsf C}(A, B)\) or \(\mathbf{C} (A, B)\)
- composition: \(f \in \text{Hom}_{\mathsf C}(A, B)\) and \(g\in \text{Hom}_{\mathsf C}(B ,C)\), then \(g \cdot f\) in \(\text{Hom}_{\mathsf C}(A, C)\).
- associativity \(f (g h) = (fg)h\)
- id: there is always \(\mathrm{id}\) in \(\text{Hom}_{\mathsf C}(A,A)\).

**** Functor

A functor \(F\) from \(\mathsf C\) to \(\mathsf D\) is two collections of mapping

- From \(\text{Obj}_{\mathsf C}\) to \(\text{Obj}_{\mathsf D}\)
  - satisfy that \(\text{id}\) is mapped to \(\text{id}\) in \(\mathsf D\)

- From \(\text{Hom}_{\mathsf C}(A , B)\) to \(\text{Hom}_{\mathsf D}(FA, FB)\)
  - satisfy that \(F (g h) = F g F h\)

Also the diagram commutes:

\[\begin{tikzcd}
A \arrow[r, "f"]
  \arrow[d, "F"]   & B \arrow[d, "F"] \\
FA\arrow[r, "Ff"]  & FB
\end{tikzcd}\]

**** natural transformation

Functors naturally forms a category where \(\text{Obj}\) are all the functors, and the morphisms are called natural transformation.

natural transformation is a transformation between functors. Here a natural transformation \(\theta\) from \(F\) \(G\).

For every \(\text{Obj}\) in \(\mathsf C\), say \(A\), there is morphism \(T(A)\) in \(\text{Hom}_{\mathsf D}(FA, GA)\), that makes the diagram commutes:

\begin{displaymath}
\begin{tikzcd}
FA \arrow[r, "F(f)"]
   \arrow[d, "T(A)"] & F B \arrow[d, "T(B)"]\\
GA \arrow[r, "G(f)"] & GB
\end{tikzcd}
\end{displaymath}

**** cartesian product

First we need the definition of cartesian product. Here we use universal properties.

A cartesian product \(A\times B\) is a terminal object in followingly constructed categories:

First. Obj in this category is defined as an object \(C\) in \(\mathsf C\), and a pair of morphisms in \(\text{Hom}_{\mathsf C}(C, A)\) and \(\text{Hom}_{\mathsf C}(C, B)\):

\begin{displaymath}
\begin{tikzcd}
C \arrow[r, "f_{1}"]
  \arrow[d, "f_{2}"] & A\\
B
\end{tikzcd}
\end{displaymath}

Second. Morphism from \(C\) (with \(f_{1}\) and \(f_{2}\)) to \(D\) (with \(g_{1}\) and \(g_{2}\)) is defined as a morphism \(h\) from \(\text{Hom}_{\mathsf C}(C,D)\), such that the diagram commutes:

\begin{displaymath}
\begin{tikzcd}
C \arrow[rrd, bend left, "f_{1}"]
  \arrow[rdd, bend right,"f_{2}"]
  \arrow[rd, "h"]                 & & \\
                                  & D \arrow[r, "g_{1}"]
                                      \arrow[d, "g_{2}"] & A\\
                                  & B
\end{tikzcd}
\end{displaymath}
Okay we can not define the cartesian product in the normal sense:

 we can prove that \(A\times B\) (in the sense of set theory) together with \(\pi_{1}\) and \(\pi_{2}\) are the terminal object in the category above, that is for every obj \(C\), there is unique Hom \(h\) from \(C\) to \(A\times B\).

**** cartesian and product

In the sense of set theory, product is cartesian product. However, not every category is \(\mathsf{Set}\), so cartesian product as a concept in set theory does not apply in other thing.

Product is sometimes called direct product. There is also direct sum. And there is also tensor product. They are all different things. One can check for sometime articles. In physicist call tensor product as «direct product», absolutely brain dead behavior.

**** cartesian category

A cartesian category is such category where the terminal obj described above has been given to us: for every pair of obj \(A, B\), there is \(A\times B\) in the category.

**** before diving into CCC

Why we need cartesian category. Because we treat the conjuction of formula \(A \land B\) as \(A \times B\) in the sense of set theory.

We need a category where \(A\) \(B\) are "formula", \(A \land B\) should also be "formula".
*** CCC

**** before diving into CCC

so in the section above, we need CC for such rule: \(A\), \(B\) in \(\text{Obj}_{\mathsf C}\) then \(A\times B\) in \(\text{Obj}_{\mathsf C}\).

Here we have another one, we call exponential: \(A\), \(B\) in \(\text{Obj}_{\mathsf C}\), so \(A\Rightarrow B\) is in \(\text{Obj}_{\mathsf C}\), however, using the notation from set theory, \(A\Rightarrow B\) is written as \(B^{A}\).

**** exponential

We defined a cat:
Obj: diagram:

\begin{displaymath}
\begin{tikzcd}
C\times A \arrow[d, "f"]\\
B
\end{tikzcd}
\end{displaymath}

Hom: commutative diagram, where \(\lambda(f)\) is important:

\begin{displaymath}
\begin{tikzcd}
C\times A \arrow[rr, "\lambda(f)\times \mathrm{id}"]
          \arrow[rd, "f"]    & & D \times A
                                   \arrow[ld, "g"] \\
                             & B
\end{tikzcd}
\end{displaymath}

As you can guess \(B^{A}\) together with (evaluation) \(\epsilon\) is the terminal (to be exact, the terminal is noted as \(B^{A}\) and \(\epsilon\) if they exist).
*** examples of CCC

1. \(\mathsf {Set}\). Duhhh, obviously.
2. Scott Domain. We can use sequent calculus to define Scott domain instead of topological spaces.
3. Scott Domain is crucial for understanding coherent space in the future section.

*** scott domain described using logic

A scott domain is a pair \((X, \mathcal F)\), where \(X\) is a set, \(\mathcal F\) is a set of axioms made of \(x _{1}, \dots x_{n} \vdash x\) and something like \(x_{1}\dots x_{n}\vdash\) (notice this one is empty). Also these axioms are consistent, that is with the logical rules, structural rules and cut rule, one can not prove \(\vdash\).

A =coherent= subset of \(X\) is such subset \(A\), that \(\mathcal F \cup \{ \vdash x\semicolon x \in A\}\) is consistent.

A =saturated= subset is such =coherent= subset \(A\), that can not be «expanded», that is the corresponding axioms \(\mathcal F \cup \{ \vdash x \semicolon x \in A\}\) can not prove a \(\vdash y\) where \(y\) is outside of \(A\).

As a result, for every coherent set \(A\), there can be a «closure», noted as \(\bar A\). And here we use a new notation \(A \sqsubset_{\mathcal F} X\) which means \(A\) is a saturated subset of \((X,\mathcal F)\).

A morphism \(\varphi\) from \((X, \mathcal F)\) to \((Y, \mathcal G)\) suits that:

1. \(A \sqsubset X\) then \(\varphi(A)\sqsubset Y\)
2. \(A = \uparrow\bigcup_{i}A_{i}\) then \(\varphi(A) = \uparrow\bigcup_{i}\varphi(A_{i})\)
*** logic in a CCC

A syllogistic's view: view \(\text{Hom}_{\mathsf C}(A, B)\) as the sequent \(A \vdash B\). A proof that proves the sequent is a morphism in \(\text{Hom}_{\mathsf C}(A, b)\). And to generalise the idea of \(\Gamma \vdash A\), we introduce product.

Left rules:
what?

*** \(\eta\)-conversion

We want find something unique in exponential. Given \(B^{A}\) and an object \(C\), we want to find a morphism from \(C\) to \(B^{A}\). Let us say it is \(g\), we can have an equation for \(g\), provided with the diagram:

\begin{displaymath}
\begin{tikzcd}
C\times A
\arrow[rr, "g\times \mathrm{id}"]
\arrow[rd, "\epsilon (g \times \mathrm{id})"]
                &   & B ^{A} \times A
                      \arrow[ld, "\epsilon"]  \\
                & B
\end{tikzcd}
\end{displaymath}
the equation:
\begin{displaymath}
g = \lambda (\epsilon \cdot (g \times \mathrm{id}))
\end{displaymath}

if we now treat \(\lambda\) as something we are more familiar with, we have this immediate reduction or \(\eta\)-conversion

\[
g = \lambda x (g)x
\]

Remind me of what \(\lambda\) means in the first place. When given a function \(f\) from \(C\times A\) to \(B\), we have a \(\lambda(f)\) from \(C\) to \(B^{A}\), which means that we can factor a function \(f\) to a \(\lambda(f)\) and an evaluation function \(\epsilon\). How does that transfer from \(\lambda\) to this \(\lambda\)?

So a \(g\) here is function from \(C\) to \(B^{A}\). We can use \(g\) to construct a \(C\times A\) to \(B\). And then we use this constructed result, we can then use \(\lambda\) to find a \(C\to B^{A}\). Who is exactly \(g\) itself, since we know this is a CCC.

Okay I don' know what the hell I am say.

*** surjective pairing in category

the surjective pairing is below equation:

\begin{equation}
(\pi_{1}a, \pi_{2}a) = a
\end{equation}

where \(a\) is a variable of type \(A\land B\), of form \((x, y)\), where \(x\) and \(y\) are of type \(A\) and \(B\) respectively.

here we use the idea of unicity above, we consider the unique Hom as an unknown variable, we can have an equation, provided by the commutative diagram:

\begin{displaymath}
\begin{tikzcd}
C \arrow[rrd, bend left, "\pi_{1}\cdot h"]
  \arrow[rdd, bend right,"\pi_{2}\cdot h"]
  \arrow[rd, "h"]                 & & \\
                                  & A\times B \arrow[r, "\pi_{1}"]
                                              \arrow[d, "\pi_{2}"] & A\\
                                  & B
\end{tikzcd}
\end{displaymath}
the equation is:
\begin{equation}
(\pi_{1} \cdot h, \pi_{2} \cdot h) = h
\end{equation}

here we can learn about the corresponce a little bit more, where a formula in is represented as a hom from \(\text{Hom}_{\mathsf C}(C, A)\), where \(C\) is a random object, and \(A\) is the respective formula. And very true that a \(h\) which is hom in \(\mathrm{Hom}_{\mathsf C}(C , A\times B)\), is treated as an variable for the type \(A \times B\).

Here \(\pi_{1}\cdot h\) is an obvious variable for formula \(A\), which is constructed via projecting from \(A \land B\).

** chapter 8: coherent spaces

*** the definition of coherent spaces

A coherent space has some components:

1. web: a underlying set namely \(X\).
2. coherence: a reflexive and symmetric relation. I don't know how to typeset the symbol yet.
3. clique: a clique \(a\sqsubset X\), is a subset of \(X\), made of pair-wise coherent points.

*** the coding of scott domains

An example here is the cartesian product, namely \(\mathbf{bool}\times \mathbf{bool}\).

And we treat the space as a space with four point, namly: \(v\), \(f\) and \(v'\) and \(f'\). The points are pointwise coherent besides \((v, f)\) and \((v', f')\), and we consider a mapping \(F\) from the coherent space where the objects are cliques---the set whose elements are point-wise coherent.

\begin{align}
F(a)         &= v, \text{if } a \text{ has } v\\
F(\{f, f'\}) &= f\\
F(b)         &= \emptyset, \text{otherwise}
\end{align}

the coding is about representing the \(F\). We need some redundancy. we need to list all the \(a\) in the first equation above. Because in the definition of coherent space, if \(b \subset a\) then something is true also for \(b\). So it becomes

\begin{displaymath}
\begin{aligned}
F(\{v\}) = v\\
F(\{v'\}) = v\\
F(\{v, f'\}) = v\\
F(\{v', f\}) = v
\end{aligned}
\end{displaymath}

This is the coding of \(F\). The redundancy is need. I don't really know what is section is talking about.

*** stable function and stability

here \(X\) and \(Y\) are two coherent spaces, a stable function from \(X\) to \(Y\) satisfies:

Cliques: if \(a \sqsubset X\) then \(F(a)\sqsubset Y\)
Monotonicity: if \(a \subset b \sqsubset X\) then \(F(a)\subset F(b)\)
Continuity: \(F(\uparrow \bigcup_{i}a_{i}) = \uparrow \bigcup_{i}F(a_{i})\)
Stability: if \(a \cup b \sqsubset X\) then \(F(a \cap b ) = F(a) \cap F(b)\)

Stable order between the stable functions is defined:
Berry order: \(F\sqsubset G\) iff for all \(a \subset b \sqsubset X\), \(F(a) = F(b) \cap G(a)\).

What the hell is this one?

** chapter 9: linear logic

*** linearity

the definition of linearity is stable function that preserves coherent unions:

For instance \((C\cup D) a = (C)a \cup (D)a\).

** supplements

*** directed sets

A collection of sets is said to be directed, if the sets are indexed by a partially ordered set.

For example \(\{x_{i}\}\), \(i \in I\), where\(I\) is a partial order set.

*** logic complexity, first order

Logic complexity is about something like first order, second order; about the logic hierarchy, logic classification.

**** unbound quantifiers

Unbound quantifiers refer to quantifiers whose domain is not specified. But I think we can understand it as quantifiers whose domain is unbound, which is infinite.

**** prenex

prenex form is of form
\[
Q\sb{1}x\sb{1}\dots Q\sb{n}x\sb{n} A
\]
where \(A\) is quantifier free. /prenex/ is not that useful.

**** \(\Sigma\sp{0}\sb{1}\) and \(\Pi \sb{1}\sp{0}\)

The sb of the above notations is about the alternation of unbound quantifiers. I don't really know what is an unbound quantifiers. But zero alternation means no quantifier.

One alternation means the quantifiers are the same. For \(\Sigma\), the quantifer can only be \(\forall\), and for \(\Pi\) \(\exists\). Thus all formulas in \(\Sigma\sb{1}\sp{0}\) and \(\Pi\sb{1}\sp{0}\) are respectively of form:
\[
\exists x\sb{1}\dots\exists x\sb{n}A,\quad\quad
\forall x\sb{1}\dots\forall x\sb{n}A
\]
where \(A\) is \(Q\)-free. So you know alternation \(2\) means something like \(\forall\forall\forall\exists\exists\exists\exists\exists\exists\)

**** \(\Sigma\sp{0}\sb{1}\) sets

Here we understand \(\Sigma\sp{0}\sb{1}\) as a collection of sets. A \(\Sigma\sp{0}\sb{1}\) set is a set \(A\) that satify:
\[
x \in A \Leftrightarrow \exists y R(x,y)
\]
where \(R\) is a \(Q\)-free quantifier and can not be arbitrary.

**** \(\Delta\sb{1}\sp{0}\)

\(\Sigma_{1}^{0}\) and \(\Pi\sb{1}\sp{0}\) sets are respectively semi-decidable and co-semi-decidable.

semi-decidable means \(x \in A\) is decidable if \(x \in A\).
co-semi-decidable means \(x \in A\) is decidable if \(x \notin A\).

So a \(\Delta^{0}_{1}\) set is decidable, since \(\Delta^{0}_{1}:= \Sigma^{0}_{1}\cap \Pi _{1}^{0}\).

**** a classic example of \(\Delta\sp{0}\sb{1}\) set
(given by Copilot)

Sure! A classic example of a \(\Delta_1\sp{0}\) set is the set of even numbers.

- The set of even numbers can be defined by the formula: \( x \in \text{Even} \iff \exists y \, (x = 2y) \). This shows that the set of even numbers is in \(\Sigma_1\).

- The set of even numbers can also be defined by the formula: \(x \in \text{Even} \iff \forall y \, (x \neq 2y + 1) \). This shows that the set of even numbers is in \(\Pi_1\).

Since the set of even numbers can be characterized by both an existential and a universal quantifier, it is in the intersection of \(\Sigma_1\) and \(\Pi_1\), making it a \(\Delta_1\) set.

*** arithmetic RR

RR is the child of formalism. And we introduce \(=\) \(<\) and some constant \(0\) \(S\) \(\times\) \(+\).

Arithmetic system right now is boring because it is merely formalism shit.

**** group: equality

- \(x = x\)
- \(x = y \Rightarrow y = x\)
- \(x = y \land y = z \Rightarrow x = z\)
- \(x = y \land z =t \land x < z \Rightarrow y < t\)
- \(x  = y \Rightarrow S x = Sy\)
- \(x = y \land z = t \Rightarrow x + z = y + t\)
- \(x = y \land z = t \Rightarrow x \times z = y \times t\)

There rules are used to prove \(x = y \land A[x] \Rightarrow A[y]\).

**** group: definitions

- \(x + 0 = x\)
- \(x + S y = S(x+y)\)
- \(x \times 0 = 0\)
- \(x \times S(y) = (x\times y) + x\)

These can prove that if two terms \(x,y\) are the same number, then \(x = y\) is provable.

the 3rd and 4th peano axioms

- \(Sx \ne 0\)
- \(S x = S y \Rightarrow x = y\)

These group can be used to prove that if \(x, y\) are different number, then \(x \ne y\) is provable.
Also, these two things shamelessly assume an infinite domain, otherwise \(\overline{10} = \bar{0}\) could be proved.

**** group three: a last axiom
\[
x < y  \lor x = y \lor x > y
\]
what is this one used for? Let us check what Girard says:

#+begin_quote
  the last axiom of a slightly different nature from the rest, since it is not needed for /incompleteness/: the representation of expansive properties is handled by the definition axioms. It is used in the representation of recursive functions and therefore in the algorithmic undecidability of RR and all its consistent extensions. It is also used in the Rosser variant.
#+end_quote

*** PA

**** induction schema

Let us check how to express induction schema:
\[
A[0] \land \forall y (A[y] \Rightarrow A[Sy]) \Rightarrow \forall x A[x]
\]
here [[*\(\mathrm{PA}\sb{2}\) second order peano arithmetic][\(\mathrm{PA}\sb{2}\) second order peano arithmetic]], we have relative description for induction schema, where we have that if \(x\) is a nat number, then
\[
A[0]\land \forall y(A[y] \Rightarrow A[Sy]) \Rightarrow A[x]
\]
is provable.

**** the definition of PA

Peano's Arithmetic is derived from \textbf{RR}, added with induction schema.
\[
A[0] \land \forall y (A[y] \Rightarrow A[Sy]) \Rightarrow \forall x A[x]
\]
Here \(A\) is not arbitrary.

*** satisfiable

A formula is satisfiable if there is an assignment that makes it true.

An assignment for a formula is a set of assignments (the process of give value, some people use validate) for variables in the formula. After giving value to variables, we can now decide the true/false of the formula (by writing down the truth table).

A (propositional) formula is unsatisfiable means that there is no way for it to be true.

Remark: The definition of satisfiable can be extended to predicate logic, where instead of assignment, we say /model/. I just don't remember the terminology.

*** there is an assignment makes it true

There is an entry in truth table that is true.

*** ground terms

Ground terms are terms that have no variable.

*** ground instances

A ground instance of a formula \(S\) is a formula derived from \(S\), where all the variables are replaced with ground terms.

*** alternative explain for Herbrand's theorem

An formula \(S\) has a Herbrand's model means that there are a set of ground terms that makes \(S\) true.

*** simultaneous substitution \(\theta\)

A \(\theta\) can be applied to a formula or expression \(E\), but anyway I prefer \(A\) for a formula, \(\Gamma\) for a set of formulas.

\(\Gamma\theta\) means carry the substitution to every formulas in \(\Gamma\). We can compose those \(\theta\). For example, \(\theta\sigma\) means carrying \(\theta\) first and then \(\sigma\).

*** unifier \(\theta\)

A unifer \(\theta\) for a set of expressions \(S:=\{E\sb{i}\}\), is such \(\theta\), that

\[
E\sb{1}\theta = \dots = E\sb{n}\theta
\]

*** most general unifier

The most general unifier, a.k.a., m.g.u. is like the smallest unifier. A m.g.u. noted as \(\theta\), suit that \(\forall \sigma\) which is a unifier, there is a \(\rho\) such that
\[
\sigma = \theta \rho
\]

*** unification algorithm

unification algorithm is an effective algorithm used for search m.g.u.

Using the algorithm, we will find a mgu \(\sigma\) satisfies that for all unifier \(\theta\)
\[
\sigma \theta = \theta
\]
holds.

*** herbrand's theorem and cut-elimination

Herbrand’s Theorem: This theorem provides a way to transform a first-order logic formula into a purely propositional form.

It states that if a first-order formula is universally valid, then there is a finite set of ground instances (instances with no variables) of its clauses that are propositionally valid.

How they are related is that they both transfer something undecidable to decidable.

* some books

** hott

*** the intuition of dependent type theory and hott

A path of two point in topology is \(f\colon [0,1] \to X\), a "path" for a path to another path is homotopy, which is of type

\[
H \colon [0,1] \times [0,1] \to X
\]

\(H\) can be called a two-dimensional path, and \(f\) is a one-dimensional path. We can say a point is a zero-dimensional path.

This formulate a simple kinds of dependent type theory, we say \(x, y\) are equal if and only if there is a path from \(x\) to \(y\). This kind of "equal" can be equated with another "equal".

First we have \(x =_{A} y\), if there is a path from \(x\) to \(y\), where they are point in \(A\). And then we say \(p = _{(x = _{A} y)} q\) if we have a homotopy \(H\), where \(p, q\) are "points" in \(x =_{A} y\).

** some reviews

* little bishe
** some cnn

*** convolution

Convolution is used for smoothing a noised function sometimes. Say \(x(t)\) is the noised function where \(t\) is time and \(x\) is distance of a airplane. And then we use a weighting function \(w(a)\), where\(a\) is the age of a measurement(?). The convolution is defined as

\[
s(t) = \int x(a)w(t-a) \mathrm{d}a
\]
we write as
\[
s(t) = (x * w) (t)
\]
https://www.deeplearningbook.org/contents/convnets.html

*** input and kernel

in \(s(t) = (x * w)(t)\), \(x\) is *input* and \(w\) is *kernel*. The output is called feature map.

both input and kernel can be expressed with multi-dimensional matrix.

*** sparse interactions

A covolution is matrix manipulation. It allows sparse interactions by making kernel smaller than input, as oppose to traditional ways where an interaction is made by matrix multiplication.

*** pooling

The pooling layer is used for smoothing the result. In max pooling, the output \(s\sb{i}\) is the \(\max\) of the vicinity of input \(x\sb{i}\), for example

\[
s\sb{i} = \max\{x\sb{i-1},x\sb{i},x\sb{i+1}\}
\]

It is said that the pooling enable a resist for a small amount of translation. Damn bitch, why use the word translation?

** EdgeSimCloud

* diary

** [2024-10-08 Tue] 对于人的态度的察觉

以前是在高中，在那里，其实每个人都是没有什么坏的倾向的。我是一个老师，那么我的职责就是教好你。我是你的同学，那我们就共同进步，或者至少不相互干扰。

一种勉强、一种敷衍、一种自己没有察觉的沉醉，都是极度让人敏感的……我们亲爱的学长，仿佛是自己也不怎么想干这种事情的样子……真是令人无语……尤其对于我来说。

也就是这种事情最能打击人的积极性……

** [2024-10-07 Mon] 双重身

“我觉得我说很多事情都被当放屁了”，我当时是怎么想的？

说实话我完全记不得了……

这其实是有问题的啊

** [2024-10-06 Sun] being a women

基于一种刻板印象，在老友记之中，joey 说“不是你说的内容，而是你如何说的的”，随后惊呼“OMG，我是个女人”。但现实果真如此？

非也。我就是会被人的态度强烈地影响的人。其实这就是所谓的下头之一。当然下头更像是 [[*\[2024-10-06 Sun\] 重叠的经验][[2024-10-06 Sun] 重叠的经验]] 所提及的，矛盾的体验之间带来的一种 bewilderedness。

感性的人吗？但事实上不是如此……每个人都有感性的功能在其大脑内运作着。

** [2024-10-06 Sun] 重叠的经验

在话语形成之前，一般是先有体验，这是经验的一种，一种还没能成形的经验。我说，我一般先体会重叠的经验带来的感觉，然后才能用嘴说出话来。

比如说，我今日看到一个抱着玩偶的女孩。很不自然地、很可耻地，一种 sexualize 的行为就在这里产生了。一种重叠而且矛盾的经验让我有一种违和感。何者？对于“漫画”之中的这种形象的情感，放在现实之中就成为恶心的事情了，一种现实和虚构之中一个桥梁就搭了起来，现实和虚构相互渗透。

这种违和感仿佛是在对我说，“你个比差点成变态了”。

错位的经验带了一种还难以言表的震撼。仿佛是一个无形的链条的断裂。我是先有感觉才有解释，这倒是一种很麻烦的事情，因为我的解释是基于“感觉”而非事实。

** [2024-10-02 Wed] 对于“精神营养”

对于“精神营养”的追求，仿佛是一种幻觉，妄想在“精神”上赋予自己文化产品以高层次的意义。

面向一个男性向的作品，“男性向”一个评价似乎已经足够，但是在这里展现的是如此的言语：
“我仿佛没有感觉到你的作画的意图
“仿佛是为了画而画
“没有对精神营养的追求
“女性的身体作为一种客体被呈现在画之中
“能够感觉到创作的污浊的眼神”
……

幻想一般的对自己的消费的拔高，还加之这种无耻的性别叙事的应用。

其实是一种混用、以及这种混用导致的对于自己行为的合法化，也就是说，当我的需求是 mental 的时候，我却认为，我的需求是一种 spiritual 的。占据了前者的合法性，同时后者还让其占据了所谓的“意义”的高点。

他们的观点在事实上是一种滥觞。当有些人开始讨论消费者和创作者之间的关系、这种彼此相互渗透的两个全体之间的关系的语境之下，性别叙事作为一种50年前的老古董就很少的出现在这里，并且文化创作也不区分同人创作以及商业创作，所谓的“男性向”作品也并非以所谓的“色图”的形式出现。作为50年前的"先进"理论也是理所当然地有着市场，但只不过是作为一种最没有“营养”的评价而存在。

lead you astray

** [2024-09-30 Mon] nlab

nlab 让人恼火，虽然 nlab 对于很多人来说是不错的资源。但还是令人恼火。

比如说 coherent space 这个部分，事实上来说，我们抛弃掉拓扑的语言反而方便我们的理解。

damn

** [2024-09-29 Sun] 建议抬走

我看到 EdgecloudSim 其实是一个 Java 的玩意，货不对版啊！

其实应该报告一下。

** [2024-09-28 Sat] 不存在后悔

没有后悔，在那里只是为了弥补认知的不足所付出的代价。如果你的后悔只是“如果当初不用支付那种代价就好了”，那你还不如许个更实际的愿望，比如说“如果我家里当初拆迁然后突然多了200万就好了”。

** [2024-09-25 Wed] 装置的大小

装置是什么？舞台装置！

若要写个好短篇小说，你的装置得往大的写。仿佛是在进入眼帘的一刻开始就在蓄力。

对于长篇小说就不必然，装置可以和玩具一样。但是说它的大小变得如此之小，而玩弄的意味是如此的足，让人觉得其仅仅是玩具，像是 margin 上的涂鸦，像是道具而不是装置，像是一个超链接，给你导到不知哪里去了。

乔伊斯的«芬尼根的守灵夜»，我只看过第一句话，其首字母没有大写，明显是半句。“不会吧”，心里想着，翻到全书最后一句话上，“居然真的是”，确实是和第一句能接上……这似乎是最大的装置，贯穿了整部小说！但果真如此吗？

** [2024-09-24 Tue] 脆弱的“好看”

看到了一个reddit的帖子：有哪部电视剧是如此烂尾以至于你一直记恨它。我立刻就想起了我曾经看过的那些。

- lost 莫名其妙的结局，浪费了10季的时间
- arrow 本身就垃圾
- gothom 除了第一季就是垃圾
- the big thoery 越后来就越垃圾
- sherlock 最后一季根本不应该存在
- 2 broken girls 毫无疑问的垃圾

除了质量下滑之外，自己那么会被这种事情 trigger，其实是因为这种“好看”有一种非常脆弱的基底。你的“好看”为什么那么容易地被摧毁？怎么没见人被战争与和平恶心到，那里也有地雷的情节。

本身阅读（观影）是一种对话。但是人看小说就跟自慰一样，小说也写得和黄片一样。何来对话？

举个例子，人曾说自己因为犯了某一个错误，小说订阅数崩了几万。这似乎能说明对于人来说，小说只是随意替换的道具，就如同你的飞机杯坏了就买个新的。替换小说就如同关掉这个黄片然后打开另一个黄片一样简单。

在这里不仅是对话゠阅读，而且还是对于好看的质疑。若是以一种脆弱的形式享受，潜在的风险让人觉得不值当。这是某种抽离、自己的理智给出的警告——一种对下降的警告。

** [2024-09-22 Sun]

I feel like I am alive again.

** [2024-09-21 Sat] 行动力的缺失……到底如何诊断
[2024-09-21 Sat 10:15]

昨天很累，12 点钟就很累。没了，没啥别的想法。

有时候感觉自己是ADHD，注意力相当的涣散。这原来是早就有的迹象。

** [2024-09-20 Fri]

跑了好几趟，行动力的错误方向。一种虚伪的行动力。

还得更加那个。

** COMMENT [2024-09-19 Thu]
[2024-09-19 Thu 12:07]

[[*写作的步骤]]
[[*任务书，这周末]]

Preparing for shit.

作为人类的缺点，并不是不在乎……其实是清楚的

* entries

#+BEGIN: columnview :hlines 1 :id local :format "%item %todo"
| item                 | todo |
|----------------------+------|
| entries              |      |
| 书评 of east of eden | DONE |
| bishe                | TODO |
| visa 咨询            | TODO |
| 那个问题的描述       |      |
#+END:

** DONE 书评 of east of eden
<2024-10-08 Tue>

John Steinbeck 的 east of eden。首先是让人惋惜，其次是让人骂出，“其实获普利策奖的作家也只不过是一个二流作家”。

一开始最引人瞩目的就是简洁轻快的语言。以及之中完全在把握之中的叙事。

但是这种把握却让人觉得二流。因为……

对于角色的描写，仿佛是为了剧情而服务的。在这里原本两者一体的，却变为两个分离的个体了。主人公母亲的性格仿佛就是用来打破的---她不被允许发表自己的意见---在某一章节的最后，仿佛是理所当然的，她破天荒说出了自己的心里话。仿佛她的存在就是为了这一刻，作为一个死板的家庭里面的、无法表示自己观点的母亲，以这个身份在章节的末尾营造一种戏剧性。

又一章的末尾，主人公和自己的哥讲点心里话。但这却让人疑惑，“他果真是如此讲话的吗？”。主人公难得有一段长一些的对话，但却感觉和他的性格对不上？但其实是“他的性格”本身居然没有定形下来。

小说里面最难写的就是对话。至此我已经放弃这本书了，因为对话让人发觉作者水平不咋的。不会写对话建议像百年孤独一样别写。

** TODO bishe
<2024-10-08 Tue>

** TODO visa 咨询                                                     :visa:

- [ ] 询问消息。1. 对后面的影响 2. 前人申诉的经历、自己是否应该进行申诉 3. 询问材料的缺失
- [ ] 是再次申请还是等别的？

*** 那个问题的描述

* archive

#+BEGIN: columnview :hlines 1 :id local :format "%item %todo %tags"
| item             | todo | tags |
|------------------+------+------|
| archive          |      |      |
| [2024-09-27 Fri] |      |      |
| 写作的步骤       |      |      |
| 2-3 核心论文     |      |      |
| 实现的工作       |      |      |
| 做实验           |      |      |
| 写论文           |      |      |
#+END:

** [2024-09-27 Fri]

- EdgeCloudSim 代码重构
- 从别人的方法复现

** 写作的步骤

*** 2-3 核心论文

- 顶会发展
- 比较近
- 积累一些 introduction
  - 研究背景

*** 实现的工作

- 展现的形式
  - 怎么布局、怎么画图
  - 框架图
  - 技术细节
- =复现=

*** 做实验

- 指标
- 形式、图标
- 日志

*** 写论文

- 文字衔接、润色
