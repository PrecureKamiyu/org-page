#+title:Note
#+author:MAO
#+startup: overview
#+options: tex:dvisvgm toc:2

#+latex_header: \usepackage{amsmath}
#+latex_header: \usepackage{mathrsfs}
#+latex_header: \usepackage{amssymb}
#+latex_header: \usepackage{tikz}\usetikzlibrary{cd}
#+latex_header: \usepackage{bussproofs}
#+latex_header: \usepackage{prftree}
#+latex_header: \usepackage{cmll}
#+latex_header: \usepackage[mathletters]{ucs}
#+latex_header: \usepackage[utf8x]{inputenc}
#+latex_header: \newcommand{\semicolon}{\mathbin{;}}
#+latex_header: \newcommand{\N}{\mathbb{N}}
#+latex_header: \newcommand{\R}{\mathbb{R}}
#+latex_header: \newcommand{\C}{\mathbb{C}}
#+latex_header: \newcommand{\B}{\mathbb{B}}
#+latex_header: \newcommand{\DoubleSlash}{/ \kern-3pt /}
#+latex_header: \renewcommand{\succ}{\texttt{succ}}
#+latex_header: \newcommand{\Prop}{\mathsf{Prop}}
#+latex_header: \newcommand{\Type}{\mathsf{Type}}
#+latex_header: \newcommand{\Hom}{\mathrm{Hom}}
#+latex_header: \newcommand{\Fam}{\mathrm{Fam}}
#+latex_header: \newcommand{\Sets}{\mathbf{Sets}}
#+latex_header: \newcommand{\Pred}{\mathbf{Pred}}
#+latex_header: \newcommand{\Fibre}[2]{%
#+latex_header:   \begin{gathered}\scriptstyle #1\\[-7pt]
#+latex_header:   \scriptstyle \downarrow\\[-7pt]
#+latex_header:   \scriptstyle #2
#+latex_header: \end{gathered}}
#+latex_header: \newcommand{\netarrow}[2]{\ifnum #2>0 \ar[to=#1, to path={ -- ([yshift=#2ex]\tikztostart.north) -| (\tikztotarget)}] \else \ar[to=#1, to path={ -- ([yshift=#2ex]\tikztostart.south) -| (\tikztotarget)}] \fi}
#+latex_header: \newcommand{\zerohbox}[1]{\makebox[0pt]{$#1$}}
# 802160

* Books :noexport:

** The Blind Spot by Girard

*** chapter 3: LK

**** part 1: starting up, rules and more

***** the problem with Modus Ponens

There are actually problems with Modus Ponens. Modus Ponens is actually very like cut rule.  Why?  Because in the view of proof finding, we start from nothing: if we want to proof \(B\), then probably we have to image such \(A\),

\[
A \to B,
\]

holds.  "We have to imagine such \(A\)".  And this is the problem.

***** generalization

From \(A[x]\) to \(\forall xA[x]\). And this is called *generalization*.  What is wrong with it?

***** problems with hilbert's formalism and system

Anyway, the two rules make the Hilbert's logic system non-usable, because it has no good structural property.
And thus even the slightest automated deduction is impossible.

It is said that hilbert's system is a garbage, in the sense of equivalence, it does no more than other system but hinder our understanding.

***** deduction theorem

if \(B\) is provable in system \(\mathcal F + A\), with \(A\) closed, then \(A \Rightarrow B\) is provable in \(\mathcal F\).

The proof is not provided somehow.

***** sequent and hypothesis

I don't really know the intuition behind the idea that LK is better than Hilbert's, but it seems that it is because of the introduction of hypothesis.

Anyway, the problem of "let us imagine such a premise \(A\)" can be solved with two tools:

  1. LK
  2. cut-elimination.

Actually LK is no better than Hilbert's system. It just because that Hilbert's system is so bad for proving.

***** sequents

A sequent is expression of \(\Gamma \vdash \Delta\), where \(\Gamma\) and \(\Delta\) are all finite sequences of formulas.

***** the notion of context

We use more notion of context, in order to make a difference from other stuff.  The reason we are doing this?  I don't know.

But anyway, \(\Gamma \vdash A, \Delta\) for example, the context of it is \(\Gamma, \Delta\), one can write as \(\Gamma \vdash {-}, \Delta\)

***** the intuitive "semantic" of LK

\[
\Gamma \vdash \Delta
\]
for this expression, it means "if all the formulas in \(\Gamma\) are correct", then "one of the formula in \(\Delta\) is correct". By the way, the symbol \(\vdash\) is called "turnsile".

The symbols used in LK are:
1. left commas: means «and»
2. right commas: «or»
3. turnsiles: «implies»

***** special sequent in LK

\begin{gather}
A \vdash\\
\vdash \\
\vdash A
\end{gather}
the first one means \(A\) leads to nothing, which means \(A\) is contradictory, and 2 means an expression like \(v \to f\), which is bad, 3 means \(A\) is true without hypothesis, which is to say \(A\) is tautology.

The proof of second is important, given by Gentzen, saying that LK is *consistent*.

***** negate of \(A\)

I think we have this \(A \vdash\) then \(\vdash \neg A\). The precise definition is not here tho.

***** rules for LK

The content of LK is the rules. The rules are divided into three groups, we can actually only introduce those important ones, because introducing those about the connectives is always not necessary.

****** the identity group

There are two rules in this group they are called identity and cut rule
\[
\frac{}{A \vdash A}\ (\mathrm{id}) \quad \quad \quad
\frac{\Gamma \vdash A, \Delta \quad \Lambda, A \vdash \Pi}{\Gamma, \Lambda\vdash \Delta, \Pi}\ (\mathrm{cut})
\]
There are special cases for cut rules, because of the context. They are 1. modus ponens 2. transitivity
\[
\frac{\vdash A\quad A \vdash B}{\vdash B}\ (\mathrm{modus ponens})
\]
\[
\frac{A\vdash B\quad B\vdash C}{A\vdash C} \ (\mathrm{transitivity})
\]
cut rule is most important one.

****** Hauptsatz of Gentzen

Hauptsatz is called the cut-elimination. So basically it says that cut rule is redundant. Hauptsatz means big in Germany. So it can be called the main theorem.

****** structural group

\begin{align}
&&\frac{\Gamma\vdash\Delta}{\sigma(\Gamma)\vdash\tau(\Delta)}\tag{exchange}\\
&\frac{\Gamma \vdash \Delta}{\Gamma, A \vdash \Delta}
&&\frac{\Gamma \vdash \Delta}{\Gamma \vdash A, \Delta}\tag{weak} \\
&{\Gamma \vdash A, A, \Delta \over \Gamma \vdash A, \Delta}
&&{\Gamma, A, A \vdash \Delta \over \Gamma, A \vdash \Delta}\tag{contract}
\end{align}

****** contraction and weakening

The rules seem intuitive at first, but there are spaces for we to discuss.

See page 61.

****** logical group

The logical group is about those connectives like \(\forall\) \(\exists\) and so on. Here is the deal

\[
\frac{\Gamma\vdash A, \Delta}{\Gamma \vdash \forall x A, \Delta}(\vdash \forall) \quad \quad
\frac{\Gamma, A[t/x]\vdash \Delta}{\Gamma, \forall x A\vdash \Delta}
(\forall\vdash)
\]

there are important constrains for the rules:

1. for \(\forall \vdash\) and \(\vdash \exists\), \(t\) should not use \(x\).
2. \(\vdash \forall\) and \(\exists\vdash\), as \(x\) is introduced, there should be no appearance of \(x\) in context of \(A\), that is \(\Gamma \vdash {-}, \Delta\).

And of course, there are some other rules for \(\Rightarrow\)

\[
\frac{\Gamma, A \vdash B, \Delta}{\Gamma \vdash A\Rightarrow B, \Delta}\quad\quad
\frac{\Gamma \vdash A, \Delta\quad\Lambda, B \vdash \Pi}{\Gamma,\Lambda, A\Rightarrow B\vdash\Delta,\Pi}
\]

You may need some time to process this one.

\[
\frac{\Gamma \vdash A[t], \Delta}{\Gamma \vdash \exists xA[x], \Delta}\ (\vdash \exists)
\quad\quad
\frac{\Gamma, A\vdash \Delta}{\Gamma, \exists x A\vdash \Delta}\ (\exists\vdash)
\]

here \(A[t]\) is the same as \(A[t/x]\), the latter is more precise. Why are there two symbols because those are symbols used separately in book « proof theory and logic complexity » and « The Blind Spot ».

****** eigenvariable

The notion eigenvariable is not unambiguous.

Since we introduce a variable \(x\) in \(\vdash \forall\), we don't want the premise has \(x\) in it. Thus the premise is written as \(\Gamma \vdash A[v/x],\Delta\).

The thing is to avoid the notion of "bound variable".

****** laxist notation of eigenvariable

Here we have the laxist notation when confronting the variable \(x\) in the rule \(\vdash \forall\). This is because "bound variable" is sometimes confusing.

We say that when introducing \(x\), the premise has no occurrence of \(x\) in it. \(A\) becomes \(A[v/x]\) using a placeholder variable \(v\) that does not appear anywhere.

Significant details really.

***** right hand calculus of LK

The symmetry of LK makes it possible to decrease the size of rules significantly.

\[\begin{aligned}
&\frac{}{\vdash \neg A, A}\ (\mathrm{id})
& %
& \frac{\vdash \Delta, A\quad \vdash \neg A, \Pi}{\vdash \Delta, \Pi} (\mathrm{cut})
\\
\\
& \frac{\vdash \Delta}{\vdash \tau (\Delta)}\ (X)
& \frac{\vdash \Delta}{\vdash A, \Delta}\ (W)\quad\quad
& \frac{\vdash A,A,\Delta}{\vdash A, \Delta}\ (\mathrm{con}) %
\\
\\
& \frac{\vdash A, \Delta}{\vdash A \lor B, \Delta}\ (\vdash \lor)
& %
& \frac{\vdash A, \Delta\quad \vdash B, \Delta}{\vdash A\land B, \Delta}\ (\vdash \land)
\\
\\
& \frac{\vdash A, \Delta}{\vdash \forall x A, \Delta}
&
& \frac{\vdash A[t/x], \Delta}{\vdash \exists x A, \Delta}
\end{aligned}\]

**** part 1.5: some exercises

***** some basic

Check for [[*logical group]]

****** \(A \to (B \to A)\)

\begin{prooftree}
\AxiomC{\(A\vdash A\)}
\UnaryInfC{\(A, B\vdash A\)}
\UnaryInfC{\(A\vdash B \to A\)}
\UnaryInfC{\(\vdash A\to (B \to A)\)}
\end{prooftree}

****** \((A\to (B \to C)) \to (A\to B) \to A \to C\)

\begin{prooftree}
\AxiomC{\(A \vdash A\)}
   \AxiomC{\(A \vdash A\)}  \AxiomC{\(B \vdash B\)}
   \BinaryInfC{\(A \to B, A \vdash B\)}
                                    \AxiomC{\(C\vdash C\)}
                    \BinaryInfC{\(B \to C, A\to B, A \vdash C\)}
\BinaryInfC{\(A\to (B \to C), A, A\to B \vdash C\)}
\end{prooftree}

****** \(A \to A\lor B\)

apparent

****** \((\neg A\to\neg B) \to (\neg A\to B)\to A\)

I don't want to prove it.

****** \(\forall xA[x] \to A[t/x]\)

\begin{prooftree}
\AxiomC{\(A[t]\vdash A[t]\)}
\UnaryInfC{\(\forall xA[x]\vdash A[t]\)}
\UnaryInfC{\(\vdash \forall x A[x]\to A[t]\)}
\end{prooftree}

****** \(A[t/x]\to \exists xA[x]\)

\begin{prooftree}
\AxiomC{\(A[t]\vdash A[t]\)}
\UnaryInfC{\(A[t]\vdash \exists xA[x]\)}
\UnaryInfC{\(\vdash A[t]\to \exists xA[x]\)}
\end{prooftree}

***** prove that it is legit to restrict the id axiom on atom formula

that is, if axioms \(\dfrac{}{A \vdash A}\) holds only when \(A\) is atomic, the system is still legit.

Proof is simple. We use induction on formula \(A\)

****** if \(A\) is of form \(B \to C\)

\begin{prooftree}
\AxiomC{\(C\vdash C\)}
\AxiomC{\(B\vdash B\)}
\BinaryInfC{\(B\to C, B \vdash C\)}
\UnaryInfC{\(B\to C\vdash B\to C\)}
\end{prooftree}

****** if \(A\) is of form \(\neg B\)

\begin{prooftree}
\AxiomC{\(B \vdash B\)}
\UnaryInfC{\(\vdash B, \neg B\)}
\UnaryInfC{\(\neg B \vdash \neg B\)}
\end{prooftree}

****** if \(A\) is of form \(B \lor C\)

\begin{prooftree}
\AxiomC{\(B\vdash B\)}
\UnaryInfC{\(B \vdash B \lor C\)}
  \AxiomC{\(C\vdash C\)}
  \UnaryInfC{\(C\vdash B \lor C\)}
\BinaryInfC{\(B\lor C\vdash B \lor C\)}
\end{prooftree}

****** if \(A\) is of form \(B\land C\)

\begin{prooftree}
\AxiomC{\(B\vdash B\)}
\UnaryInfC{\(B\land C \vdash B\)}
  \AxiomC{\(C\vdash C\)}
  \UnaryInfC{\(B\land C\vdash C\)}
\BinaryInfC{\(B\land C\vdash B\land C\)}
\end{prooftree}

****** if \(A\) is of form \(\forall x B\)

****** if \(A\) is of form \(\exists xB\)

***** without structural rules, set-based sequent calculus

Structural rules are exchange, weakening and contraction. See [[*structural group]]

If we exclude those rules, we have set-based sequent calculus, where we treat \(\Gamma\) as set! This can be viewed as a variation of sequent calculus.

How can I prove this?

***** signature

Here is the def of signature of an occurrence of formula \(P\) in \(A\). \(\pi\) is an occurrence of \(P\).

1. if \(A\) is atomic, \(P\) is pos.
2. if \(\pi\) is pos in \(A\), then it is so in \(A\lor B\), \(A\land B\), \(B\to A\), \(\forall xA\), \(\exists xA\).
3. if \(\pi\) is pos in \(A\), then it is neg in \(A \to B\) and \(\neg A\).

We need also def signature of \(\pi\) of \(P\) in a sequent \(\Gamma \vdash \Delta\). as one can imagine, if \(\pi\) is pos in \(\Gamma\), then \(\pi\) is neg in sequent. if \(\pi\) is pos in \(\Delta\), then \(\pi\) is still pos in the sequent.

Prove that cut-free proofs preserve the signature.

Proof. Obvious.

***** \(A\sb{1}\dots A\sb{n}\vdash B\sb{1}\dots B\sb{m}\) leads to \(A\sb{1}\land,\dots,\land,A\sb{n}\vdash B\sb{1}\lor,\dots,\lor B\sb{m}\)

The proof is actually obvious. I think.

**** part 2: cut free and subformulas properties

***** Hauptsatz of Gentzen

The theorem can be stated as follow:

Every theorem in *LK* or *LJ* has a cut-free proof.

***** the proof given by Gentzen

The proof of Hauptsatz is not worth trusting. It is tedious and hard to follow. The idea is that there are many key cases he can list, where the cut can be push upward. In this process of pushing and potentially expanding the proof size, we can prove that the process is decisive.

And thus we can conclude that we can construct a cut-free proof.

***** DONE subformula properties

****** subformulas

if \(A\) is atomic, then the subformula of \(A\) is \(A\) itself and nothing else.

if \(A\) is of form \(B * C\), then the subformula of \(A\) is \(A\) itself and subformulas of \(B\) and \(C\).

if \(A\) is of form \(\forall x B\) or \(\exists x B\), then the subformula of \(A\) is \(A\) itself and the subformulas of \(B[t]\) for some term \(t\).

****** the theorem

The cut-free proofs of a sequent \(\Gamma \vdash \Delta\) are consisted of sequents made out of the subformulas in \(\Gamma\) and \(\Delta\).

It is obvious that if there is quantifiers (\(\forall\), \(\exists\)) in the \(\Gamma\vdash \Delta\), there are infinite number of subformulas. The proof searching seems to be indecisive. However.

****** description subformulas in Proofs and Types

Let \(\delta\) be a /normal/ deduction in the (\(\land \Rightarrow \forall\)) fragment. Then

1 every formula in \(\delta\) is subformual of a conclusion or a hypothesis of \(\delta\);

2 if \(\delta\) ends in an elimination, it has a /principal branch/, i.e., a sequence of formulas \(A _{0}, \dots, A_{n}\) such that

   - \(A_{0}\) is an (undischarged) hypothesis;
   - \(A_{n}\) is the conclusion;
   - \(A_{i}\) is the principal premise of an elimination of which the
     conclusion is \(A_{i+1}\) for \(i = 0, \dots , n - 1\).

In particular \(A_{n}\) is a subformula of \(A_{0}\).

**** part 3: more

***** second order

****** the notation

We use \(X\) as variable for predicate. But we have some different notations.

- If \(X\) is arity 1, then it is supposed to be \(X(t)\) for term \(t\). But we wrote \(t \in X\), just like \(X\) is a set.
- If \(A\) is a formula, and we use it to create a predicate \(\{x; A\}\) (arity 1). Just like a set, whose elements are all \(x\) that satisfied \(A\).

I don't really know why we do this. I think this implies an relation with another expression of second order predicate logic. That is we treat \(X\) like a set. For example, we say that \(\mathbf{N}\) is the set of natural number, and thus we have:
\[
x \in \mathbf{N} := \mathsf{N}(x)
\]
where we say \(\mathsf{N}\) is a 1 arity predicate, and \(\mathsf{N}(x)\) means «\(x\) is natural number».

****** rules in second order

\[
\frac{\Gamma\vdash A,\Delta}{\Gamma\vdash\forall XA,\Delta}\ (\vdash \forall\sb{2})\quad\quad
\frac{\Gamma, A[T/X]\vdash\Delta}{\Gamma, \forall XA\vdash\Delta}\ %
(\forall\sb{2}\vdash)
\]

\[
\frac{\Gamma\vdash A[T/X],\Delta}{\Gamma\vdash \exists XA,\Delta}
\ (\vdash \exists\sb{2})
\quad\quad
\frac{\Gamma, A\vdash\Delta}{\Gamma,\exists A\vdash\Delta}
\ (\exists\sb{2}\vdash)
\]

where \(T\) should have the same arity of \(X\), in \(A[T/X]\).

***** \(\mathrm{PA}\sb{2}\) second order peano arithmetic
:PROPERTIES:
:ID:       16547be7-8ed1-431b-b733-c1cd6048cda8
:END:

We write a Dedekind integers for example:
\[
N := \{x ; \forall X(0 \in X \land \forall z(z \in X \Rightarrow Sz \in X) \Rightarrow x \in X) \}
\]
How to understand this one, we treat \(X\) as a simple propostion for example \(A\). \(x \in X\) is \(A\), and \(z\in X\) is \(A[z/x]\).
Then we may have
\[
x \in N \vdash A[0] \land \forall z(A[z/x]\Rightarrow A[Sz / x]) \Rightarrow A[x/x]
\]

***** comprehension schema in second order

This is something that I don't understand, about the terms that Girard was using.

We start with \(\vdash \forall x (A \Leftrightarrow A)\), how can we get
\[
\vdash \exists X \forall x ( x \in X \Leftrightarrow A)
\]
What he said is we use a rule \(\vdash \exists\sb{2}\), on the abstract term \(T := \{x ; A\}\)

Okay, if \(T\) is \(\{x; A\}\) then \(x\in T\) means actually \(A\). And here we should look at \(\forall x  A\). We assume that there is a \(x\) in \(A\). Then \(\forall x A \equiv \forall x (x \in T)\) seems very legit.
The overall process is \(\vdash \forall x (A \Leftrightarrow A)\) rewrite as \(\vdash\forall x (x \in T \Leftrightarrow A)\), and then we apply \(\vdash \exists\sb{2}\), with result \(\vdash \exists X \forall x(x \in X \Leftrightarrow A)\).

Okay, what the fuck is this schema used for?

***** LJ is a subsystem of LK

LJ, where all the sequent \(\Gamma\vdash\Delta\) where \(\Delta\) consists of at most one formula, is a subsystem of LK.

LJ actually enjoys Hauptsatz and subformula properties.

***** decisiveness of LJ

The introduction of LJ is owing to an obvious reason, the LJ is relatively less expressive (where law of middle excluded can be derived), but LJ is fucking decisive.

*** chapter 4: LJ

**** intuitionistic sequent

A intuitionistic sequent is of form \(\Gamma \vdash A\), where \(A\) is a formula.

That is LJ is a subset of LK, with the restriction of there is one and only one formula one the right hand side.

**** 0 in the LJ

The right hand side can not be empty but can be \(0\). There is rule of the introduction \(0\):

\[
\frac{}{\Gamma, 0 \vdash A}\ (0\vdash)
\]

**** rules

id

\[
\frac{}{A\vdash A}
\quad\quad
\frac{\Gamma \vdash A \quad \Lambda, A \vdash B}{\Gamma, \Lambda \vdash B}
\]

structural

\[
\frac{\Gamma\vdash A}{\sigma(\Gamma)\vdash A}
\]

\[
\frac{\Gamma\vdash B}{\Gamma, A\vdash B}
\]

\[
\frac{\Gamma, A, A\vdash B}{\Gamma, A\vdash B}
\]
logical group is 略

\[
\frac{\Gamma \vdash A}{\Gamma \vdash \forall x A}\ (\vdash \forall)
\quad\quad
\frac{\Gamma, A\vdash B}{\Gamma, \forall xA\vdash B}\ (\forall\vdash)
\]

\[
\frac{\Gamma \vdash A[t/x]}{\Gamma\vdash \exists x A}\ (\vdash \exists)
\quad\quad
\frac{\Gamma, A\vdash B}{\Gamma, \exists x A\vdash B}\ (\exists\vdash)
\]
\[
\frac{}{\Gamma, \mathbf{0}\vdash A}
\]

**** main formulas of the rule

We intuitively know there is a main character for a rule, for example, in NJ:

\[\prftree[r]{$\Rightarrow I$} {\prfsummary{[A]}{B}} {A \Rightarrow B}\]

Of course this rule has a main premise which is the most special formula here, that is \(A\). \(A\) is put into \(A \Rightarrow B\), and \(\Rightarrow\) is introduced.

And also sometimes we may want to number the formula and also number the connectives. For example, for the reduction of the proof in NJ, the redex (the proofs that need to be reduced) is such procedural with the same connective is introduced and eliminated immediately afterward.
The same connective means literally the same one after numbered!

**** Gödel's translation

***** use \(A \to B\) induce \(\neg B \to \neg A\)

This is trivial in LK, but not in LJ.

\begin{prooftree}
\AxiomC{\(A \vdash B\)}
   \AxiomC{ }
   \UnaryInfC{\(\textbf{0} \vdash \textbf{0}\)}
\BinaryInfC{\(A, \neg B \vdash \textbf{0}\)}
\UnaryInfC{\(\neg B \vdash \neg A\)}
\end{prooftree}

***** the intuition of «hole» in LJ

in the introduction of \(\neg B \vdash \neg A\), we notice that when migrating \(B\) to the left side, we leave a «hole» one right side that is \(\textbf{0}\). And respectively, when migrating \(A\) to the right side, it occupies the «hole».

***** double migration to prove \(A \to \neg\neg A\)

A migration of a formula \(A\) will add a \(\neg\) to it. So a double migration on right hand side of \(A \vdash A\) will naturally lead to \(A \vdash \neg \neg A\).

\begin{prooftree}
\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\end{prooftree}

in the classic logic we can prove \(\neg\neg A \vdash A\), this is because we can do a double migration on the left hand side of \(A\vdash A\).

Although it is not possible to prove \(\neg\neg A \vdash A\), \(\neg\neg\neg A \Leftrightarrow \neg A\) is provable.

***** the Gödel's theorem

\(A\) is classically provable (provable in LK) iff \(A^{g}\) is intuitionistically provable (provable in LJ).

where we get \(A^{g}\) by adding \(\neg\neg\) to the front of every atomic formulas, quantifiers and connectives:

- \(A^{g} := \neg\neg A\),
- \((A \land B)^{g} := \neg\neg(A^{g} \land B^{g})\)
- \((\forall x A)^{g} := \neg\neg \forall x A^{g}\)

Proof is simple. And a sort of completeness is achieved in LJ.

**** Hauptsatz in LJ

***** the decidability of LJ

there is a remark that I don't understand in page 74.

***** intuitionistic existence and disjunction

LJ has a remarkable property:

\textsc{Theorem} if \(\vdash A\lor B\) is provable, then either \(\vdash A\) or \(\vdash B\) is provable.

\textsc{Theorem} if \(\vdash \exists xA\) is provable, then there is an appropriate \(t\), such that \(\vdash A[t/x]\) is provable.

***** the myth around the fine property of LJ

there is some misunderstanding about the fine property above: if we can \(\vdash A\) is provable, why bother to prove \(A\lor B\)?

From \(A \lor B\) to either \(A\) or \(B\), this is explicit only if you use cut-free proof system, but a cut-free proof is so tedious and something unnecessary. We may implicitly prove (with cut) \(A\lor B\) without knowing which one is true. We know only after cut-elimination!

Keep in mind that the propery is the corollary of Hauptsatz.

***** explicitable logic

LJ is not an explicit logic (it is if cut-free), but we can say it is explicitable.

**** NJ

***** conclusion and hypothesis

The begin of structure like
\[
\prfsummary{\Gamma}{A}
\]
The structure needs more clarification. It is more like a tree but upside down. The root is \(A\), and there are multiple leaves. The leaves are called hypothesis. The \(\Gamma\) here is the set of leaves in the proof tree.

\[
A
\]

is the proof of \(A\vdash A\), where the hypothesis \(A\) and the conclusion \(A\) itself.

***** introduction of \(\Rightarrow\)

\[\prftree[r]{$(I \Rightarrow)$}
{\prfsummary{[A]}{B}}
{A\Rightarrow B}\]

The introduction of \(\Rightarrow\) introduce \([A]\) means the \(A\) is marked as discarded. So one of the subproof (whose conclusion is \(A\)) is marked as used.

Let us say the proof of \(A\) use hypothesis \(\Gamma_{1}\), and the proof of \(B\) use hypothesis \(\Gamma_{1} * \Gamma_{2}\), here the proof of \(A\Rightarrow B\) use hypothesis \(\Gamma_{2}\), because the proof of \(A\) is abandoned. This becomes clear when you translate NJ to LJ.

*** chapter 5: functional interpretation

**** Proofs as functions

For example, conjunction: \(\theta\) is a proof of \(A \land B\) iff \(\theta  = (\theta _{1}, \theta_{2})\) where \( \theta_{1}\) is proof of \(A\) and \(\theta_{2}\) is the proof of \(B\).

*Implication*: \(\theta\) is a proof of \(A \Rightarrow B\) iff \(\theta\) is a function that maps proofs of \(A\) \(\theta_{1}\) to proof of \(B\).

*Universal quantification*: \(\theta\) is a proof of \(\forall x A\) iff \(\theta\) is a collection of proof \(\theta(n)\) of \(A [\bar n / x]\).

I think here the atom formulas here are \(\bf 0\) and \(t = u\) and \(t < u\).

*Existential quantification*: \(\theta\) is a proof of \(\exists x A\) iff \(\theta\) is a pair \((n, \theta_{1})\) where \(\theta_{1}\)is the proof of formula \(A[\bar n/x]\).

**** remarks

A few remarks:

  - It is not a matter of formal proofs. A formal proof is a sequence of
    symbols, by no way an application; it is rather an /interpretation/ of
    formal proofs, or again the attempt at /explaining/ logic out of a primitive
    material external to formalism.

  - However this approach could, in disguise, be an alternative definition of
    formal proofs. This is tenable for all operations, except implication and
    universal quantification which refer to applications whose domain is not
    finite (neither definite in the case of implication). Kreisel’s attempt to
    overcome this mismatch foundered into /sectarianism/ (Section 5.A).

  - The cases of existence and disjunction (which are reminiscent of the
    wellknown properties of system LJ) show that one has in mind cut-free,
    explicit proofs: one is quite far from the /deductive/ world.

  - The disjunctive clause does not only mean « a proof of \(A\) or a proof of \(B\)
    », it also says /which/ one. This immediately induces, even in the finite
    case, an immense difference with semantics. Indeed, anything is a proof of
    \(0 = 0\);but a proof of \(0 = 0 \lor 0 =0\) is not a proof of one or the other (in
    this case it would not matter); it is a pair (i, \(\theta\))where \(\theta\) does not
    matter, but where \(i\) is a bit making a left/right choice. This is a radical
    novelty w.r.t. semantics; for instance, the not quite exciting Kripke
    models. The functional interpretation is not concerned with the raw fact
    of knowing that A is true, it says /how/: here, leftwise or rightwise.

*** interlude: sum type

From Proofs and Types

**** Empty type

\textsf{Emp} is considered to tbe the empty type. For this reason, there will be a canoical function \(\varepsilon_{U}\) from Emp to any type \(U\): if \(t\) is of type Emp, then \(\varepsilon_{U}t\) is of type \(U\). The commutation for \(\varepsilon_{U}\) is set out in fives cases:
\[\pi _{1} (\varepsilon _{U \times V} t)\leadsto \varepsilon_{U}t,\qquad
\pi_{2} (\varepsilon _{U \times V} t) \leadsto \varepsilon _{V}t,\]
\[(\varepsilon _{U \to V} t)u \leadsto \varepsilon _{V}t\]
\[\varepsilon _{U}(\varepsilon_{\textsf{Emp}}t)\leadsto \varepsilon _{U}t\]
\[\delta x . u  y .  v ( \varepsilon _{R + S}t) \leadsto \varepsilon _{U}t\]
The last case is called the pattern matching: it means that it will maps a value of type \(R + S\) to either \(u\) or \(v\) where \(U\) is the common type of \(u\) and \(v\).
\[\delta x. u  y . v (\iota _{1} r) \leadsto u[r/x],\qquad \delta x. u y . v (\iota _{2} s)\leadsto v[s/y]\]

Here we introduce the sum type, that is \(R + S\), which is a datetype that is often seen in functional programming languages like ML and Haskell.

**** Sum type

\(U\) and \(V\) are two types, and then \(U +V\) is called the sum type. Sum type is the dual of product type \(U \times V\). For more you can check out the definition of Cartesian product and coproduct in category theory.

  - If \(u\) is of type \(U\), then \(\iota_{1}u\) is of type \(U + V\).
  - If \(v\) is of type \(V\), then \(\iota_{2} v\)  is of type \(U + V\).
  - If \(x, y \) are variables of respective types \(R\) and \(S\), and \(u, v\), \(t\) are of respective types \(U\),\(U\), \(R + S\), then
    \[\delta x . u  y . v t\]
    is a term of type \(U\). Furthermore, the occurrences of \(x\) in \(u\) are bound by this construction, as are those of \(y\) in \(v\). This corresponds to the pattern matching in ML.

Schemes like \(\iota_{1}\), \(\iota_{2}\) correspond to rules like \(\vdash \lor\) and \(E \lor\):

\[\prftree[r]{$I_{1}\lor$}{A}{A\lor B}\quad
\prftree[r]{$I_{2}\lor$}{B}{A \lor B}\qquad
\prftree[r]{$E \lor$}{A \lor B}{\prfsummary{[A]}{C}}{\prfsummary{[B]}{C}}{C}\]

As you can see, a term \(u\) where a variable \(x\) occurs can be interpreted as a proof of
\[\prfsummary{[A]}{C}\]
How fascinating! For the commuting conversion in the form of NJ. Check section 4 Commuting conversions.

*** chapter 6: system F

**** generalities

system F is the first work of Girard in logic. And the same phenomenon is discovered at the same time by a computer scientist. We are always discovering things that are identical but with different faces.

System F, contray to simply typed \(\lambda\)-calculus, is constructed around Curry--Howard, as the isomorphic image of inituitionistic second-order propositional calculus. Where we say system F, there is an ambiguity as to the basic connectives: one can choose either a minmal system (based on \(\Rightarrow\), \(\forall\)) or richer systems, involving the connectives \(\lor\), \(\land\), \(0\), \(\exists\). We shall opt for the minimal choice, this for two reasons:

- As usual, these other connectives induce bureaucratic complications. (!?)
- System F is so expressive that the missing connectives casn be translated in the basic version,provided one reliquished commutative conversions; remember that the existence and disjunction properties hold without commutative conversions.

The types of system F are bult from type variables \(X\), \(Y\), \(Z\), \(\dots\) by means of implication and universal quantificatin: thus \(\forall X(X\Rightarrow X)\). The rules of term formation are those of the ~ply typed calculus (Section 5.3.1) to which have been added:

*Generalisation*: if \(t\) is a term of type \(A\) and if the type variable \(X\) is not free in the type of a free variable of \(t\), then \(\Lambda X t\) is a term of type \(\forall X A\).

*Extraction*: if \(t\) is a term of type \(\forall X A\) and if \(B\) is a type, then \(\{t \} B\) is a term of type \(A [B / X]\).

(ps. *Generalisation* is like abstraction, and *extraction* is like application. The notation has this relation \(\lambda \Leftrightarrow \Lambda\), \((\cdot) \Leftrightarrow \{\cdot\}\))

About the free variable in this situation, we list a special case: \(\Lambda X  x ^{X}\). Is this term a legit term?
**** application in system F


System \textbf{F} is like the second order lambda calculus. We introduce another set of notation \(\Lambda\) and \(\{\}\).

\[\lambda x. t,\]

is read as \((\lambda x .t )y \Rightarrow t[y / x]\). So in system F, the notation is
- \(\lambda \Rightarrow \Lambda\)
- \(()\Rightarrow \{\}\)

So an term in system F could be

\[\Lambda X.t,\]
and is read as \(\{\Lambda X. t\} A \Rightarrow t[A / X]\).

**** immediate reduction

There is an immediate reduction in system F, which is like that in lambda calculus

\[\{\Lambda X t\} B \leadsto t [B / X],\]

where \(\leadsto\) means reduction. Church Rosser theorem holds of course.
**** forgetful functor

There is a forgetful functor from system F to simply-typed lamda calculus. And that is obvious.

**** the expressive ability of system F

The ability of system \(F\) is stunning. Let us see how Girard uses system F to express all the connectives like \(\Rightarrow\) and \(\lor\).

*Conjunction*: Define \(A \land B := \forall X((A \Rightarrow (B \Rightarrow X))\Rightarrow X)\). The corresponding operations become

\[
\begin{aligned}
  \langle t, u\rangle  &:= \Lambda X \lambda x ^{( A \Rightarrow (B \Rightarrow X))} ((x)t)u,\\
  \pi_{l} t & := (\{t\} A)\lambda x ^{A} \lambda y ^{B}x,\\
  \pi_{r} t & := (\{t\} B)\lambda x ^{A} \lambda y ^{B}y.
\end{aligned}\]

And we see how system F construct a type easily.

*Disjunction*: Define \(A \lor B:= \forall X((A \Rightarrow X) \Rightarrow ((B \Rightarrow X) \Rightarrow X))\). The corresponding operations become

\[
\begin{aligned}
  \iota_{l} t                 & := \Lambda X \lambda x ^{A \Rightarrow X} \lambda y ^{B \Rightarrow X} (x)t, \\
  \iota_{r} t                 & := \Lambda X \lambda x ^{A \Rightarrow X} \lambda y ^{B \Rightarrow X} (y)t, \\
  \delta(x ^{A} u)(y ^{B} v)t & := ((\{t\}C)\lambda x^{A} u)\lambda y ^{B} v.
\end{aligned}\]

*Absurdity*: Define \(0 := \forall XX.\) And

\[\emptyset ^{A} t := \{t\} A.\]

This translation verifies nothing (there are only commutative rules); its only virtue is to exist!

*Existence*: Define \(\exists X A := \forall Y(\forall X(A \Rightarrow Y)\Rightarrow Y)\). Existential types are not attractive enough to spend much time with them; one can toy with writing schemas of term construction corresponding to the rules

\[
\prftree[r]{$(\exists _{2} I)$}
        {\prfsummary{}{A[B / X]}}
        {\exists X A}
        \qquad
\prftree[r]{$(\exists _{2} E)$}
        {\prfsummary{}{\exists X A}}
        {\prfsummary{[A]}{B}}
        {B}\]

and the reduction corresponding to

\[
\prftree[r]{$(\exists _{2} E)$}
        {
          \prftree[r]{$(\exists _{2} I)$}
                  {\prfsummary{}{A[B/X]}}
                  {\exists X A}
        }
        {\prfsummary{[A]}{C}}
        {\prfsummary{C}{}}
        \qquad
        \raisebox{40pt}{$\leadsto$}
        \qquad
        %%
\prfsummary{\prfsummary{}{A[B/X]}}
           {\prfsummary{C}{}}\]

**** TODO free structure using system F

We can use system F to create some type (structure). I still don't really know why they are called free structure.

*** chapter 7: CCC
**** pole and polar
***** def

Given a binary operation \(+\colon A \times B \to C\), and a subset of \(C\), namely \(P\) (the pole), we can give the polar set \(X^{p}\) with the respect to a subset of \(A\), namely \(X\):

\[
X^{p} := \{y \in B \semicolon \forall x \in X, x + y \in P\}
\]

***** some properties

We write \(\langle a, b\rangle\) as \(a * b\). Don't want to bother to type langle and rangle.

****** 1. \(X \subset X^{pp}\)

Proof. if \(x \in X\) we need to prove that \(\forall y \in X^{p}\), \(x * y \in P\).

Let us look at \(y \in X^{p}\), \(\forall x' \in X, x' * y \in P\). So of course \(x * y \in P\) is valid.

****** 2. \(X^{pp}\) is the smallest polar set that containing \(X\).

Proof. we need to prove that if \(Z\) is polar set that containing \(X\), then \(Z\) also containing a polar set \(X^{pp}\).

Let us say that \(Z = (Z^{-p})^{p}\), here \(Z^{-p}\subset B\). All we have is that if \(x \in X\) then \(x \in Z\), which is \(X \subset Z\). We need to prove that if \(x \in X ^{pp}\) then \(x \in Z\). Or we can prove that if \(x \notin Z\) then \(x \notin X^{pp}\). Let us go with the latter:

If \(x \notin Z\), then \(\exists z \in Z^{-p}\), that \(x * z \notin P\).

How can we prove that \(x \notin X ^{pp}\)? Let us say that if \(x \in X^{pp}\), then therefore \(\forall y \in X^{p}\), \(x * y \in P\). So we know that \(z\) in \(Z^{-p}\) is =not= in \(X^{p}\).

Consequently, \(z\) suited that there is a \(\exists x' \in X\), \(x' * z \notin P\). However, at the same time, \(z \in Z^{-p}\), then for  \(\forall x'' \in Z\) also include those \(\forall x'' \in X\), that \(x'' * z \in P\). A contradiction is derived from \(x \in X^{pp}\).

The formula is that if \(x \notin Z\) and if \(x\in X^{pp}\) there would be a contradiction.

So \(x\in X^{pp}\Rightarrow x\in Z\) which implies \(X^{pp}\subset Z\). And because we don't have pre-set condition of \(Z\), \(\forall Z \supset X, Z \supset X^{pp}\)

The smallest polar set is \(\bigcap Z = X^{pp}\). The equation holds because \(X\) is finite, the number of \(Z\) is limited.

****** 3. \(X^{p} = X^{ppp}\)

Proof. \(X\subset X^{pp}\) is enough to prove \(X^{p}\subset X^{ppp}\). And then we need \(X^{ppp}\subset X^{p}\).

We say like this \(X\) is a polar set, then \(X^{pp} \subset X\).

Let us say \(x \in X^{pp}\) we need \(x \in X\). Or we can say \(x \notin X\), then we need \(x\notin X^{pp}\).

\(x\notin X\), then \(\exists y \in X^{-p}, x * y \notin P\), thus (because \(X^{-p} \subset X^{p}\)) \(\exists y \in X^{p}, x * y \notin P\), which \(x \notin X^{pp}\).

***** connection

- \(A \Rightarrow \neg\neg A\)
- \(A \Rightarrow \neg B \vdash \neg\neg A \Rightarrow \neg B\)
- \(\neg A \Leftrightarrow \neg\neg\neg A\)


First one: \(A \vdash \neg\neg A\)

\begin{prooftree}
\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\end{prooftree}

Second one: \(A \Rightarrow \neg B \vdash \neg\neg A \Rightarrow \neg B\)

\begin{prooftree}
\AxiomC{\(A \vdash A\)}
  \AxiomC{\(B \vdash B\)}  \AxiomC{\(0 \vdash 0\)}
  \BinaryInfC{\(\neg B, B \vdash 0\)}
\BinaryInfC{\(A\Rightarrow \neg B, A, B \vdash 0\)}
\doubleLine
\UnaryInfC{\(A\Rightarrow \neg B, \neg\neg A, B \vdash 0\)}
\UnaryInfC{\(A\Rightarrow \neg B, \neg\neg A \vdash \neg B\)}
\UnaryInfC{\(A\Rightarrow \neg B \vdash \neg\neg A\Rightarrow\neg B\)}
\end{prooftree}

Third one: \(\vdash \neg A \Leftrightarrow \neg\neg\neg A\)

\begin{prooftree}
  \AxiomC{ }
  \UnaryInfC{\(\neg A\vdash\neg A\)}
    \AxiomC{ }
    \UnaryInfC{\(0 \vdash 0\)}
  \BinaryInfC{\(\neg A, \neg \neg A \vdash 0\)}
  \UnaryInfC{\(\neg A\vdash \neg \neg\neg A\)}
  \UnaryInfC{\(\vdash \neg A \Rightarrow \neg\neg\neg A\)}

\AxiomC{ }
\UnaryInfC{\(A\vdash A\)}
  \AxiomC{ }
  \UnaryInfC{\(0 \vdash 0\)}
\BinaryInfC{\(A, \neg A \vdash 0\)}
\UnaryInfC{\(A\vdash \neg\neg A\)}
\doubleLine
\UnaryInfC{\(A, \neg\neg\neg A \vdash 0\)}
\UnaryInfC{\(\neg\neg\neg A\vdash \neg A\)}
\UnaryInfC{\(\vdash \neg\neg\neg A \Rightarrow \neg A\)}

\BinaryInfC{\(\vdash \neg A \Leftrightarrow \neg \neg \neg A\)}
\end{prooftree}

***** translation!

- \(A\) is \(X\)
- \(\neg A\) is \(X^{p}\)
- \(0\) is \(P\)
- \(\vdash\) is "we can find a way to use the left hand side variable to get one right hand side variable"

Examples of translation

- \(A \vdash A\) to \(x \in X \vdash x' \in X\)

  We can find a way to use \(x \in X\) to get a \(x' \in X\): we just use identity function.

- \(A, \neg A \vdash 0\) to \(x \in X, y \in X^{p} \vdash c \in P\)

  We can find a way to use \(x\) and \(y\) to get a \(c \in P\): we just use \(x * y\)

- \(\neg A:= A \Rightarrow 0\) to \(X^{p}\) defined as set of all function that maps \(X\) to \(P\)

  The element \(f_{y}\) in \(X^{p}\) is a way to map \(x\in X\) to \(P\), by

  \[f_{y}\colon X \to P, f_{y}(x):= x * y\]
**** [#A] three layers :text:

Instead of the usual explanation of logic with its infinity (transfinite, but /predicative/, they say: see Section 7.B.4) of /matrioshka/-turtles, one will modestly content oneself with three foundational layers, three undergrounds not at all (meta-)isomorphic. Layer--1 will be the level of truth, layer--2 the level of /functions/, layer--3 the level of actions.

**** the first underground

*Sense and denotation*

Frege, the founder of modern logic, was surely a damned essentialist: witness his contempt for the geometrical ideas of Riemann – whose /Habilitationschrift/ anticipated, in the middle of the XIXth century, the theory of general relativity.

His opposition between /sense/ (implicit) and /denotation/ (explicit) is typical of a not too hot approach to logic. For instance, the two expressions « the morning star » and « the evening star » have different senses, but the same /denotation/, Venus. In this line of thought, logic appears as a sort of « calculus of denotations »: a theorem (whose sense is anything except « true ») has the same denotation as « true »: the proof is a way to make this denotation explicit.

In the same way, one can say that the equality \(t=u\) is interesting only because it is not an /identity/, that \(t\) and \(u\) are distinct /at the level of sense/.

This thought quickly finds its limitations which are those of the dichotomy subject/object. Everything takes place in a universe where the subject (which will become a formal system) and the object (a model, therefore a set) answer to each other without ever meeting. Completeness/soundness establishes a sort of duality, between proofs of \(A\) and models of \(\neg A\)

*Soundness*: if one has both a proof of \(A\) and a model of \(\neg A\), then... contradiction.

*Completeness*: proofs and models are polar in this duality.

**** layer--1

Layer--1 is conceptually very poor: truth, consistency. With a big effort, one arrives at admissible rules: « if A is provable, B is provable ». The $1000 question: find the relation between admissible rules and logical implication... how bleak!

I have a propensity to believe that an interpretation confined to the « first underground », the layer « true/provable », is quite sufficient in that case. Indeed, classical logic rests upon a duality with an empty pole, which only recognises provable/consistent and succeeds in this way in justifying biased principles such as the excluded middle. It is therefore likely that the search for fine grain interpretation of classical proofs belongs to the realm of /methodological/ mistakes... A non-dogmatic viewpoint, subject to contradictory discussion: I didn’t say « technical baloney » or « triviality », since the works on classical proofs are anyway worthy of interest.

**** the second underground: covenant

*The covenant*. In the same order of thought, I think that it is a methodological mistake (!?) to seek /semantics/ for intuitionistic or linear logics.

It is however technically possible: Kripke or topological models in the intuitionistic case (Section 4.E); phase models in the linear case (Section 10.1). In the latter case, models /even/ turned out to be technically useful, witness for instance certain results of Lafont [70]. This being said, technical usefulness is not a /guarantee/ of sense: one should then take seriously the paraconsistent system used by Rosser in his symmetrisation of Gödel’s theorem (Section 2.D.3) (ps. Rosser's variant). The question is not whether one has the right to use models outside classical logic, the answer being obviously « yes »; it is whether this kind of explanation is /appropriate/: the answer is clearly « no ».

Indeed, if we stay within the opposition true/provable, there is little, except consistency, to satisfy our hunger. But what is a consistent intuitionistic theory, which however admits a Kripke model? A nothing, a meaningless doohickey: for instance classical logic is a consistent extension of intuitionistic logic, so what? It is the place to introduce the idea of a /covenant/ – which will eventually lead us to refine the duality sense/denotation.

The /covenant/ of a formal system can be /plausibility/. It is a judiciary version of logic – « what I say is not false » – this is the one prevailing in front of a tribunal, every defendant being supposedly innocent; one should rather say /not-guilty/, since, among all those lifetime senators that escape jail to the benefit of doubt, there must surely be a couple of criminals...

*Plausibility* is the existence of a model, or, in an equivalent way, consistency: it is the /classical/ covenant, but it is not the only possible one. Think for instance of a bank; if the bank says: « you have got $1000 », we don’t only want it to be plausible, we also want to know that we can get these $1000. By the way, everybody knows people who are expert at promising without paying: those are adepts of classical logic, since it is exactly what happens with the excluded middle:

*System*: \(A \lor \neg A\)

*I*: I don't believe in this.

*System*: If both are false, \(A\) is false, hence \(\neg A\) is true.

*I*: Yes indeed!

*System*: But you told me that \(\neg A\) is false.

*I*: I give up, you are too smart.

This discussion with an expert in sophisms leaves an unpleasant after-taste: indeed the contradictor gets mixed up, but the system does not argue earnestly.

A /covenant/ better adapted to banking style realities is therefore the following: if one announces an existence, one must be able to find a witness. For instance, if one says that « there are weapons of mass destruction », one must be able to exhibit them, since one cannot be happy with the first underground, with the classical version: « he who says the contrary is part of the Axis of Evil ». The exigency of /testimony/ must not be confused with a professed /explicit deduction/, of which we already exposed the oxymoronic character (Section 4.2.3) (ps. the discussion about the fine property of intuitionistic logic about disjunction and conjunction). A bank is not supposed to keep money: it should make it circulate; otherwise it is styled differently: it is called a miser. If one asks a bank for one’s money, it should yield it, even if it takes some time; the failure to do this is known as /bankruptcy/, the financial form of inconsistency.

One therefore arrives at the following covenant: if I prove a disjunction \(A \lor B\), I must be able to justify one of the two sides. This is why the only /methodologically sound/ notion of intuitionistic consistency is that of a theory consistent in the usual sense, but also satisfying the properties of existence and disjunction.

Let us come back to the /fregean/ paradigm – to divert it from its setting subject/object to a setting that would rather be subject/subject. A proof has a /sense/ and a /denotation/; the denotation makes explicit the data linked to existence and disjunction. Logical operations should therefore be interpretable as operations on this implicit contents.

(ps. for sense and denotation, see Proofs and Types Chapter 01)

**** category-theoretic reading

It is what is done by the functional interpretation of Chapter 5, of which we shall restrict the scope. Logic now belongs in a /category/ whose « objects » are the formulas and whose /morphisms/ are the proofs; the details will follow later. For the moment, we content ourselves with the observation that the pair morphism/object is clearly more interesting than the pair proof/model subject/object – of the classical world. The rule of /Modus Ponens/, or rather the transivity of implication, the /syllogism/, becomes the /composition/ of morphisms:

\[
\begin{tikzcd}
  A \arrow[rr, "g \circ f"]
    \arrow[rd, "f"]
    && C \\
  & B \arrow[ru, "g"]
\end{tikzcd}\]
Let us compare this to the « first underground » reading. In the years around 1920, Łukasiewicz expalined the transitivity of implication by the transitivity of inclusion: \(A \subset B \subset C\), then \(A \subset C\). The height of derision: it is the transivity of implication explains the transitivity of inclusion, not the other way around!

**** Commutations

If classical logic, i.e., the interpretation by provability/consistency, were really satisfactory, we would have a general completeness theorem, not only for predicate calculus. Now, there is nothing of the like; the stumbling block being the incompleteness theorem, more precisely the fact that:

#+begin_quote
  Provability does not commute with negation.
#+end_quote

To make provability and negation commute is obviously a procedural, cognitive idea, since opposes a strct dichotomy subject/object. It is ven a good idea, provided one changed everything, from the cellar to the attic, only retaining a lax setting: we witnessed the ruination of epistemic, non-monotonic logics -- not to speak of the procedural negation PROLOG (Section 4.D.4) --, all based upon an uncouth commutation.

In general, the idea of making proofs and locial connectives commute is excellent and if one sticks to operations less « loaded » than negation, plausible. In this way, intuitionism realises -- at the second underground -- a commutation between /proof/ and /disjucntion/. To prove \(A \lor B\) is to prove \(A\) or to prove \(B\). W.r.t. Tarski, one has replaced truth with proof. The « or » of « or prove » is a /procedural/ disjunction, operating one the proof itself. Starting with this idea, one can write deductive logical rules (the system NJ) and discover that these logical rules actually enjoy the disjuction property. in other terms, one has an equivalence between the /rules of logic/ and the /logic of rules/.

This equivalence is not the result of a discretionary action: for instance one could not have declared (see /supra/) that « to prove \(\neg A\) is not to prove \(A\) ». It results from a deep /equilibrium/ expressed by the theorem of normalisation of system NJ.

**** The third underground

**** CC

***** categories

\(\mathsf{C}\) is a cat.

- Obj: \(\text{Obj}_{\mathsf C}\) or \(|\mathbf{C}|\) (we prefer the former)
- Hom: \(\text{Hom}_{\mathsf C}(A, B)\) or \(\mathbf{C} (A, B)\)
- composition: \(f \in \text{Hom}_{\mathsf C}(A, B)\) and \(g\in \text{Hom}_{\mathsf C}(B ,C)\), then \(g \cdot f\) in \(\text{Hom}_{\mathsf C}(A, C)\).
- associativity \(f (g h) = (fg)h\)
- id: there is always \(\mathrm{id}\) in \(\text{Hom}_{\mathsf C}(A,A)\).

***** Functor

A functor \(F\) from \(\mathsf C\) to \(\mathsf D\) is two collections of mapping

- From \(\text{Obj}_{\mathsf C}\) to \(\text{Obj}_{\mathsf D}\)
  - satisfy that \(\text{id}\) is mapped to \(\text{id}\) in \(\mathsf D\)

- From \(\text{Hom}_{\mathsf C}(A , B)\) to \(\text{Hom}_{\mathsf D}(FA, FB)\)
  - satisfy that \(F (g h) = F g F h\)

Also the diagram commutes:

\[\begin{tikzcd}
A \arrow[r, "f"]
  \arrow[d, "F"]   & B \arrow[d, "F"] \\
FA\arrow[r, "Ff"]  & FB
\end{tikzcd}\]

***** natural transformation

Functors naturally forms a category where \(\text{Obj}\) are all the functors, and the morphisms are called natural transformation.

natural transformation is a transformation between functors. Here a natural transformation \(\theta\) from \(F\) \(G\).

For every \(\text{Obj}\) in \(\mathsf C\), say \(A\), there is morphism \(T(A)\) in \(\text{Hom}_{\mathsf D}(FA, GA)\), that makes the diagram commutes:

\begin{displaymath}
\begin{tikzcd}
FA \arrow[r, "F(f)"]
   \arrow[d, "T(A)"] & F B \arrow[d, "T(B)"]\\
GA \arrow[r, "G(f)"] & GB
\end{tikzcd}
\end{displaymath}

***** cartesian product

First we need the definition of cartesian product. Here we use universal properties.

A cartesian product \(A\times B\) is a terminal object in followingly constructed categories:

First. Obj in this category is defined as an object \(C\) in \(\mathsf C\), and a pair of morphisms in \(\text{Hom}_{\mathsf C}(C, A)\) and \(\text{Hom}_{\mathsf C}(C, B)\):

\begin{displaymath}
\begin{tikzcd}
C \arrow[r, "f_{1}"]
  \arrow[d, "f_{2}"] & A\\
B
\end{tikzcd}
\end{displaymath}

Second. Morphism from \(C\) (with \(f_{1}\) and \(f_{2}\)) to \(D\) (with \(g_{1}\) and \(g_{2}\)) is defined as a morphism \(h\) from \(\text{Hom}_{\mathsf C}(C,D)\), such that the diagram commutes:

\begin{displaymath}
\begin{tikzcd}
C \arrow[rrd, bend left, "f_{1}"]
  \arrow[rdd, bend right,"f_{2}"]
  \arrow[rd, "h"]                 & & \\
                                  & D \arrow[r, "g_{1}"]
                                      \arrow[d, "g_{2}"] & A\\
                                  & B
\end{tikzcd}
\end{displaymath}
Okay we can not define the cartesian product in the normal sense:

 we can prove that \(A\times B\) (in the sense of set theory) together with \(\pi_{1}\) and \(\pi_{2}\) are the terminal object in the category above, that is for every obj \(C\), there is unique Hom \(h\) from \(C\) to \(A\times B\).

***** cartesian and product

In the sense of set theory, product is cartesian product. However, not every category is \(\mathsf{Set}\), so cartesian product as a concept in set theory does not apply in other thing.

Product is sometimes called direct product. There is also direct sum. And there is also tensor product. They are all different things. One can check for sometime articles. In physicist call tensor product as «direct product», absolutely brain dead behavior.

***** cartesian category

A cartesian category is such category where the terminal obj described above has been given to us: for every pair of obj \(A, B\), there is \(A\times B\) in the category.

***** before diving into CCC

Why we need cartesian category. Because we treat the conjuction of formula \(A \land B\) as \(A \times B\) in the sense of set theory.

We need a category where \(A\) \(B\) are "formula", \(A \land B\) should also be "formula".

**** CCC

***** before diving into CCC

so in the section above, we need CC for such rule: \(A\), \(B\) in \(\text{Obj}_{\mathsf C}\) then \(A\times B\) in \(\text{Obj}_{\mathsf C}\).

Here we have another one, we call exponential: \(A\), \(B\) in \(\text{Obj}_{\mathsf C}\), so \(A\Rightarrow B\) is in \(\text{Obj}_{\mathsf C}\), however, using the notation from set theory, \(A\Rightarrow B\) is written as \(B^{A}\).

***** exponential

We defined a category.
*Obj*: an object is a diagram:

\begin{displaymath}
\begin{tikzcd}
C\times A \arrow[d, "f"]\\
B
\end{tikzcd}
\end{displaymath}

*Hom*: a hom is a commutative diagram, where \(\lambda(f)\) is important:

\begin{displaymath}
\begin{tikzcd}
C\times A \arrow[rr, "\lambda(f)\times \mathrm{id}"]
          \arrow[rd, "f"]    & & D \times A
                                   \arrow[ld, "g"] \\
                             & B
\end{tikzcd}
\end{displaymath}

As you can guess \(B^{A}\) together with (evaluation) \(\epsilon\) is the terminal (to be exact, the terminal is noted as \(B^{A}\) and \(\epsilon\) if they exist).

**** examples of CCC

  1. \(\mathsf {Set}\). Duhhh, obviously.
  2. Scott Domain. We can use sequent calculus to define Scott domain instead
     of topological spaces.
  3. Scott Domain is crucial for understanding coherent space in the future
      section.

**** scott domain described using logic

A scott domain is a pair \((X, \mathcal F)\), where \(X\) is a set, \(\mathcal F\) is a set of axioms made of \(x _{1}, \dots x_{n} \vdash x\) and something like \(x_{1}\dots x_{n}\vdash\) (notice this one is empty). Also these axioms are consistent, that is with the logical rules, structural rules and cut rule, one can not prove \(\vdash\).

A =coherent= subset of \(X\) is such subset \(A\), that \(\mathcal F \cup \{ \vdash x\semicolon x \in A\}\) is consistent.

A =saturated= subset is such =coherent= subset \(A\), that can not be «expanded», that is the corresponding axioms \(\mathcal F \cup \{ \vdash x \semicolon x \in A\}\) can not prove a \(\vdash y\) where \(y\) is outside of \(A\).

As a result, for every coherent set \(A\), there can be a «closure», noted as \(\bar A\). And here we use a new notation \(A \sqsubset_{\mathcal F} X\) which means \(A\) is a saturated subset of \((X,\mathcal F)\).

A morphism \(\varphi\) from \((X, \mathcal F)\) to \((Y, \mathcal G)\) suits that:

1. \(A \sqsubset X\) then \(\varphi(A)\sqsubset Y\)
2. \(A = \uparrow\bigcup_{i}A_{i}\) then \(\varphi(A) = \uparrow\bigcup_{i}\varphi(A_{i})\)
**** logic in a CCC

A syllogistic's view: view \(\text{Hom}_{\mathsf C}(A, B)\) as the sequent \(A \vdash B\). A proof that proves the sequent is a morphism in \(\text{Hom}_{\mathsf C}(A, b)\). And to generalise the idea of \(\Gamma \vdash A\), we introduce product.

Left rules:
what?

**** \(\eta\)-conversion

We want find something unique in exponential. Given \(B^{A}\) and an object \(C\), we want to find a morphism from \(C\) to \(B^{A}\). Let us say it is \(g\), we can have an equation for \(g\), provided with the diagram:

\[\begin{tikzcd}
C\times A
\arrow[rr, "g\times \mathrm{id}"]
\arrow[rd, "\epsilon (g \times \mathrm{id})"]
                &   & B ^{A} \times A
                      \arrow[ld, "\epsilon"]  \\
                & B
\end{tikzcd}\]
the equation:
\[
g = \lambda (\epsilon \cdot (g \times \mathrm{id}))
\]

if we now treat \(\lambda\) as something we are more familiar with, we have this immediate reduction or \(\eta\)-conversion

\[
g = \lambda x (g)x
\]

Remind me of what \(\lambda\) means in the first place. When given a function \(f\) from \(C\times A\) to \(B\), we have a \(\lambda(f)\) from \(C\) to \(B^{A}\), which means that we can factor a function \(f\) to a \(\lambda(f)\) and an evaluation function \(\epsilon\). How does that transfer from \(\lambda\) to this \(\lambda\)?

So a \(g\) here is function from \(C\) to \(B^{A}\). We can use \(g\) to construct a \(C\times A\) to \(B\). And then we use this constructed result, we can then use \(\lambda\) to find a \(C\to B^{A}\). Who is exactly \(g\) itself, since we know this is a CCC.

Okay I don' know what the hell I am say.

**** surjective pairing in category

the surjective pairing is below equation:

\begin{equation}
(\pi_{1}a, \pi_{2}a) = a
\end{equation}

where \(a\) is a variable of type \(A\land B\), of form \((x, y)\), where \(x\) and \(y\) are of type \(A\) and \(B\) respectively.

here we use the idea of unicity above, we consider the unique Hom as an unknown variable, we can have an equation, provided by the commutative diagram:

\begin{displaymath}
\begin{tikzcd}
C \arrow[rrd, bend left, "\pi_{1}\cdot h"]
  \arrow[rdd, bend right,"\pi_{2}\cdot h"]
  \arrow[rd, "h"]                 & & \\
                                  & A\times B \arrow[r, "\pi_{1}"]
                                              \arrow[d, "\pi_{2}"] & A\\
                                  & B
\end{tikzcd}
\end{displaymath}
the equation is:
\begin{equation}
(\pi_{1} \cdot h, \pi_{2} \cdot h) = h
\end{equation}

here we can learn about the corresponce a little bit more, where a formula in is represented as a hom from \(\text{Hom}_{\mathsf C}(C, A)\), where \(C\) is a random object, and \(A\) is the respective formula. And very true that a \(h\) which is hom in \(\mathrm{Hom}_{\mathsf C}(C , A\times B)\), is treated as an variable for the type \(A \times B\).

Here \(\pi_{1}\cdot h\) is an obvious variable for formula \(A\), which is constructed via projecting from \(A \land B\).

*** chapter 8: coherent spaces

**** interpretation of logic from Proofs and Types
:PROPERTIES:
:ID:       91e6b93f-7da1-44f0-9c2a-548d2eb54c5d
:END:

The interpretation are something that is very useless. The first idea of interpretation (semantics) is like this one:

  + type = set.
  + \(U \to V\) is the set of all functions (i nthe se-theoretic sense) from \(U\) to \(V\).

This interpretation is all very well, but it does no explain anything. The computationally interesting objects just get drowned in a sea of set-theoretic functions. The function spaces also quicly become enormous.

Kreisel had the follwoing idea (hereditarily effective operations):

  + type = partial equivalence relation on \(N\)
  + \(U \to V\) is the set of (codes of) partial recursive functions \(f\) such that, if \(x U y\), then \(f(x) V f (y)\), subject to the equivalence relation:
    \[f(U \to V) g \Leftrightarrow \forall x , y (x U y \Rightarrow f(x) V g(y))\]

This sticks more closely to the computational paradigm which we seek to model --- a bit too closely, it seems, for in fact it hardly does more than interpret the syntax by itself, modulo some unexciting coding.

Scott's idea is much better

  + type = topological space.
  + \(U \to V\) = continuous functions from \(U\) to \(V\)

Now it is well known that topology does not lend itself well to the construction of function spaces. When should we say that a sequence of functions converges pointwise, or uniformly in some way?

To resolve these problems, Scott was led to imposing drastic restrictions on his toplogical spaces which are far removed from the traditional geometrical spirit of topology (if I remember clearly, scott domain as a topological space is hardly Hausdorff). In fact his spaces are really only partially ordered sets with directed joins: the topology in an incidentalfeature. So it is natural to ask oneself whether perhaps the topological intuition is itself false, and look for something else.

**** the definition of coherent spaces

A coherent space has some components:

  + *Web*: a underlying set namely \(X\).
  + *Coherence*: a reflexive and symmetric relation. I don't know how to typeset
    the symbol yet.
  + *Clique*: a clique \(a\sqsubset X\), is a subset of \(X\), made of pair-wise
    coherent points.

**** the coding of Scott domains

An example here is the cartesian product, namely \(\mathbf{bool}\times \mathbf{bool}\).

And we treat the space as a space with four point, namly: \(v\), \(f\) and \(v'\) and \(f'\). The points are pointwise coherent besides \((v, f)\) and \((v', f')\), and we consider a mapping \(F\) from the coherent space where the objects are cliques---the set whose elements are point-wise coherent.

\begin{align}
F(a)         &= v, \text{if } a \text{ has } v\\
F(\{f, f'\}) &= f\\
F(b)         &= \emptyset, \text{otherwise}
\end{align}

the coding is about representing the \(F\). We need some redundancy. we need to list all the \(a\) in the first equation above. Because in the definition of coherent space, if \(b \subset a\) then something is true also for \(b\). So it becomes

\begin{displaymath}
\begin{aligned}
F(\{v\}) = v\\
F(\{v'\}) = v\\
F(\{v, f'\}) = v\\
F(\{v', f\}) = v
\end{aligned}
\end{displaymath}

This is the coding of \(F\). The redundancy is need. I don't really know what is section is talking about.

**** stable function and stability

here \(X\) and \(Y\) are two coherent spaces, a stable function \(F\) from \(X\) to \(Y\) satisfies:

  + Cliques: if \(a \sqsubset X\) then \(F(a)\sqsubset Y\)
  + Monotonicity: if \(a \subset b \sqsubset X\) then \(F(a)\subset F(b)\)
  + Continuity: \(F(\uparrow \bigcup_{i}a_{i}) = \uparrow \bigcup_{i}F(a_{i})\)
  + Stability: if \(a \cup b \sqsubset X\) then \(F(a \cap b ) = F(a) \cap F(b)\)

Stable order between the stable functions is defined:

Berry order: \(F\sqsubset G\) iff for all \(a \subset b \sqsubset X\), \(F(a) = F(b) \cap G(a)\).

There is a trick here for the stability and berry order. Stability is about two subsets of \(X\), which satisfy \(a \cup b \sqsubset X\), while in Berry order, it is about \(a \subset b \sqsubset X\).
It is clear that these two sets of objects are some how something identical.
\(a \cap b\) in former should be \(a\) in the latter. So we rewrite the latter

\[F(a \cap b) = F(b) \cap G(a \cap b).\]

Why we are talking about this? It is because we want to find out if \(F \sqsubset F\) holds. If \(F \sqsubset F\) holds, then we would have

\[F(a \cap b) = F(b) \cap F(a \cap b) = F(a) \cap F(b)\]

Because for \(F\), \(F(a \cap b) = F(a) \cap F(b)\) holds, the equation above holds.

**** Interlude: the coding of stable function, trace and skeleton

We can define the trace (the old name), or the skeleton of a stable function \(F\). The Trace is significant because it is the coding of a stable function:

Skeletons and stable functions have a /one-one/ correspondence!

And [[*Berry Order][Berry Order]] and [[*linearity][linearity]] can be expressed using skeleton!

**** Berry Order

Berry order: \(F\sqsubset G\) iff for all \(a \subset b \sqsubset X\), \(F(a) = F(b) \cap G(a)\).

**** TODO Stable function and determinism

Stability corresponds to a determinism of computation (not only of its result): when performing a computation, a /well-defined/ part of the data is actually used.
Which is not the case for the « parallel or », since there is an ambiguity as to the information actually needed: the answer is « true » when one of the two arguments is true, which yields two possible ways of acting when both are true.

The stable order appears as the necessary technical companion of stability, in view of the adjunction which defines the function space. Indeed, a stable function from \(X \times Y\) to \(Z\) must appear as a stable function from \(X\) to \(Z ^{Y}\). In (8.4)–(8.6) (PS. [[*the coding of Scott domains][the coding of scott domains]]), compare the unary functions \(F_{\emptyset}(\{v'\}) = \{v\}\), \(F_{\emptyset}(a)= \emptyset\) \((a \neq \{v'\})\), and \(F_{\{v\}}(a) = \{v\}\): since \(F_{ \emptyset} (\emptyset)= \emptyset \neq \{v\}= F_{\emptyset} (\{v '\})\cap F_{\{v\}}(\emptyset)\), \(F_{\emptyset} \sqsubset F_{\{v\}}\) fails. Indeed, these two stable maps are such that \(F_{\emptyset} \subset F_{\{v\}}\), but the minimal data for \(F_{\emptyset}\) are no longer minimal for \(F_{\{v\}}\).

# what the heck

**** Coherent space as CC

And we add product and terminal for CC.

It seems that we can add some structure to make a coherent space a CC.

We add cartesian product in the coherent space.

*Web*:
\[|X\ \& \ Y| := |X| + |Y|\]
*Coherence*:
\[
\begin{aligned}
&(x, 1) \coh _{X \& Y} (x', 1) \Leftrightarrow x \coh_{X} x'\\
&(y, 2) \coh _{X \& Y} (y', 2) \Leftrightarrow y \coh_{Y} y'\\
&(x, 1) \coh _{X \& Y} (y, 2)
\end{aligned}\]

The last line means the relation does /hold/ in any condition.

Proposition: if \(a \sqsubset X\) and \(b \sqsubset Y\) then obviously \(a + b \sqsubset X \ \& \ Y\)

Two projections:

\[
\begin{aligned}
  \pi _{ l} (a + b) = a,\\
  \pi _{r} (a + b ) = b.
\end{aligned}\]

It is obvious that projections are stable (and even linear, see Chapter \(9\)).
Let us proceed with our studious checking: if \(F, G\) are stable functions from \(Z\) into \(X, Y\), then one can define \(F, G\) from \(Z\) into \(X \ \& \ Y\) by

\[(F, G)(c) := F(c) + G(c)\quad (c \sqsubset Z),\]

is immediate that \(\pi _{l} \cdot (F, G) = F , \pi _{r} \cdot (F, G) = G\). Unicity is so obvious that one does not even dare to justify it (!?)

**** TODO Coherent space as CCC
**** proposition 10

Let \(F\), \(G\) be stable functions from \(X\) into \(Y\), let \(A \sqsubset X\) and \(y \in F(A)\). Then:

  * There exists \(a \subset A\), with \(a\) finite, such that \(y \in F(a)\).
  * If \(a\) is chosen minimal, it is minimum, i.e., unique.
  * If \(F \sqsubset G\), then \(y \in G(a)\) and \(a\) remains the minimum choices.

Proof. Write \(A\) as directed union of its finite subsets. Then

\[F(A) = \uparrow \bigcup \{ F(a) \semicolon a \subset A, a\ \text{finite}\}.\]

Which proves (i).

Another choice \(b\subset A\) would yield by stability, since \(a \cup b \sqsubset A\) (?), \(y \in F(a) \cap F(b) = F(a \cap b)\): if \(a\) is minimal, then \(a = a \cap b\), hence \(a \subset b\). Which proves (ii).
# ok, this is weird. Because we choose such b that also has y ∈ F(b), so y ∈
# F(a) ∩ F(b) would be true. While the latter is equal to F(a ∩ b). How to
# interpret a is chosen minimal? Doesn't this is gibberish?

Finally, if \(b \subset a\) and \(y \in G(b)\), then \(y\in G(b) \cap F(a) = F(b)\) hence \(a = b\). Which proves (iii).
# Okay stable function is weird. How is this theorem even related?

**** Skeleton

If \(F\) is a stable map from \(X\) into \(Y\), one defines its skeleton \(\mathrm{Sk}(F)\):

\[\mathrm{Sk}(F):= \{ (a, y) \semicolon y \in F(a) \land \forall b \subsetneq a \ \ y \notin F(b)\}.\]

This is not a matter of graphs. For instance, take the most trivial stable function, the identity function \(\iota_{X}\) from \(X\) into \(X\): its graph is made of the pairs \((a, a)\), where \(a \sqsubset X\); while its skeleton corresponds to hte minimal solutions to \(y \in F(a) = a\): this yields \(\mathrm{Sk}(\iota _{X}) = \{(\{x \}, x) \semicolon x \in |X| \}\).

One now defines the coherent space \(X \Rightarrow Y\). Let us introduce the notation \(x \scoh x'\) for strict coherence, i.e., for \(x \coh x ' \land x \neq x'\).

**** TODO Berry Order and Skeleton
**** Exponential for coherent spaces

We can define \(X \Rightarrow Y\) for two coherent spaces \(X\), \(Y\).

*Webs*: \(| X \Rightarrow Y| := X_{\mathrm{fin}} \times |Y|\), where \(X_{\mathrm{fin}}\) is the set of /finite/ cliques of \(X\).

*Coherence*:

\[\begin{aligned}
  (a , y) \coh _{X \Rightarrow Y} (a' , y') \iff & (a \cup a' \sqsubset X \Rightarrow y \coh _{Y} y')\\
                                                 & \land (a \cup a' \sqsubset X \land a \neq a ' \Rightarrow y \scoh _{Y} y')
\end{aligned}\]

**** Representation, from clique in X ⇒ Y to a stable function from X → Y

Sk defines a bijection between the stable functions from \(X\) into \(Y\) and the clique of \(X \Rightarrow Y\). The reciprocal bijection associates to a clique \(C \sqsubset X \Rightarrow Y\) the stable function \((C) \cdot \) defined by

\[(C) A := \{ y \semicolon \exists a \subset A\ (a, y)\in C \}.\]

Moreover the bijection exchanges the Berry Order and inclusion.

**** Coherent spaces do form a CCC

The proof is in the book.

**** WTF

#+begin_quote
Personally, I find this sort of result /illegible/. One must write it, but not read it, under the penalty of becoming a /bureaucrat/. The real result is the theorem, which does establish the right correspondence. Since this correspondence is natural in the natural sense of the term, it is also natural in the category-theoretic sense.

--Girard
#+end_quote

Girard always has his way with his words.

**** Use Coherent space to interpret system F
**** Embedding

An embedding of \(X\) into \(Y\) is an injective function from \(|X|\) into \(|Y|\) such that \(x \scoh x'\) iff \(f (x) \scoh f(x')\).

**** TODO Contravariant and covariant

Indeed, \(X \Rightarrow Y\) is covariant in \(Y\), cotravariant in \(X\): from stable functions \(f\) from \(X'\) into \(X\) and \(g\) from \(Y\) into \(Y'\), one can pass from \(X \Rightarrow Y\) to \(X' \Rightarrow Y'\) by composition:

\[C \leadsto \mathrm{Sk}(g \circ (C) \cdot \circ f ).\]

*** chapter 9: linear logic

**** before linearity

If we consider the elimination rules as functions from the main premise to the conclusion: \(\pi _{l}\colon X\ \&\ Y \mapsto X\), \(\pi_{r} \colon X \ \& \ Y \mapsto Y\), \((\cdot) a \colon (X \Rightarrow Y) \mapsto Y\), it turns out that they enjoy an additional property, linearity.

**** linearity

The definition of linearity is stable function that preserves coherent unions:

For instance \((C\cup D) a = (C)a \cup (D)a\).

What the hell is this?

**** COH

Defnition 21 (COH). One defines the category *COH* by:

*Objects*: coherent spaces.

*Morphisms*: \(\mathbf{COH}(X, Y)\) consists of the linear functions from \(X\) to \(Y\).

This category satisfies almost everything expected from a category. It is not a CCC, but it is a closed *monoidal* category; moreover, one can reconstitute a structure of CCC in it.

**** linear implication

We use symbol ~\multimap~, which looks like a lolipop, which sounds a little bit erotic.

\[A \multimap B\]

\(F\) from \(X\) to \(Y\) is linear iff its sk is made of pairs \((\{x\}, y)\)

Def 22 If \(X\), \(Y\) are coherent spaces, we define \(X \multimap Y\) by:

*Web*: \(| X \multimap Y | = |X | \times | Y |\)

*Coherence*:

\[
\begin{aligned}
  (x, y) \coh_{X \multimap Y}(x', y')\Leftrightarrow & (x \coh _{X} x' \Rightarrow y \coh _{Y}y') \\
                                                     &   \land (x \scoh _{X} x' \Rightarrow y \scoh _{Y} y').
\end{aligned}\]

**** linear negation

**** duality

**** perfect linear connectives
[2024-10-26 Sat]

Perfect linear connectives are those connectives like linear implication \(\multimap\), and \(\with\) \(\parr\) and so on. As oppose to imperfect linear connectives including \(!\) and \(?\) that is mainly aimed at weakening and contraction rule.

**** multiplicatives

Using the De Morgan laws, we define a conjuction (times, or tensor) \(X \otimes Y := {\sim} (X \multimap {\sim} Y)\) and a disjunction \(X \parr Y := {\sim} X \multimap Y\) (par, or cotensor). These connectives (as well as implication) are called multiplicative, sicne they are based upon the cartesian product of the webs.

Definition 25 (Multiplicatives). If \(X\), \(Y\) are coherent spaces, we define the coherent spaces \(X \otimes Y\), \(X \parr Y\):

\[
\begin{aligned}
{} | X \otimes Y | = | X \parr Y | & := |X| \times |Y|,\\
(x, y) \coh _{X \otimes Y} (x', y') & : \Leftrightarrow x \coh_{X} x' \lor y \coh_{Y} y', \\
(x, y) \scoh_{X \parr Y}(x', y') & : \Leftrightarrow x \scoh _{X} x' \lor y \scoh _{Y} y'.
\end{aligned}\]

The two defnitions are related /modulo/ De Morgan:

\[
\begin{aligned}
{\sim}(X \otimes Y) & = {\sim} X \parr {\sim} Y.\\
{\sim}(X \parr   Y) & = {\sim} X \otimes {\sim} Y, \\
X \multimap Y & = {\sim} X \parr Y = {\sim} (X \otimes {\sim} Y).
\end{aligned}\]

which exchanges conjunction and disjunction, \(\coh\) and \(\scoh\).

One verifies certain canonical isomorphisms:

*Commutativity*: \(X \otimes Y \simeq Y \otimes X\), \(X \parr Y \simeq Y \parr X\), to which one can relate \(X \multimap Y\simeq {\sim} Y \multimap {\sim} X\).

*Ass*: \(X \otimes (Y \otimes Z) \simeq (X \otimes Y) \otimes Z\), \(X \parr (Y \parr Z) \simeq (X \parr Y)\parr Z\), to which one can relate \(X \multimap (Y \multimap Z) \simeq (X \otimes Y) \multimap Z\), \(X \multimap (Y \parr Z) \simeq (X \multimap Y)\parr Z\).

*Neu*: the one-point sapce, denoted by, depending on the context, \(1\) or \(\simperp\), is neutral, i.e., \(X \otimes 1 \simeq X\), \(X \parr \simperp \simeq X\), to which one relates \(1 \multimap X \simeq X\) and \(X \multimap {\simperp} \simeq {\sim} X\).

In relation to our considerations about the second undergound (or layer), note that two isorphic coherent spaces (\(1\) and \(\simperp\)) have a very different status at layer \(-1\). At the categorical layer, the distinction between them is only a preciosity. The same problem of /unfaithfulness/ will be found again with the adtive neutrals; more dramatically, since hte identification between \(0\) and \(\top\) would produce a logical inconsistency.

**** imperfect connectives
[2024-10-26 Sat]

Stability strikes back. We could try to give a categorical interpretation of intuitionistic logic in the linear world, thus reading a proof of \(A_{1},\dots A_{n}\) \(\vdash B\) as a multilinear function. Since we shall soon do it in earnest, let us forget the details and observe that everything would work well, if not for the /structural/ rules of weakening and contraction. In what follows, A and B are supposedly interpreted by coherent spaces \(X\) and \(Y\):

*Weakening*: reduced to its simplest expression, weakening corresponds to « material implication »: if I have \(B\), then I still have \(B\) under hypothesis \(A\). If a proof of \(B\) has been interpreted by a clique \(b \sqsubset Y\), then « \(B\) under hypothesis \(A\) » will be the constant function \(F(a) = b\). Such a function is stable, but not linear: indeed \(F(\emptyset)\neq \emptyset\).

*Contraction*: reduced to its simplest expression, contraction corresponds to the reuse of hypotheses: if I got \(B\) under the hypotheses \(A\) and \(A\), then I can get \(B\) under hypothesis \(A\). In other terms if \(f(x, y)\) is a bilinear function from \(X\), \(X\) into \(Y\), then \(f(x, x)\) should be linear ... Baloney! Everybody knows that it is /quadratic/.
# what?

At the level of the skeleton, constant functions induce elements of the form \((\emptyset , y)\) and quadratic functions induce elements of the form \((\{x, x'\}, y)\); more generally, the unbridled use of structural rules produces elements of the form \((a, y)\), where \(a\) is a finite clique of \(X\). A stable function is a sort of polynomial of unknown degree, this is why one easily reaches /analytic/ functions (Sections 8.A.2 and 15.A).

**** Pons Asinorum
[2024-10-28 Mon 17:37]

The *Pons Asinorum*, the « bridge of asses » (!?), is a rhetorical figure of medieval pedagogy: the student (the ass) is brought to the middle of the bridge of knowledge with the help of a simple, but striking, example.

Linear logic, whose main value rests in its /perfect fragment/ – whose category theoretical structure we just described –, would be no more than another /paralogic/ – not as hateful as paraconsistent, epistemic, non-monotonic or fuzzy logics, but a paralogic anyway – if it were reduced to its perfective, perfect, part. The absence of relation to usual logic, classical or intuitionistic, fatally leads to /sectarianism/ and /marginalisation/: witness the fate of the aforementioned paralogics.

The climacteric remark is that usual (i.e., intuitionistic) implication is a /particular case/ of linear implication.

Def 27 (Of course!). If \(X\) is a coherent space, we define \(!X\) as follows:

\[\begin{gathered}
  {} |! X| = X_{\mathrm{fin}}, \\
  a \coh_{! X} a' \Leftrightarrow a \cup a ' \sqsubset X.
\end{gathered}\]

Def 28 (Why not?). If \(X\) is a coherent space, we define \(?X\) as follows:

\[
\begin{gathered} |?X| = ({\sim} X)_{\mathrm{fin}},\\ a \scoh _{?X} a ' \Leftrightarrow a \cup a ' \not\sqsubset {\sim} X,
\end{gathered}\]

which is not a legible definition (!?); I only fabricated a dua;:

\[
\begin{aligned}
  {\sim} ! X & = {?} {\sim} X, \\
  {\sim} ? X & = {!} {\sim} X.
\end{aligned}\]

**** Isomorphism for imperfect connectives

There is an isomorphism. It explain why « ! » and « ? » are styled exponentials:

\[
\begin{gathered}
  ! (X \with Y) \simeq {!} X \otimes {!} Y, \\
  ? (X \otimes Y) \simeq {?} X \parr {?} Y.
\end{gathered}\]

# what the hell is this?
Since a (finte) clique of \(X \with Y\) decomposes as \(a + b\) where \(a, b\) are (finte) cliques of \(X\) and \(Y\). To these isomorphisms we can relate the 0-ary case:

\[
\begin{gathered}
  ! \top \simeq 1 , \\
  ? 0 \simeq {\simperp}.
\end{gathered}\]
Indee, I know no other canonical isomorphism in COH (except mistakes of logic like \(\top \simeq 0\)). Thus, the adjunction \(\with\) / \(\Rightarrow\) is a consequence of our list of isomorphisms:

\[
\begin{aligned}
  X \Rightarrow ( Y \Rightarrow Z) & = {!} X \multimap (! Y \multimap Z)\\
                                   & \simeq {!} X \otimes {!} Y \multimap Z \\
                                   & \simeq {!} (X \with Y) \multimap Z \\
                                   & \simeq (X \with Y) \Rightarrow Z.
\end{aligned}\]
# what the hell is this?

**** LL systems generalities

Contrary to classical logic, linear logic admits a non-degenerate category-theoretic interpretation. But, due to the left/right symmetry expressed by linear negation, it cannot be written in « natural deduction » style. We are therefore led to express linear logic in the setting of sequent calculus, which appears, at least at first sight, as a regression.
# Left/right symmetry is very interesting. NJ is not symmetric for sure, but
# what about NK?

Constructive linear negation – i.e., the symmetry left/right recovered – enables one to understand differently intuitionistic logic. Before linear logic, one thought that the restriction one formula on the right was the cause of phenomena of the style « disjunction property ». There is now a much better explanation: the absence of structural rules, especially contraction. A prohibition ensured by the intuitionistic maintenance: one must be two to contract. This is why linear logic, with its calculus « everything on the right », will still enjoy the existence and disjunction (\(\oplus\)) properties. We also better understand the reduction at absurdity, /contraposition/: it is wrong in the intuitionistic regime, because of the left contractions/weakenings which produce stable functions which are non-linear, hence with no adjoint. In other words, what is « reprehensible » in the reduction at absurdity is not the fact of assuming \(\neg B\) to get \(\neg A\), it is assuming it twice or more.

Linear Logic is truly issued from the category-theoretic interpretation in coherent spaces. This interpretation, wholly in the second underground, yields no logical indication in the usual sense; for instance, it does not distinguish between the empty space and its negation, while, logically speaking, their identification causes an inconsistency. In other words, the sequent calculus which follows is only approximately founded upon coherent spaces.
# What the hell are you talking about?

**** LL languauges

Since there are twice more connectives that usual, we will choose a right version. Concretely: formulas are built from literals \(p\), \(q\), \(r\), \({\sim }p\), \({\sim} q\), \({\sim}r\), …, i.e., of atomic formulas and their negations and the constants \(1\), \(\bot\), \(\top\), \(0\), by means of the connectives \(!\) and \(?\) (unary) and \(\otimes\), \(\parr\), \(\oplus\), \(\with\) (binary) and the quantifiers \(\forall x A\) and \(\exists x A\). We can also consider, mutatis mutandis, second-order quantifications. We shall not insist too much on the aspect « quantifiers », which is the less innovative aspect of linear logic.

Linear negation is defined by Me Morgan style equations:

\[
\begin{aligned}
  {\sim} 1             & := {\simperp}, \\
  {\sim} 0             & := \top, \\
  {\sim}(p)            & := {\sim} p, \\
  {\sim} (A \otimes B) &  := {\sim} A \parr {\sim} B, \\
  {\sim} (A \oplus B)  & := {\sim} A \with {\sim} B, \\
  {\sim}(! A)          & := {?} {\sim} A, \\
  {\sim}(\exists x A)  & := \forall x {\sim} A,
\end{aligned} \qquad \begin{aligned}
  {\sim} \simperp      & := 1, \\
  {\sim} \top          & := 0, \\
  {\sim} (\sim p)      & := p, \\
  {\sim} (A \parr B)   & := \sim A \otimes \sim B, \\
  {\sim} (A \with B)   & := \sim A \oplus \sim B, \\
  {\sim} (? A)         & := ! \sim A, \\
  {\sim} (\forall x A) & := \exists x \sim A.
\end{aligned}\]

# what the heck!

Linear implication is defined as

\[A \multimap B := \sim A \parr B.\]

The sequents are of the form \(\vdash \Delta\); bilateral sequents \(\Gamma \vdash \Delta\) can be translated as \(\vdash \sim \Gamma, \Delta\).

**** Question: How linear logic is special

If you are clueless as I am right now, how about trying to find the answer of this question.

**** Some of the rules in LL

identity:

\[
\prftree[r]{\quad(\textit{identity})}
        {}
        {\vdash {\sim} A, A}
\qquad
\prftree[r]{\quad(\textit{cut})}
        {\vdash \Gamma, A}
        {\vdash {{\sim} A}, \Delta}
        {\vdash \Gamma, \Delta}
        \]

structure:

\[
\prftree[r]{\quad(\textit{exchange})}
        {\vdash \Gamma}
        {\vdash \Gamma'}\]
Logic:

\[\prftree[r]{\quad(\textit{one})}
          {}
          {\vdash 1}\]

还有很多就不列了，累死。

**** symmetric monoidal categories

*** interlude: Coherent Semantics
:PROPERTIES:
:ID:       b2b2438c-1269-4f20-9fad-1ee9dab934be
:END:

Before reading this chapter, we should know that is coherent space.

\textsf{Emp} is naturally interpreted as the coherence space \(Emp\) wheore web is empty, and the interpretation of \(\varepsilon_{U}\) follows immediately.

The sum, on the other hand, poses some delicate problems. When \(\cal A\) and \(\cal B\)  aare tow coherent space, there is jut one obvious of sum, namely the direct sum introduced below. Unfortunately, the \(\delta\) scheme is not interpreted. This objection also holds for other kinds of semantics, for example /Scott domains/.

After examining and rejecting a certain number of fudged alternatives, we are led back to the original solution, which would work with /linear/ functions (i.e., preserving unions), and we arrive at a representation of the sum type as:

\[{!}\mathcal A \oplus {!} \mathcal B\]

It is this decomposition which is the origin of linear logic: the operation \(\oplus\) (direct sum) and \({!}\) (linearilisation) are in fact logical operations in their own right.

[[id:bf8931a1-dd75-4a68-8a7c-ce74b6fe4a2b][new connectives in LL]].

We defining *direct sum* as something that we already seen: The web is the disjoint union \(| X \oplus Y | = |X| + |Y|\), and the two point in the web are coherent iff they are in the same coherent space and are coherent. To be honest, I am a little bit of confused, because this is the same definition as tensor product \(\with\).

We have stable function \(\mathcal Inj^{1}\) and \(\mathcal Inj^{2}\), from \(\cal A\) to \(\mathcal A \oplus \mathcal B\), and from \(\mathcal B\) to \(\mathcal A \oplus \mathcal B\):
\[\mathcal I nj ^{1} (a) = \{ 1 \} \times a,\qquad \mathcal I nj ^{2}(b) = \{ 2 \} \times b\]
but this definition coincide on emptyset. Domain-theoretically, this amounts to taking the disjoint union with the ∅ element identified, so it is sometimes called an /amalgamated/ sum.

A first solution is given by adding two tags \(1\) and \(2\) to \(| \mathcal A \oplus \mathcal B|\) to form \(\mathcal A \amalg \mathcal B\): \(1\) is coherent with the \((1, \alpha)\) but not with the \((2, \beta)\) and likewise \(2\) with \((2, \beta)\) but not with the \((1, \alpha)\). What the hell?

We can then define:
\[\amalg ^{1} (A) = \{ 1 \} \cup \mathcal I nj ^{1}(a), \qquad \amalg ^{2}(b) = \{ 2 \} \cup \mathcal I nj ^{2}(b)\]
What is this \(H\) here?
Now, from \(F\) and \(G\), the casewise definition is possible:
\[H(\amalg ^{1}(a)) = F(a),\qquad H(\amalg ^{2}(b)) = G(b)\]
\[H(c) = \emptyset, \quad \text{if } c \cap \{1,2\}= \emptyset\]
In other words, in order to know whether \(\gamma \in H (c)\), we look inside \(c\) for a tage \(1\) or \(2\), the nif we find one (say \(1\)), we werite \(c = H ^{1}(a)\) and ask whether \(\gamma \in (G)\).

This solution interprets the standard conversion schemes:
\[\delta x. u y. v(\iota ^{1} r) \leadsto v [r / x] ,\qquad \delta x . u y . v(\iota _{2} s) \leadsto v[s / y]\]
What?
However the interpretation \(H\) of the term \(\delta x . (\iota ^{1} x) y. (\iota ^{2} y) z\), which is defined by
\[H (\amalg ^{1}(a)) = \amalg ^{1}(a),\qquad H (\amalg ^{2}(b)) = \amalg ^{2}(b)\]
\[H(c) = \emptyset, \quad \text{if } c \cap \{1,2\} = \emptyset\]
does not always satisfy \(H(c) = c\). In fact tis equation is satisfied only for \(c\) of the form \(\amalg ^{1}(a)\) , \(\amalg ^{2}(b)\) or \(\emptyset\).

*** interlude: What is linear logic

Proofs and Types
author: Yves Lafont

**** forewords

Linear logic was originally discovered in coherence semantics (see chapter 12). It appears now as a promising approach to fundamental questions arising in proof theory and in computer science.

In ordinary (classical or intuitionistic) logic, you can use an hypothesis as many times as you want: this feature is expressed by the rules of weakening and contraction of Sequent Calculus. There are good reasons for considering a logic without those rules:

  - From the viewpoint of proof theory, it removes pathological situations
    from classical logic (see next section) and introduces a new kind of
    invariant (proof nets).

  - From the viewpoint of computer science, it gives a new approach to
    questions of /laziness/, side effects and memory allocation [GirLaf, Laf87,
    Laf88] with promising applications to parallelism.

**** classical logic is not constructive

Intuitionistic logic is called constructive because of the correspondence between proofs and algorithms (the Curry-Howard isomorphism, chapter 3). So, for example, if we prove a formula \(\exists n \in \N. P(n)\), we can exhibit an integer \(n\) which satisfies the property \(P\).

**** new connectives in LL
:PROPERTIES:
:ID:       bf8931a1-dd75-4a68-8a7c-ce74b6fe4a2b
:END:

We use two connectives \(\otimes\) and \(\with\). The former is called tensor product and the latter is called direct product. And we have a dual version of these two (the notation is dual as well): \(\parr\) the tensor sum, or the cotensor product, and \(\oplus\) the direct sum.

  - \(\otimes\): *tensor* product
  - \(\with\): *direct* product
  - \(\parr\): tensor sum
  - \(\oplus\): direct sum

Tensor product can be called cumulative conjuction and direct product can be called alternative conjuction.

A linear negation is introduced in order to utilize symmetric:

The linear negation of \(A\) is denoted as \(A ^{\bot}\)

And we have using de Morgans:

\((A \otimes B) ^{\bot} = A ^{\bot} \parr B ^{\bot}\) and \((A \with B) ^{\bot} = A ^{\bot} \oplus B^{\bot}\)
\((A \parr B) ^{\bot} = A ^{\bot} \otimes B ^{\bot}\) and \((A \oplus B) ^{\bot} = A ^{\bot} \with B^{\bot}\)

**** of course! why not?

\[{\sim}(! A) = ? ({\sim} A),\qquad {\sim}(?A) = ! {\sim} A\]

or you can say

\[(! A)^{\bot} = ? A ^{\bot}, \qquad (? A)^{\bot} = ! A ^{\bot}\]

Note that we use right hand rules:

\[
\prftree[r]{!}{\vdash A , ? \Gamma}{\vdash ! A, ? \Gamma}
\quad
\prftree[r]{\sf W?}{\vdash \Gamma}{\vdash ? A , \Gamma}
\quad
\prftree[r]{\sf C?}{\vdash ?A , ?A , \Gamma}{\vdash ?A , \Gamma}
\quad
\prftree[r]{\sf D?}{\vdash A , \Gamma}{\vdash ? A , \Gamma}\]

The last one is called /dereliction/. It is equivalent to the axiom \(B \multimap ? B\), or dually \(! B \multimap B\)

**** representing NJ using linear logic

We have

\[A \land B := A \with B,
\quad A \lor B := {!} A \oplus {!} B,
\quad A \Rightarrow B := {!} A \multimap B,
\quad \neg A = {!} A \multimap 0\]

in such a way that an intuitionistic formula is valid iff its translation is porvable in linear logic (so, for example, derecliction expresses that \(B \Rightarrow B\)).
This translation is in fact used for the coheretn semantics of typed lambda calculus.

It is also possible to add (first and second order) quantifiers, but the main features of linear logic are already contained in the propostional fragment.

Okay my question is that what the hell is this « of course! » and « why not? » used for? I do think that we need to tackle for the nature of indecibility that is brought by weakening and contraction rule.

But can you explain more on that?

**** proof nets, multiplicative linear logic

Here the proof nets are drawn with something I don't know in the book. But anyway, I am drawing the way like /The Blind Spot/ does.

Multiplicative Linear Logic (MLL) is about the proof nets.

Here we shall concentrate on the so-called multiplicative fragment of linear logic, /i.e./, the connectors \(\otimes\), \(1\), \(\parr\), \(\with\) and \(\bot\). In this fragment, rules are consercative over contexts: the context in the conclusion is the disjoin union of those of the premises. The rules for \(\withh\) and \(\top\) are not, and if we renouce these connectors, we must renounce their duals \(\oplus\) and \(0\).

From an algorithmic viewpoint, this fragment is very /unexpressive/, but this restriction is necessary if we wawnt to tackle porblem progressively. Furthermore, multiplicative connectors and rules can be generalised to make a genuine programming language. (ps. I bet it suck egg) (ps. it somehow relates to the concurrency or parallel programming?)


Sequent proofs contain a lot of redundancy: in a rule such as

\[
\prftree[r]{$\parr$}{\vdash A , B, \Gamma}{\vdash A \parr B , \Gamma}\]

The context, namely \(\Gamma\), which plays a passive role, is rewritten without any change. By expelling all those boring contexts, we obtain the /substantifique moelle/ of the proof, called the proof net.

For example, the proof

\[
\prftree[r]{$\parr$}{
  \prftree[r]{}
          {
            \prftree[r]{$\otimes$}
                    {
                      \prftree[r]{$\otimes$}
                              {\vdash A , A ^{\bot}}
                              {\vdash B, B ^{\bot}}
                              {\vdash A \otimes B, A ^{\bot}, B ^{\bot}}
                    }
                    {\vdash C , C ^{\bot}}
                    {\vdash (A \otimes B) \otimes C , A^{\bot}, B^{\bot}, C^{\bot}}
          }
          {\vdash A ^{\bot}, B ^{\bot}, (A \otimes B) \otimes C,  C ^{\bot}}
}
{
  \vdash A ^{\bot} \parr B ^{\bot}, (A \otimes B) \otimes C, C ^{\bot}
}\]

becomes



\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
  A\ar[rd]\netarrow{negA}{2} & {} & B\ar[ld]\netarrow{negB}{5} \\
  & \makebox[0pt]{$A \otimes B$}\ar[rd] & & C \ar[ld]\netarrow{negC}{2} &&&& |[alias=negA]|A ^{\bot}\ar[rd] && |[alias=negB]|B ^{\bot}\ar[ld]\\
  && \makebox[0pt]{$(A \otimes B) \otimes C$} &&& |[alias=negC]|C^{\bot} &&& \makebox[0pt]{$ A ^{\bot} \parr B ^{\bot}$}
\end{tikzcd}\]



while also comes from form

\begin{center}
\begin{prooftree}
\AxiomC{\(\vdash A, A^{\bot}\)}
\AxiomC{\(\vdash B , B ^\bot\)}
\BinaryInfC{\(\vdash A \otimes B, A ^{\bot}, B ^{\bot}\)}
\UnaryInfC{\(\vdash A ^{\bot}, B ^{\bot}, A \otimes B\)}
\UnaryInfC{\(\vdash A ^{\bot} \parr B ^{\bot} , A \otimes B\)}
\UnaryInfC{\(\vdash A \otimes B, A ^{\bot} \parr B ^{\bot}\)}
\AxiomC{\(\vdash C, C^{\bot}\)}
\BinaryInfC{\(\vdash (A \otimes B) \otimes C, A ^{\bot} \parr B ^{\bot}, C ^{\bot}\)}
\end{prooftree}
\end{center}

**** others

Essentially, we lose the (inessential) appplications order of rules.

At this point precise definitoins are needed. A /proof/ structure is just a graph biult from the follwing components:

Link:

\[
\begin{tikzcd}[every arrow/.append style={dash, line width=1pt, rounded corners}]
 A \netarrow{1-2}{2} & A^{\bot}
\end{tikzcd}\]

Cut:

\[
\begin{tikzcd}[every arrow/.append style={dash, line width=1pt, rounded corners}]
A \netarrow{1-2}{-2} & A^{\bot}
\end{tikzcd}\]

logical rules

\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
A \ar[rd] & & B \ar[ld]\\
& \makebox[0pt]{$A \otimes B$}
\end{tikzcd}\]

\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
A \ar[rd] & & B \ar[ld]\\
& \makebox[0pt]{$A \parr B$}
\end{tikzcd}\]

\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
&& A\netarrow{2-1}{2}\ar[rd] && B \netarrow{2-7}{2}\ar[ld]\\
A^{\bot} &&& \zerohbox{A \parr B} &&& B^{\bot}
\end{tikzcd}\]










\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
  A\ar[rd]\netarrow{2-9}{4} && B\ar[ld]\netarrow{2-7}{2}\\
  & \zerohbox{A \otimes B} \ar[rd] && C\ar[ld]\netarrow{3-5}{2} &&& B^{\bot}\ar[rd] && A^{\bot}\ar[ld]
  & A\ar[rd]\netarrow{3-15}{4} && B\ar[ld]\netarrow{3-13}{2}\\
  && \zerohbox{A \otimes B \otimes C} && C^{\bot} &&& \zerohbox{A ^{\bot} \parr B^{\bot}} \netarrow{3-11}{-2}
  &&& \zerohbox{A \otimes B} && B ^{\bot} && A ^{\bot}
\end{tikzcd}\]

* Little topics :noexport:

** alternative explain for Herbrand's theorem

An formula \(S\) has a Herbrand's model means that there are a set of ground terms that makes \(S\) true.

** abeliean category and sheaf

4. /Applications in Sheaf Theory/: Sheaves of abelian groups (or more generally, sheaves of modules over a ringed space) form abelian categories. This allows you to apply homological algebra to study sheaf cohomology, which is crucial in algebraic geometry and topology.

** A-module category

\(A\) is a ring.

A module category is a category where all the objects are abelian groups, together with a \(A\) action, \(a \in A\) we have \(a\colon M \to M\) where \(M \in \mathrm{Obj}\).

** additive category

*** *Ad1. Morphisms form abelian groups*

For each pair of objects \( A, B \in C \), the set of morphisms \( \text{Mor}(A, B) \) forms an /abelian group/. This means:
- There is an operation of /addition/ of morphisms, denoted \( f + g \), satisfying associativity, commutativity, and the existence of an identity element (the /zero morphism/ \( 0: A \to B \)).
- Each morphism has an /additive inverse/ \( -f \).

Additionally, composition of morphisms /distributes/ over addition. This translates to two distinct statements:
1. *Left distributivity*: For \( f: A \to B \), \( g, h: B \to C \), we have:
   \[
   f \circ (g + h) = f \circ g + f \circ h.
   \]
2. *Right distributivity*: For \( f, g: A \to B \), \( h: B \to C \), we have:
   \[
   (f + g) \circ h = f \circ h + g \circ h.
   \]

*** *Ad2. Zero object*

The category \( C \) has a /zero object/, denoted \( 0 \). This is an object that is both:

- *Initial*: There is a unique morphism \( 0 \to A \) for every object \( A \in C \).
- *Terminal*: There is a unique morphism \( A \to 0 \) for every object \( A \in C \).

The zero object is the "trivial" object where all morphisms to or from it are unique.

*** *Ad3. Finite products*

The category \( C \) has /binary products/ \( A \times B \) for any pair of objects \( A, B \in C \). By induction, this implies the existence of products for any finite number of objects.

** arithmetic RR

RR is the child of formalism. And we introduce \(=\) \(<\) and some constant \(0\) \(S\) \(\times\) \(+\).

Arithmetic system right now is boring because it is merely formalism shit.

*** group: equality

- \(x = x\)
- \(x = y \Rightarrow y = x\)
- \(x = y \land y = z \Rightarrow x = z\)
- \(x = y \land z =t \land x < z \Rightarrow y < t\)
- \(x  = y \Rightarrow S x = Sy\)
- \(x = y \land z = t \Rightarrow x + z = y + t\)
- \(x = y \land z = t \Rightarrow x \times z = y \times t\)

There rules are used to prove \(x = y \land A[x] \Rightarrow A[y]\).

*** group: definitions

- \(x + 0 = x\)
- \(x + S y = S(x+y)\)
- \(x \times 0 = 0\)
- \(x \times S(y) = (x\times y) + x\)

These can prove that if two terms \(x,y\) are the same number, then \(x = y\) is provable.

the 3rd and 4th peano axioms

- \(Sx \ne 0\)
- \(S x = S y \Rightarrow x = y\)

These group can be used to prove that if \(x, y\) are different number, then \(x \ne y\) is provable.
Also, these two things shamelessly assume an infinite domain, otherwise \(\overline{10} = \bar{0}\) could be proved.

*** group three: a last axiom
\[
x < y  \lor x = y \lor x > y
\]
what is this one used for? Let us check what Girard says:

#+begin_quote
  the last axiom of a slightly different nature from the rest, since it is not needed for /incompleteness/: the representation of expansive properties is handled by the definition axioms. It is used in the representation of recursive functions and therefore in the algorithmic undecidability of RR and all its consistent extensions. It is also used in the Rosser variant.
#+end_quote

** atomic measurable space :measure:probability:

Given \(\Omega\), we have a partition, \(\{E_{\alpha}\}_{\alpha \in A}\).

We can generate a sigma algebra based on this shit. The measurable space generated by this partition is called atomic measurable space because every measurable set is equal to some disjoint union in \(\{E_{\alpha}\}_{\alpha \in A}\).

** banach space

Space with norm \(\Vert\cdot\Vert\). The norm suits some rules.

And cauchy sequences converge.

有限维向量空间上的范数总是完备的。因此，带有范数的有限维向量空间都是 Banach 空间。但这种情形太简单，人们更感兴趣的是无限维的 Banach 空间.

** banach space; topology; bounded operator

The norm in banach space can induce a metric.
A metric can induce topology.

A linear operator from \(X\) to \(Y\) is a linear mapping from \(X\) to \(Y\), which is also a continuous function.

** beta reduction

beta reduction is \((\lambda  x . t) t' \leadsto t[t'/x]\).

related tait's method. About the strong normalization of beta reduction.

beta reduction is reduction of =application=. Application is cut in the sense of ch isomorphism.

** boolean algebra and sigma algebra
:PROPERTIES:
:ID:       2c7e830c-51e5-4974-ac56-4ed1cdffc1af
:END:

Boolean algebra can be modeled as set.
Boolean algebra has three operations:

  1. or
  2. and
  3. negate

Boolean algebra should be closed under those operations. If there are two terms that is in boolean algebra, then their sum or product should be in the algebra also.

Additionally, boolean algebra should have suprema and infimum. And it should be a lattice. We shall give definitions (modeling) using set

  1. \(E,F \in \cal F\) then \(E \cup F \in\cal F\)
  2. \(E, F \in \cal F\) then \(E \cap F \in \cal F\)
  3. \(E \in \cal F\) then \(\neg E \in \cal F\)
  4. \(\exists X \in \cal F\), \(\forall E \in \cal F\) \(E \subset X\)
  5. \(\exists \varnothing \in \cal F\), \(\forall E \in \cal F\), \(E \supset \varnothing\)

It is a lattice because it is ordered by inclusion, \(X,\varnothing\) are in the \(\cal F\).
It suffices boolean algebra because for it is closed under operations.

Sigma algebra is boolean algebra added with one condition.

** borel sets

We generate borel sets using sigma algebra generator, with the respect to open sets in Euclidean spaces.

** CIC

what is CIC?

CIC stands for the Calculus of Inductive Constructions (Calcul des Constructions Inductives). It is a formal system used in the Coq proof assistant. The CIC extends the Calculus of Constructions (CoC) by adding inductive types, which are essential for defining data structures and reasoning about them.

*** Key Features of CIC:

1. *Inductive Types*: These allow the definition of recursive data structures, such as lists, trees, and natural numbers. Inductive types are defined by a set of constructors that specify how to build elements of the type.

2. *Pattern Matching*: This is used to define functions over inductive types by case analysis on the constructors.

3. *Dependent Types*: Like in the Calculus of Constructions, CIC supports dependent types, where the type of a term can depend on another term. This is crucial for expressing properties and proofs in a precise manner.

4. *Propositions as Types*: In CIC, propositions are represented as types, and proofs of these propositions are represented as terms of these types. This is known as the Curry-Howard isomorphism.

*** Example in Coq:

Here is a simple example of defining natural numbers using an inductive type in Coq:

#+begin_src coq
Inductive nat : Type :=
  | O : nat
  | S : nat -> nat.
#+end_src

This defines the type =nat= with two constructors: =O= (representing zero) and =S= (representing the successor function).

You can then define a function that adds two natural numbers using pattern matching:

#+begin_src coq
Fixpoint add (n m : nat) : nat :=
  match n with
  | O => m
  | S n' => S (add n' m)
  end.
#+end_src

*** Importance in Coq:

The CIC is the foundational logic of Coq, providing a powerful framework for both programming and proving. It allows users to write programs and prove properties about them within the same system, leveraging the Curry-Howard isomorphism to bridge the gap between computation and logic.

*** Relation to Martin-Löf's Type Theory:

Martin-Löf's type theory is a foundational system for constructive mathematics, and it has influenced the development of CIC. Both systems emphasize the role of types in both programming and proving, and they share the idea of propositions as types. However, CIC extends Martin-Löf's type theory by incorporating features like inductive types, making it more suitable for practical programming and proof development in Coq.

*** Summary:

- *CIC*: Calculus of Inductive Constructions, used in Coq.
- *Features*: Inductive types, pattern matching, dependent types, propositions as types.
- *Example*: Defining natural numbers and addition function in Coq.
- *Relation*: Influenced by Martin-Löf's type theory, extends it with inductive types.

Understanding CIC is crucial for working effectively with Coq and for appreciating the interplay between programming and proving in a constructive logic framework.

** CoC and Coq and Martin-Lof's system
:PROPERTIES:
:ID:       098bc0a2-af8b-4af0-9487-c4a61e6a339a
:END:

There is connection between constructive logic and CoC and Coq

See The Blind Spot 6.A.4

** convex

convex sum for two points \(x,y\) is
\[w_1 a + w_2 b\]
where \(w_1 + w_2 = 1\). Convex subset \(X\) is such subset where \(\forall x ,y\) the convex sum
is in \(X\).

** directed sets

A collection of sets is said to be directed, if the sets are indexed by a partially ordered set.

For example \(\{x_{i}\}\), \(i \in I\), where\(I\) is a partial order set.

** diagram
:PROPERTIES:
:ID:       a96c058c-5a34-4c55-99d4-7943d692bf13
:END:

A diagram of category \(\mathcal C\) can be defined a wacky way by a functor from a locally small cat \(\mathcal J\) to \(\mathcal C\).

Anyway we can use \(\mathcal J\) to express a graph (because a category can be viewed as a graph). Anyway this is kinda wacky.

We define limit for a diagram.

** dynkin system

- negation: \(d \in D\) then \(d^{c} \in D\)
- disjoint union: if \(\{d_{i}\}\) disjoint and \(d_{i} \in D\), then \(\bigcup d_{i}\in D\)

** existential type

In system F, the existential type is defined as?
\[\forall Y (\forall X(A \Rightarrow Y) \Rightarrow Y)\]

1. treat existential type as "co-universal type"
2. treat existential type as usage involving type class?
3. treat existential type constructively in Martin-Lof type system
   see also [[*sigma type and Martin-Lof type theory][sigma type and Martin-Lof type theory]]
   see also [[id:098bc0a2-af8b-4af0-9487-c4a61e6a339a][CoC and Coq and Martin-Lof's system]]

See also in zhihu question

** existential type explain

#+begin_src lean
  structure Sigma {α : Type u} (β : α → Type v) where
    fst : α
    snd : β fst
#+end_src

#+begin_src lean
  def existsType {α : Type u} (A : α → Type v) : Type (max u v) :=
    Sigma A
#+end_src

#+begin_src lean
  def existsTypeSystemF {α : Type u} (A : α → Type v) : Type (max u v) :=
    (Y : Type (max u v)) → ((X : α) → A X → Y) → Y
#+end_src

#+begin_src lean
  def toSystemF {a : Type u} {A : α → Type v} (p : existsType A) : existsTypeSystemF A :=
    fun Y f => f p.fst p.snd
#+end_src

#+begin_src lean
  def fromSystemF {α : Type v} {A : α → Type v} (p : existsTypeSystemF A) : existsType A :=
    p (Sigma A) (fun X a => ⟨X, a⟩)
#+end_src

** focalisation

\[\frac{(\vdash \xi_{i} , \Delta)_{i\in I}}{\xi \vdash \Delta}(-)\quad
\frac{( \xi _{I} \vdash\Delta_{I})_{I \in \mathcal N}}{\vdash \xi , \bigcup \Delta_{I}}(+)\]

** functional dependencies

Check functional dependencies in haskell

#+begin_src haskell
  class Mult a b c | a b -> c where
    (*) :: a -> b -> c
#+end_src

** ground instances

A ground instance of a formula \(S\) is a formula derived from \(S\), where all the variables are replaced with ground terms.

** ground terms

Ground terms are terms that have no variable.

** herbrand's theorem

To prove \(\exists x A\), we can interchangably to prove \(A[t_1/x] \lor A[t_2/x] \lor \dots \lor A[t_{n} / x]\) where \(t_{i}\)s are ground terms. This is because the existential quantifier can be satisfied by any one of these ground instances (intuitively saying).

** herbrand's theorem and cut-elimination

Herbrand’s Theorem: This theorem provides a way to transform a first-order logic formula into a purely propositional form.

It states that if a first-order formula is universally valid, then there is a finite set of ground instances (instances with no variables) of its clauses that are propositionally valid.

How they are related is that they both transfer something undecidable to decidable.

** hausdorff dimensin

Check [[id:c7497fe1-ea96-46f2-a494-66af9d63a675][Amann IX.3.4]]

There are some properties for hausdorff dimension. But I am not interested in them anymore.

** hahn's theorem

For a linear space \(X\) over field \(\R\), if \(p\) is such function:

1. positive Homo: \(p (ax) = ap(x)\) for \(x \ge 0\)
2. subadditivity: \(p(x+y) \le p(x) + p(y)\)
3. \(Y\) is a linear subspace in \(X\), and a linear functional on \(Y\) is dominated by \(p\):
   \[l(y) \le p(y)\]
   where \(y \in Y\).

Then we have that \(l\) can (whose domain) be extended to \(X\), and still be dominated by \(p\).

** indicator random variable :probability:

The indicator random variable for an event \(E\) is noted \(1_{E}\)
which means that if \(E\) happens, takes value \(1\).
Otherwise \(0\).

We have
\[P(E) = \mathbf E(1_{E})\]

** interval in \(\mathbb R^{n}\)

\((a,b)\) in \(\mathbb R^{n}\) is \(\{a_{i}\}\) and \(\{b_{i}\}\). I think \(a_{i} < b_{i}\) for every \(i = 1,\dots,n\).

We can define a \(n\)-dimensional volume for an interval in \(\mathbb R^{n}\).

** intuitionistic property and herbrand's theorem

In intuitionistic, \(\exists x A\) means there is a \(t\) such that \(A[t/x]\).

Herbrand's theorem tells us that to prove \(\exists x A(x)\), we can equivalently prove

\[A(t_{1}) \lor \dots \lor A(t_{n})\]

for ground terms \(t_{1},\dots,t_{n}\). Herbrand can be viewed as a version

** jump continuous function from Amann

*** jump continuous function; piece-wise continuous function

Def for jump continuous function: for every \(x \in I\),
\[f ( x \pm 0) = \lim _{y\to x\pm 0}f(y)\]
exists. The discontinous points are like laspes.

if \(f\) has finitely discontinuous points, then \(f\) is piece-wise continuous.

- \(\cal T\) is stair case function
- \(\cal SC\) is piece-wise continuous function
- \(\cal S\) is jump continuous function

*** jump continuous function; properties

(b) if \(f\) is \(\cal T\), on partition \(\Omega\), every refinement \(\bar \Omega\) of \(\Omega\) is also partition for \(f\).

(c) if \(f \colon I \to E\) is a \(\cal S\), neither \(f(x + 0)\) nor \(f(x-0)\) need equal \(f(x)\) for \(x \in I\).

(d) \(\mathcal S(I,E)\) is a vector subspace of \(B(I,E)\)

Proof. The linearity of one-sided limits implies immediately that \(\mathcal S\) is a vector space. If \(f \in \mathcal S - B(I, E) \), we can find a sequence \((x_{n})\) in \(I\) with
\[\Vert f(x_{n}) \Vert \ge n\quad \text{for } n \in \mathbb{N}\]
because \(I\) is compact, there is a subsequence \((x_{n_{k}})\) of \((x_{n})\) and \(x\in I\) such taht \(x_{n_{k}} \to x\) as \(k \to \infty\). By choosing a suitable subsequence of \((x_{n_{k}})\), we find a sequence \((y_{n})\), that converges monotonically to \(x\).

If \(f\) is jump continuous, there is a \(v \in E\) with \(\lim f(y_{n}) = v\) and thus \(\lim \Vert f(y_{n}) \Vert = \Vert v\Vert\) (compare with Example). Because every convergent sequence is bounded, we have contradicted (1.1). Therefore \(\cal S \subset \mit B\).
: so we just prove that all the values

(e)
\[\mathcal T(I,E) \subset \mathcal{SC}(I,E) \subset \mathcal S(I,E)\]

*** jump continuous function; theorem
:PROPERTIES:
:ID:       c260be84-d97c-40b3-ab83-b635b6a26ebc
:END:

**** theorem part I : use stair case function to approximate jump continuous
1.2 Theorem \(A\) function \(f: I \rightarrow E\) is jump continuous if and only if there is a sequence of staircase functions that converges uniformly to it.

Proof " \(\Rightarrow\) " Suppose \(f \in \mathcal{S}(I, E)\) and \(n \in \mathbb{N}^{\times}\). Then for every \(x \in I\), there are numbers \(\alpha(x)\) and \(\beta(x)\) such that \(\alpha(x)<x<\beta(x)\) and
\[\|f(s)-f(t)\|<1 / n \quad \]
\(\text { for } s, t \in(\alpha(x), x) \cap I \quad \text { or } \quad s, t \in(x, \beta(x)) \cap I .\)
: s,t are at the same side

Because \(\{(\alpha(x), \beta(x)) ; x \in I\}\) is an open cover of the compact interval \(I\), we can find elements \(x_0<x_1<\cdots<x_m\) in \(I\) such that \(I \subset \bigcup_{j=0}^m\left(\alpha\left(x_j\right), \beta\left(x_j\right)\right)\).
: because I = [α, β] is compact

Letting \(\eta_0:=\alpha, \eta_{j+1}:=x_j\) for \(j=0, \ldots, m\), and \(\eta_{m+2}:=\beta\), we let \(\mathfrak{Z}_0=\left(\eta_0, \ldots, \eta_{m+2}\right)\) be a partition of \(I\). Now we select a refinement \(\mathfrak{Z}_1=\left(\xi_0, \ldots, \xi_k\right)\) of \(\mathfrak{Z}_0\) with
: setting a partition
\[\| f(s) - f(t) \| < 1 / n,\]
where \(s,t \in (\xi_{j-1}, \xi_{j})\) and \(j = 1\dots k\).
\[\begin{aligned}
&f_n(x):=\left\{\begin{array}{cl}
f(x), & x \in\left\{\xi_0, \ldots, \xi_k\right\}, \\
f\left(\left(\xi_{j-1}+\xi_j\right) / 2\right), & x \in\left(\xi_{j-1}, \xi_j\right), \quad j=1, \ldots, k .
\end{array}\right.
\end{aligned}\]
: construct a good function

Then \(f_n\) is a staircase function, and by construction
\[
\left\|f(x)-f_n(x)\right\|<1 / n \quad \text { for } x \in I \text {. }
\]
Therefore \(\left\|f-f_n\right\|_{\infty}<1 / n\).

**** theorem part II

"\(\Leftarrow\)" Suppose there is a sequence \(\left(f_n\right)\) in \(\mathcal{T}(I, E)\) that converges uniformly to \(f\). The sequence also converges to \(f\) in \(B(I, E)\).
# fn is here
Let \(\varepsilon>0\). Then there is an \(n \in \mathbb{N}\) such that \(\left\|f(x)-f_n(x)\right\|<\varepsilon / 2\) for all \(x \in I\). In addition, for every \(x \in(\alpha, \beta]\) there is an \(\alpha^{\prime} \in[\alpha, x)\) such that \(f_n(s)=f_n(t)\) for \(s, t \in\left(\alpha^{\prime}, x\right)\). Consequently,
\begin{multline}
\|f(s)-f(t)\| \leq\left\|f(s)-f_n(s)\right\|+\left\|f_n(s)-f_n(t)\right\|\\ +\left\|f_n(t)-f(t)\right\|<\varepsilon\notag
\end{multline}
for \(s,t \in (\alpha ', x)\).
# use metric inequality

Suppose now \(\left(s_j\right)\) is a sequence in \(I\) that converges from the left to \(x\). Then there is an \(N \in \mathbb{N}\) such that \(s_j \in\left(\alpha^{\prime}, x\right)\) for \(j \geq N\), and (1.2) implies
\[
\left\|f\left(s_j\right)-f\left(s_k\right)\right\|<\varepsilon \quad \text { for } j, k \geq N .
\]

Therefore \(\left(f\left(s_j\right)\right)_{j \in \mathbb{N}}\) is a Cauchy sequence in the Banach space \(E\), and there is an \(e \in E\) with \(\lim _j f\left(s_j\right)=e\). If \(\left(t_k\right)\) is another sequence in \(I\) that converges from the left to \(x\), then we can repeat the argument to show there is an \(e^{\prime} \in E\) such that \(\lim _k f\left(t_k\right)=e^{\prime}\). Also, there is an \(M \geq N\) such that \(t_k \in\left(\alpha^{\prime}, x\right)\) for \(k \geq M\). Consequently, (1.2) gives
\[\| f(s_{j}) - f(t_{k}) \| < \epsilon\]
for \(j, k\ge M\).

After taking the limits \(j \rightarrow \infty\) and \(k \rightarrow \infty\), we find \(\left\|e-e^{\prime}\right\| \leq \varepsilon\). Now \(e\) and \(e^{\prime}\) agree, because \(\varepsilon>0\) was arbitrary. Therefore we have proved that \(\lim _{y \rightarrow x-0} f(y)\) exists. By swapping left and right, we show that for \(x \in[\alpha, \beta)\) the right-sided limits \(\lim _{y \rightarrow x+0} f(y)\) exist as well. Consequently \(f\) is jump continuous.

*** jump continuous fucntion; remark

1.3 Remark If the function \(f \in \mathcal{S}(I, \mathbb{R})\) is nonnegative, the first part of the above proof shows there is a sequence of nonnegative staircase functions that converges uniformly to \(f\).

*** jump continuous function; as vector subspaces of Banach spaces
:PROPERTIES:
:ID:       e89d93e0-1acd-4bc7-96fa-65934ae8f9a3
:END:

Theorem The set of jump continuous functions \(\mathcal S(I,E)\) is a closed vector subspace of \(B (I,E)\)and is itself a Banach space; \(\mathcal T(I,E)\) is dense in \(\mathcal S(I,E)\).

Proof From Remark 1.1(d) and (e), we have the inclusions
\[
\mathcal{T}(I, E) \subset \mathcal{S}(I, E) \subset B(I, E)
\]

According to Theorem 1.2 [[id:c260be84-d97c-40b3-ab83-b635b6a26ebc][theorem part I : use stair case function to approximate jump continuous]], we have
\[
\overline{\mathcal{T}(I, E)}=\mathcal{S}(I, E),
\]
when the closure is formed in \(B(I, E)\). Therefore \(\mathcal{S}(I, E)\) is closed in \(B(I, E)\) by Proposition III.2.12. The last part of the theorem follows from (1.3).

*** corllary of above

1.5 Corollary

(i) Every (piecewise) continuous function (\(\cal SC\)) is the uniform limit of a sequence of staircase functions.

(ii) The uniform limit of a sequence of jump continuous functions is jump continuous.

(iii) Every monotone function is the uniform limit of a sequence of staircase functions.

Proof The statement
(i) follows directly from Theorem 1.2; [[id:c260be84-d97c-40b3-ab83-b635b6a26ebc][link]]
(ii) follows from Theorem 1.4, and statement [[id:e89d93e0-1acd-4bc7-96fa-65934ae8f9a3][link]]
(iii) follows from Remark 1.1(f).

*** exercise

**** 2

define \(f\colon [-1,1] \to \mathbb{R}\) by

\[f (x):=\begin{cases}
\displaystyle \frac{1}{n+1},\quad & \displaystyle x \in \left[-\frac{1}{n}, -\frac{1}{n+1}\right) \cup \left(\frac{1}{n+1} , \frac{1}{n}\right]\\
0, \quad & x = 0.
\end{cases}\]

Prove or disprove that

(a) \(f \in \mathcal T([-1,1], \mathbb R)\)
(b) \(f \in S([-1,1], \mathbb R)\)

(a) is obviously false!
if \(f \in \mathcal T\), \(\mathfrak{Z} = \{\alpha_{0}, \dots, \alpha _{n} \}\)
and if \(0 \in (\alpha _{i}, \alpha _{i+1})\), then we know that that is impossible for \(f \in \cal T\).
there is a \(n\) such that \(1/n\) is less than \(\alpha _{i+1}\), it is the same for the left hand side.
and then \(1/(n+1), 1/n\) is valued as \(1 / (n+1)\), which is different from \(f(0)=0\).
if \(0 =\alpha _{i}\), \(f\) is constant on \((0, \alpha _{i+1})\). And that is absolutely not true.

(b) is true
obviously! Because \(f(x\pm 0)\) exists.

**** 3

Prove or disprove that \(\mathcal {SC}(I,E)\) is a closed vector subspace of \(\mathcal S(I,E)\).

This is obvious! Because \(\mathcal T(I,E) \subset \mathcal{SC}(I,E) \subset \mathcal S(I,E)\), and \(\overline{\mathcal T(I,E)} = \mathcal S(I,E)\)
Consequently, it is impossible that \(\overline{\mathcal{SC}(I,E)} = \mathcal{SC}(I,E)\)

**** 4

Show these statements are equivalent for \(f\colon I \to E\)

(Ⅰ) \(f \in \mathcal S(I,E)\)
(Ⅱ) \(\exists (f_{n})\) in \(\mathcal T(I,E)\) such that \(\sum_{n}\|f_{n}\|_{\infty} < \infty \) and \(f = \sum^{\infty}_{n=0}f_{n}\)

(II) → (I) is obvious
Define \((f'_{n})\) where \(f'\sb n = \sum ^{n}_{i=0} f_{i}\)

(I) → (II)
I think it is similiar.

**** 5

Prove that every jump continuous function has at most a countable number of discontinuities.

**** 6

Denote by \(f\colon [0,1] \to \R\) the Dirichlet function on \([0,1]\). Does \(f\) belong to \(S([0,1], \R)\).

** kernel

kernel can be defined in a category where there is \(0\) object, which is a terminal and a initial.

A kernel of a morphism \(f\colon B\to C\) is a morphism \(i\colon A \to B\) satisfy
1. \(f \circ i\) is zero morphism from \(A\to C\).
2. \(i\) is the "largest" one (subobject of \(B\)) which means that it satisfies the universal property.

\[
\begin{tikzcd}
  & Z \ar[ld, dashed, "\exists!\bar j" swap] \ar[d, "j"]\\
  A \ar[r, "i"] & B \ar[r, "f"] & C
\end{tikzcd}\]

Cokernel can be defined if you reverse the direction of arrow. Do remember reverse the arrow for dashed morphism in the above diagram.

** lattice

Lattice is partially ordered set with suprema and infimum

** lebesgue outer measure

we use [[id:6ea8307c-098f-4bcf-a62e-6edb68a48e2e][volume for interval in \(\mathbb R^{n}\)]] to construct the outer measure

\[\mu ^{*}(A) = \inf\big\{\sum \mathrm{vol} (A_{i}) \mid \bigcup\nolimits_{i} A_{i} \subset A\big\}\]
where \(A_{i}\) are cubes in \(\mathbb R^{n}\).

You should verify that the this is indeed an outer measure.
You need to verify the sigma-subadditivity. That is

\[\mu^{*}\big(\bigcup A_{i}\big) \le \sum \mu^{ * }(A_{i})\]

*Proof.*

The proof will use the property of inf. That is for an \(B\) and for an arbitrary \(\epsilon\), there would be such \(\{B_{i}\}_{i \in I}\) that
\[\mu^{ * } (B) \ge \sum \mathrm{vol}(B_{i}) - \epsilon\]
the difference is arbitrary small.

To verify the subadditivity, for every \(A_{i}\) we need to find a sequence \(\{A_{i,j}\}_{j\in J}\), so we have, for \(i \in I\):
\[\mu^{  * } (A_{i}) \ge \sum_{j} \mathrm{vol}(A_{i,j}) - \epsilon\]
we add them up
\[\sum_{i} \mu(A_{i}) \ge \sum_{i}\sum _{j} \text{vol} (A_{i,j}) - \sum_{i} \epsilon\]
we choose epislon carefully so it can be arbitrarily small. For example, every \(\epsilon_{i} = \epsilon / 2^{i}\) which leads to \(\sum_{i} \epsilon_{i} \le k \epsilon\) where \(k\) is a constant (or we can carefull choose that \(I\) is those natural number from 1 to infty, and then \(\sum _{i} \epsilon_{i} = \epsilon\). Small modification is needed if \(I\) starts from 0). And we complete the proof.

** limit

Limit is defined for a [[id:a96c058c-5a34-4c55-99d4-7943d692bf13][diagram]]. A diagram \(D \colon \mathcal J \to \mathcal C\)

A limit for \(D\) is an object in \(\mathcal C\), noted as \(L\) with morphism \(\pi _{j} \colon L \to D (j)\) for all \(j\) in \(\mathcal J\).

pullback and product are examples of limit.

The limit \( L \) is the "best approximation" to the diagram \( D \). It is the object that "fits" into the diagram in a universal way, meaning that any other object that fits must factor uniquely through \( L \).
# answer from AI

** linear logic rules
:PROPERTIES:
:ID:       d95d7423-8e9a-4133-8efd-3dffbe78895d
:END:

\[
\begin{matrix}
  & \text{multiplicative} & \text{additive} \\ \\
  \land:
  & \displaystyle \frac{\vdash \Gamma, A \quad \vdash \Delta, B}{\vdash \Gamma, \Delta, A\otimes B}
  & \displaystyle \frac{\vdash \Gamma, A \quad \vdash \Gamma, B}{\vdash \Gamma, A \mathbin{\&} B} \\ \\
  \lor:
  & \displaystyle \frac{\vdash \Gamma, A , B}{\vdash \Gamma, A \parr B}
  & \displaystyle \frac{\vdash \Gamma, A}{\vdash \Gamma, A \oplus B}
\end{matrix}\]

** linear logic Reversibility
:PROPERTIES:
:ID:       d9faa15b-51b8-4aa1-bd76-f3971a80a6b9
:END:

[[id:d95d7423-8e9a-4133-8efd-3dffbe78895d][linear logic rules]]

The connectives \(\parr\) and \(\with\) are reversible: from the conclusion and active formula, one can recover the premises. Check your self.

For \(\otimes\), you can not split into \(\Gamma, \Delta\). For \(A \otimes B\), you can not choose whether it is \(A\) or \(B\).

** linear logic positive and negative

- negative connectives \(\parr\) and \(\with\) (reversible [[id:d9faa15b-51b8-4aa1-bd76-f3971a80a6b9][linear logic Reversibility]])
- positive connectives \(\otimes\) and \(\oplus\) (non-reversible)

** liquid types

https://zhuanlan.zhihu.com/p/701186848

types with a little bit constraint. The grammar is like that in lean4. While in the latter, we should provide the proof for the constraint, liquid type system should check that for you, which is part of the type checking.

** Lipschitz continuous

A function \( f : A \to \mathbb{R}^m \) is *Lipschitz continuous* if there exists a constant \( \lambda \geq 0 \) (called the *Lipschitz constant*) such that for all \( x, y \in A \),

\[
\|f(x) - f(y)\| \leq \lambda \|x - y\|.
\]

This means the function does not stretch distances between points by more than a factor of \( \lambda \).

** LLC

last-level cache (LLC)

** local completeness and local soundness

Soundness means the system does not prove something that is wrong.
which means no "false" information is added
Completeness means the system *can* prove something that is right.
which means no "true" information is lost

In the process of proving soundness and completeness, we can verify local soundness and local completeness first.
Local soundness refers to that introduction rule will not introduce additional information: introduction rule followed by elimination can be reduced to normal.
Local completeness refers to that elimination rule will not eliminate original information:
#+begin_quote
  if a valid logical structure can be derived using elimination rules, it can also be reconstructed using the corresponding introduction rules
#+end_quote

** logic complexity, first order

Logic complexity is about something like first order, second order; about the logic hierarchy, logic classification.

*** unbound quantifiers

Unbound quantifiers refer to quantifiers whose domain is not specified. But I think we can understand it as quantifiers whose domain is unbound, which is infinite.

*** prenex

prenex form is of form
\[
Q\sb{1}x\sb{1}\dots Q\sb{n}x\sb{n} A
\]
where \(A\) is quantifier free. /prenex/ is not that useful.

*** \(\Sigma\sp{0}\sb{1}\) and \(\Pi \sb{1}\sp{0}\)

The sb of the above notations is about the alternation of unbound quantifiers. I don't really know what is an unbound quantifiers. But zero alternation means no quantifier.

One alternation means the quantifiers are the same. For \(\Sigma\), the quantifer can only be \(\forall\), and for \(\Pi\) \(\exists\). Thus all formulas in \(\Sigma\sb{1}\sp{0}\) and \(\Pi\sb{1}\sp{0}\) are respectively of form:
\[
\exists x\sb{1}\dots\exists x\sb{n}A,\quad\quad
\forall x\sb{1}\dots\forall x\sb{n}A
\]
where \(A\) is \(Q\)-free. So you know alternation \(2\) means something like \(\forall\forall\forall\exists\exists\exists\exists\exists\exists\)

*** \(\Sigma\sp{0}\sb{1}\) sets

Here we understand \(\Sigma\sp{0}\sb{1}\) as a collection of sets. A \(\Sigma\sp{0}\sb{1}\) set is a set \(A\) that satify:
\[
x \in A \Leftrightarrow \exists y R(x,y)
\]
where \(R\) is a \(Q\)-free quantifier and can not be arbitrary.

*** \(\Delta\sb{1}\sp{0}\)

\(\Sigma_{1}^{0}\) and \(\Pi\sb{1}\sp{0}\) sets are respectively semi-decidable and co-semi-decidable.

semi-decidable means \(x \in A\) is decidable if \(x \in A\).
co-semi-decidable means \(x \in A\) is decidable if \(x \notin A\).

So a \(\Delta^{0}_{1}\) set is decidable, since \(\Delta^{0}_{1}:= \Sigma^{0}_{1}\cap \Pi _{1}^{0}\).

*** a classic example of \(\Delta\sp{0}\sb{1}\) set
(given by Copilot)

Sure! A classic example of a \(\Delta_1\sp{0}\) set is the set of even numbers.

- The set of even numbers can be defined by the formula: \( x \in \text{Even} \iff \exists y \, (x = 2y) \). This shows that the set of even numbers is in \(\Sigma_1\).

- The set of even numbers can also be defined by the formula: \(x \in \text{Even} \iff \forall y \, (x \neq 2y + 1) \). This shows that the set of even numbers is in \(\Pi_1\).

Since the set of even numbers can be characterized by both an existential and a universal quantifier, it is in the intersection of \(\Sigma_1\) and \(\Pi_1\), making it a \(\Delta_1\) set.

** Logic Programming :text:
:PROPERTIES:
:ID:       579422fc-12f1-4d6c-9ba6-7de991f0ec61
:END:

Logic programming is based on the idea of seeking the atomic theorems of a theory whose axioms are /Horn clauses/. Those are sequents \(\mathcal S\) of the form \(P_{1}\dots P_{n} \vdash Q\) hwere \(P_{1}\dots P_{n},Q\) are atomic. To each axiom \(\mathcal S\) one cannaturally associate introduction rules:

\[
\prftree[r]{$(\mathcal S \theta \ I)$}
{\prfsummary{}{P_{1} \theta\quad \dots\quad P_{n} \theta}}
{Q \theta}\]

where \(\theta\) is a subsitution. One easily shows:

  - cut-elimination (immediate, there is no \(\mathcal S\)-elimination).
  - For formulas of the shape \(\exists x_{1}\dots\exists x_{p} (R _{1} \land \dots \land R_{q})\), \(R_{1}\) \(\dots\) \(R_{q}\) atomic, classical provability matches intuitionistic provability.

Thus the follwoing idea: in such an axiomatic system, try to prove formulas of the form \(\exists _{1} \dots \exists x_{p}\) \((R_{1} \land \dots \land R_{q})\). A cut-free proof will provide one with explicti values (existence property). The search for cut-free proofs is done by means of an algorithm, the /resolution method/, based on unification.

4.D.2 PROLOG, its /grandeur/. The idea is to consider a Horn theory as a program. one will make queries of the form « find the solutions \(x_{1}\dots x_{p}\) to the conjunction \(R_{1} \land \dots \land R_{q}\) », which amounts to finding proofs of \(\exists x_{1},\dots , \exists x_{p}\) \((R_{1} \land \dots \land R_{q})\), indeed cut-free. The search is done by unification: one seeks to prove \(R\) by finding a clause \(\Gamma \vdash \mathcal S\) (axiom written as a sequent) whose /head/ (right formula) unifies with \(R\), by means of \(\theta\); then one is led back to similar problems for the formulas \(\Gamma \theta\) of the /tail/ where \(\theta\) has been performed. One stops in case one steps on empty tails (success); on the other hand, when no unifier can be found, this is failure. But by far the most likely possibility is that of a search that neither succeeds nor fails, since it does not terminate. The culture of incompleteness is here to inform us as to the pregnancy of this unpleasant eventuality.

Whatsoever, what we just described is the paradigm of logic programming, which was so glamourous in the years around 1980, mainly because of the Japanese enthusiasm for « the fifth generation ».
# ?

4.D.3 PROLOG, its misery.

Logic programming was bound to failure, not because of a want of quality, but because of its exaggerations. Indeed, the slogan was something like « pose the question, PROLOG will do the rest ». This paradigm of /declarative/ programming, based on a « generic » algorithmics, is a sort of /all-terrain/ vehicle, capable of doing everything and therefore doing everything badly. It would have been more reasonable to confine PROLOG to tasks for which it is well-adapted, e.g., the maintenance of data bases.

On the contrary, attempts were made to improve its efficiency. Thus, as systematic search was too costly, « control » primitives, of the style « don’t try this possibility if... » were introduced. And this slogan « logic + control », which forgets that the starting point was the logical soundness of the deduction. What can be said of this control which plays /against/ logic14? One recognises the sectarian attitude that we exposed several times: the logic of the idea kills the idea.

The result is the most inefficient language ever designed; thus, PROLOG is very sensitive to the order in which the clauses (axioms) have been written.

** measurable function :probability:

A function \(\phi \colon  \Omega \to R\) between two measurable spaces \((\Omega, \mathcal F), (R, \mathcal B)\) is saied to be measurable if one has \(\phi ^{-1}(S) \in \cal F\)  for all \(S \in \mathcal B\).

The def is just like that in topology.
Remember about the pullback. So the pullback is possibly the coarest measurable spaces.

** measure :probability:

 + \(\mu (\varnothing) = 0\)
 + \(\mu (E \cup F) = \mu (E) + \mu (F)\)

** measure space :probability:

measure space is measurable space together with a measure.

** null sets :probability:

sets with zero measure

** MLCC and universe

In the context of Martin-Löf's Constructive Type Theory with Co-Inductive Types (MLCC), "universe" refers to a type of types. Universes are used to avoid logical paradoxes that arise from self-referential types, such as Russell's paradox in naive set theory. By organizing types into a hierarchy of universes, MLCC ensures that the type system remains consistent and avoids circular reasoning.

*** Key Concepts:

1. /Universes/: A universe is a type whose elements are types. For example, in Coq, the universe =Type= is a type that contains other types.

2. /Hierarchy of Universes/: To avoid self-referential paradoxes, universes are organized into a hierarchy. In Coq, this is often denoted as =Type(i)= where =i= is a level in the hierarchy. For example, =Type(0)= is a universe that contains types that do not mention =Type(0)= itself, =Type(1)= contains types that may mention =Type(0)=, and so on.

3. /Cumulativity/: Universes are cumulative, meaning that if =A : Type(i)=, then =A : Type(j)= for any =j ≥ i=. This allows types to be promoted to higher universes without requiring explicit casts.

*** Example in Coq:

In Coq, the universe hierarchy is implicit, but you can see its effects in the type-checking process. Here is an example of defining a type in a universe and using it:

#+begin_src coq
  Definition myType : Type := nat.

  Definition myFunc (A : Type) : Type := A -> A.

  Check myFunc myType
#+end_src

In this example:

- =myType= is defined as =nat=, which is a type in the universe =Type=.
- =myFunc= takes a type =A= (which is itself a type in some universe) and returns a function type =A -> A=.
- The =Check= command verifies that =myFunc myType= is well-typed.

*** Importance of Universes in MLCC:

1. /Consistency/: Universes prevent logical paradoxes by ensuring that types cannot refer to themselves in a way that leads to inconsistency.

2. /Expressiveness/: Universes allow for the expression of polymorphic types and higher-order functions, which are essential for writing generic and reusable code.

3. /Type Safety/: The hierarchy of universes ensures that types are used in a way that maintains type safety, preventing ill-typed expressions from being accepted.

*** Summary:

- /Universes/: Types of types, organized into a hierarchy to avoid paradoxes.
- /Hierarchy/: Universes are indexed by levels (e.g., =Type(0)=, =Type(1)=), ensuring that types cannot refer to themselves in a paradoxical way.
- /Cumulativity/: Types can be promoted to higher universes, allowing for flexible and expressive type systems.
- /Example/: Defining and using types in Coq, with implicit universe levels.
- /Importance/: Ensures consistency, expressiveness, and type safety in constructive type theories.

Understanding universes is crucial for working with advanced type systems and proof assistants like Coq, as it provides the foundational framework for organizing and reasoning about types in a consistent and expressive manner.

** MLCC Martin-Lof's Constructive Type Theory

MLCC stands for "Martin-Löf's Constructive Type Theory with Co-Inductive Types." It is an extension of Martin-Löf's original constructive type theory, which itself is a foundational system for constructive mathematics. The addition of co-inductive types allows for the definition of potentially infinite data structures, complementing the inductive types that are used for finite data structures.

*** Key Features of MLCC:

1. *Inductive Types*: These are used to define finite data structures. For example, natural numbers, lists, and trees can be defined inductively.

2. *Co-Inductive Types*: These are used to define potentially infinite data structures. For example, streams (infinite sequences) and infinite trees can be defined co-inductively.

3. *Dependent Types*: Types can depend on terms, allowing for more precise specifications and proofs. This is a core feature of Martin-Löf's type theory.

4. *Propositions as Types*: The Curry-Howard isomorphism is fully embraced, where propositions are types and proofs are terms of those types.

*** Example in Coq:

In Coq, which is based on the Calculus of Inductive Constructions (CIC), you can define both inductive and co-inductive types.

**** Inductive Type Example:

Here is an example of defining natural numbers using an inductive type:

#+begin_src coq
Inductive nat : Type :=
  | O : nat
  | S : nat -> nat.
#+end_src

**** Co-Inductive Type Example:

Here is an example of defining a stream (an infinite sequence) using a co-inductive type:

#+begin_src coq
CoInductive Stream (A : Type) : Type :=
  | Cons : A -> Stream A -> Stream A.
#+end_src

*** Importance of MLCC:

1. *Expressiveness*: MLCC allows for the definition of both finite and infinite data structures, making it a powerful tool for both programming and proof development.

2. *Proof Automation*: The ability to define co-inductive types enables the automation of proofs involving potentially infinite structures, which can be challenging to handle with purely inductive types.

3. *Foundational Research*: MLCC is used in foundational research on type theory and constructive mathematics, providing a rich framework for exploring the boundaries of what can be expressed and proven constructively.

*** Summary:

- *MLCC*: Martin-Löf's Constructive Type Theory with Co-Inductive Types.
- *Features*: Inductive and co-inductive types, dependent types, propositions as types.
- *Example*: Defining natural numbers (inductive) and streams (co-inductive) in Coq.
- *Importance*: Enhances expressiveness, enables proof automation, and supports foundational research.

Understanding MLCC is crucial for researchers and practitioners working with constructive type theories and proof assistants like Coq, as it provides a comprehensive framework for both finite and infinite data structures and their associated proofs.

** most general unifier

The most general unifier, a.k.a., m.g.u. is like the smallest unifier. A m.g.u. noted as \(\theta\), suit that \(\forall \sigma\) which is a unifier, there is a \(\rho\) such that
\[
\sigma = \theta \rho
\]

** operational semantics and dynamics

Harper: Dynamics
Others: operational semantics
Me: reduction rules and normalization

** PA

*** induction schema

Let us check how to express induction schema:
\[
A[0] \land \forall y (A[y] \Rightarrow A[Sy]) \Rightarrow \forall x A[x]
\]
here [[id:16547be7-8ed1-431b-b733-c1cd6048cda8][\(\mathrm{PA}\sb{2}\) second order peano arithmetic]], we have relative description for induction schema, where we have that if \(x\) is a nat number, then
\[
A[0]\land \forall y(A[y] \Rightarrow A[Sy]) \Rightarrow A[x]
\]
is provable.

*** the definition of PA

Peano's Arithmetic is derived from \textbf{RR}, added with induction schema.
\[
A[0] \land \forall y (A[y] \Rightarrow A[Sy]) \Rightarrow \forall x A[x]
\]
Here \(A\) is not arbitrary.

** pattern matching and higher order unification

see also: [[*unifier \(\theta\)][unifier \(\theta\)]]

Why? It seems obvious.

** pullback :probability:

Given a function \(\phi\) from \(\Omega\) to \(R\), we can define a pullback \(\phi^{*}(\cal B)\) of a sigma algebra \(\cal B\) (of \(R\)).

it is a collection of subsets of form \(\phi^{-1}(S)\), where \(S \in \cal B\).

Thus we use a function \(\phi \colon \Omega  \to R\), and a measurable space on \(R\), to create a pullback measurable space on \(\Omega\).
There is an example of atomic measurable space, where tells that the atomic space is pullback of a coloring map from \(\Omega\) to index set treated as discrete measurable space.

This pullback is the information preserved by function \(\phi\).

** quotient universal property

Let there be a morphism \(f\colon X \to Y\), the quotient can be expressed with this diagram

\[
\begin{tikzcd}
  X \arrow[r, "\bar f"] \arrow[rd, "f"]
    & Y / \ker f \arrow[d, dashed, "h"]\\
    & Y
\end{tikzcd}\]

** random variable :probability:

Modeling! \(X\) is a random variable on range \(R\). We model the \(X\) and events
using sample space \(\Omega\). Which is just a modeling for random variable, noted
as \(X_{\Omega}\colon \Omega \to R\). We know that there is a measure on \(\Omega\). \(P(\omega)\) or \(P(E)=\sum _{\omega \in E} P(\omega)\).

And we use \(P(X = n)\) as \(P(X_{\omega}^{-1}(\{n\}))\).
Note that events are also "modeled".

** refinement type

I don't know what this is.

** Russell universe and tarski universe

*** Russell Universes:

In a Russell universe, types are treated as elements of a universe. This means that types themselves are not distinguished from other elements within the universe. Here are the key points:

1. /Types as Elements/: Types are elements of a universe, and the universe is
   a type that contains these elements.

2. /No Explicit Type of Types/: There is no explicit type of types; instead,
   types are treated uniformly as elements.

3. /Example in Coq/: In Coq, the universe =Type= is a Russell universe. Types
   like =nat= and =bool= are elements of =Type=.

#+begin_src coq
  Definition myType : Type := nat.
#+end_src

In this example, =myType= is a type (specifically, =nat=), and it is an element of the universe =Type=.

*** Tarski Universes:
:PROPERTIES:
:ID:       8c4b6fbc-18b6-45ee-a751-fecd2ba6038a
:END:

In a Tarski universe, types are distinguished from their elements. This means that there is an explicit type of types, and types are not treated uniformly as elements. Here are the key points:

1. /Types and Elements Distinguished/: Types are not elements of the universe;
   instead, there is a separate type of types.
2. /Explicit Type of Types/: There is an explicit type of types, often denoted
   as =U= or =Type(i)=.
3. /Example in Agda/: In Agda, Tarski universes are often used. Types are
   elements of a universe, and there is an explicit type of types.

#+begin_src agda
data U : Set where
  nat : U
  bool : U

El : U → Set
El nat = Nat
El bool = Bool
#+end_src

In this example:

- =U= is the universe of types.
- =nat= and =bool= are types in the universe =U=.
- =El= is a function that maps a type in =U= to its corresponding Agda type.

*** Comparison:

1. /Uniformity vs. Distinction/:
   - Russell universes treat types uniformly as elements of a universe.
   - Tarski universes distinguish types from their elements, with an explicit
     type of types.

2. /Expressiveness/:
   - Tarski universes can be more expressive because they allow for more
     fine-grained distinctions between types and their elements.
   - Russell universes are simpler and more uniform, which can make them
     easier to work with in some contexts.

3. /Implementation/:
   - Coq uses Russell universes for its type system.
   - Agda often uses Tarski universes to provide more explicit control over
     the type hierarchy.

*** Summary:

- /Russell Universes/: Types are elements of a universe; no explicit type of
  types.
  - Example: Coq's =Type=.

- /Tarski Universes/: Types are distinguished from their elements; explicit
  type of types.
  - Example: Agda's universe hierarchy.

- /Comparison/:
  - Uniformity vs. Distinction.
  - Expressiveness.
  - Implementation in Coq and Agda.

Understanding both Russell and Tarski universes is important for working with advanced type systems and proof assistants, as they provide different approaches to organizing types in a consistent and expressive manner.

** satisfiable

A formula is satisfiable if there is an assignment that makes it true.

An assignment for a formula is a set of assignments (the process of give value, some people use validate) for variables in the formula. After giving value to variables, we can now decide the true/false of the formula (by writing down the truth table).

A (propositional) formula is unsatisfiable means that there is no way for it to be true.

Remark: The definition of satisfiable can be extended to predicate logic, where instead of assignment, we say /model/. I just don't remember the terminology.

** sigma type and Martin-Lof type theory

Martin-Lof appears after System F. Ch-isomorphism is integrated into syntax.

See also The Blind Spot 5.D Martin-Löf’s type theory.

** sigma algebra


A sigma algebra \(\mathscr A\) on a set \(X\) is a family of subsets of \(X\) with the following properties;

- \(X \in \mathscr A \)
- \(A \in \mathscr A\) implies \(A ^{c} \in \mathscr A\)
- \(\{A_{i}\}_{i \in I}\) where \(A_{i} \in \mathscr A\) for all \(i\), implies \(\bigcup A_{i} \in \mathscr A\)

A set \(A\) in \(\mathscr A\) is said to be measurable. or \(\mathscr A\)-measurable.

"a reasonable measure should be able to deal with disjoint countable paritions of sets".

This is just a model. Sigma algebra is based on boolean algebra or simply algebra. See [[id:2c7e830c-51e5-4974-ac56-4ed1cdffc1af][boolean algebra and sigma algebra]]. And be to aware, Set is just a model for this algebra.

** sigma algebra and de Morgan Law

We know that there is de morgan law in boolean algebra and set theory. We have that

De Morgan Law together with negation, union \(\Leftrightarrow\) The definition of intersection

De Morgan Law together with negation, intersection \(\Leftrightarrow\) The definition of union

** sigma algebra and intersection between sigma algebra

The intersection \(\bigcap _{i \in I} \mathscr A _{i}\) is a sigma algebra.

** sigma algebra and explicit definition of generators

Check on the remarks in book Measures, Intergral and Martingales. The author further introduce Dynkin system to further address the problem.

It is not trivial to write out the definition of generators of sigma algebras. Because it is countably infinite. As for boolean algebra, there is no such problem. Fascinating, right?

** simple typed lambda calculus: syntax (statics)

\(A\) 是类型、\(t\) 是 term （或者说 \(M\) 是 term）。我们的 judgment 形如 \(t \colon A\)，意思是 \(t\) 有着类型 \(A\)。

我们接下来有 \(t\) 和 \(A\) 的生成文法。我们知道 STLC 是 CCC，这是说，如果说 \(A_{1}, A_{2}\) 是类型，则 \(A_{1} \times A_{2}\) 也是类型，\(A_{1} \to A_{2}\) 也是类型。对于 \(t\) 来说，也有对应的。我们直接写出文法：
: t ::= x | () | ⟨t₁, t₂⟩ | π₁·t | π₂·t | λx.t    | (t₁)t₂    | yes | no
: M ::= x | ⟨⟩ | ⟨M₁, M₂⟩ | M·1  | M·2  | λ(x.M)  | ap(M₁;M₂) | yes | no

我们通过 \(t \colon A\) 来建立 ch 同构。这里的同构通过一些 rules 来表示：

axiom
:
: ----------------  ----------
: Γ, x : A ⊢ x : A  Γ ⊢ ⟨⟩ : 1

intro of pair
: Γ ⊢ M₁ : A₁   Γ ⊢ M₂ : A₂
: -------------------------
:  Γ ⊢ ⟨M₁, M₂⟩ : A₁ × A₂


sub / cut
: Γ, x : B ⊢ N : A    Γ ⊢ M : B   Γ ⊢ B   Γ, B ⊢ A
: -----------------------------   ----------------
:        Γ ⊢ N[M/x] : A                Γ ⊢ A

** simple typed lambda calculus: semantics (dynamics)

*** val rules

:
: ---------  --------
:  yes val    no val

:
: --------------  ------------
:  ⟨M₁, M₂⟩ val     λx.M val

*** dynamics rules

step fst & step snd:
:    M₁ ↦ M₂           M₁ ↦ M₂
: ------------      ------------
:  M₁⬝1 ↦ M₂⬝1       M₁⬝2 ↦ M₂·2

step fst β & step snd β
:
: ---------------  -------------------
: ⟨M₁, M₂⟩·1 ↦ M₁    ⟨M₁, M₂⟩·2 ↦ M₂

step app
:      M₁ ↦ M₁'
: -----------------
:  (M₁)M₂ ↦ (M₁)M₂'

step app β
:
: ----------------------
:  (λx.M₁)M₂ ↦ M₁[M₂/x]

: app[(arr M₁); M₂] → M₁ [M₂]

** simultaneous substitution \(\theta\)

A \(\theta\) can be applied to a formula or expression \(E\), but anyway I prefer \(A\) for a formula, \(\Gamma\) for a set of formulas.

\(\Gamma\theta\) means carry the substitution to every formulas in \(\Gamma\). We can compose those \(\theta\). For example, \(\theta\sigma\) means carrying \(\theta\) first and then \(\sigma\).

** substitution \(\gamma\) from

substitution \(\gamma\) from context \(\Gamma '\) to \(\Gamma\), written \(\gamma : \Gamma ' \to \Gamma\), is a finite map sending for each variable \(x \colon A _{x} \) in \(\Gamma\) to a term \(\Gamma' \vdash M_{x} \colon A_{x}\) in context \(\Gamma '\).

The substitution \(\gamma\) can be explicitly written out as \(\{x _{1} \mapsto M_{1} , x_{2} \mapsto M_{2}, \dots\}\) for variables \(x _{i} \in \Gamma\). The extension of \(\gamma\) by another variable-term pair is written \(\gamma \{x _{n} \mapsto M_{n}\}\).

Note that in \(\gamma \colon \Gamma ' \to \Gamma\) the arrow should not be understood as a function arrow in STLC, but a morphism in the syntactic category. This definition is sometimes also written as \(\Gamma ' \vdash \gamma : \Gamma\) and the readers should be able to recognize them interchangeably.

Given a substitution \(\gamma \colon \Gamma ' \to \Gamma\), the function sending a term \(M\) of context \(\Gamma\) to the term with the subtitutions applied is denoted \(\hat \gamma\).

can you tell me about this substitution?

Certainly! The concept of substitution in the context of type theory and lambda calculus is crucial for understanding how terms and types interact within a given context. Let's break down the key points of the substitution \(\gamma\) from context \(\Gamma '\) to \(\Gamma\).

*** Context and Substitution

1. *Context*:
   - A context \(\Gamma\) is a sequence of variable declarations, typically written as \(\Gamma = x_1 : A_1, x_2 : A_2, \dots, x_n : A_n\).
   - Each \(x_i\) is a variable, and \(A_i\) is its type.

2. *Substitution*:
   - A substitution \(\gamma\) from context \(\Gamma '\) to \(\Gamma\) is a mapping that assigns to each variable \(x_i\) in \(\Gamma\) a term \(M_i\) in context \(\Gamma '\).
   - Formally, \(\gamma : \Gamma ' \to \Gamma\) means that for each \(x_i : A_i\) in \(\Gamma\), there is a corresponding term \(\Gamma' \vdash M_i : A_i\) in \(\Gamma '\).

*** Explicit Representation

- The substitution \(\gamma\) can be explicitly written as a finite map: \(\{x_1 \mapsto M_1, x_2 \mapsto M_2, \dots, x_n \mapsto M_n\}\).

- This map indicates that each variable \(x_i\) in \(\Gamma\) is replaced by
  the term \(M_i\) in \(\Gamma '\).

*** Extension of Substitution

- The extension of \(\gamma\) by another variable-term pair is denoted \(\gamma \{x_n \mapsto M_n\}\).
- This means that \(\gamma\) is extended to include the new mapping \(x_n \mapsto M_n\).

*** Application of Substitution

- Given a substitution \(\gamma : \Gamma ' \to \Gamma\), the function that applies \(\gamma\) to a term \(M\) of context \(\Gamma\) is denoted \(\hat \gamma\).

- This function replaces each variable in \(M\) according to the mappings specified by \(\gamma\).

*** Example

Consider the following contexts and substitution:

- \(\Gamma = x : A, y : B\)
- \(\Gamma ' = z : C\)
- \(\gamma = \{x \mapsto z, y \mapsto \lambda w. w\}\)

Here, \(\gamma\) maps:
- \(x\) in \(\Gamma\) to \(z\) in \(\Gamma '\)
- \(y\) in \(\Gamma\) to \(\lambda w. w\) in \(\Gamma '\)

If we have a term \(M\) in context \(\Gamma\), say \(M = x \, y\), then applying \(\hat \gamma\) to \(M\) would result in:
\[ \hat \gamma(M) = z \, (\lambda w. w) \]

** sheaf

*Sheaf Axioms*:

   - *Identity*: If \( \{U_i\}_{i \in I} \) is an open cover of \( U \), and \( s, t \in \mathcal{F}(U) \) satisfy \( \rho_{U_iU}(s) = \rho_{U_iU}(t) \) for all \( i \), then \( s = t \).

   - *Gluing*: If \( \{U_i\}_{i \in I} \) is an open cover of \( U \), and for each \( i \), \( s_i \in \mathcal{F}(U_i) \) satisfies \( \rho_{U_i \cap U_j, U_i}(s_i) = \rho_{U_i \cap U_j, U_j}(s_j) \) for all \( i, j \), then there exists \( s \in \mathcal{F}(U) \) such that \( \rho_{U_iU}(s) = s_i \) for all \( i \).

** switch in proof net

For the parr rule, we can create (multiple) switching
proof net for a normal proof net.

A parr link originally looks like this
\[
\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
  A\ar[rd] && B \ar[ld]\\
  & A \parr B
\end{tikzcd}\]
a left or right switch is
\[\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
  A\ar[rd] && B \ar[ld, dashed]\\
  & A \parr B
\end{tikzcd}(L) \quad\begin{tikzcd}[column sep=tiny, every arrow/.append style={dash, line width=1pt, rounded corners}]
  A\ar[rd, dashed] && B \ar[ld]\\
  & A \parr B
\end{tikzcd}(R)\]
where the dashed means not reachable (the trip should not go through this link) we shall use the link to give definition for a correct proof net structure:

[[id:dec08962-fc1c-418a-b316-261704a583ce][proof net: correctness]]

** proof net: correctness
:PROPERTIES:
:ID:       dec08962-fc1c-418a-b316-261704a583ce
:END:

A correct proof net is such that for all switching net its generated, they are all acyclic and connected.

** presheaf

A functor from a category of open sets of a topological space to another category (e.g., sets, groups). It assigns data to each open set and specifies how data on smaller open sets restricts to larger ones.

A contravariant functor \( \mathcal{F} \) from the category of open sets of \( X \) (with inclusions as morphisms) to \( \mathcal{C} \):
   \[
   \mathcal{F}: \mathbf{Open}(X)^{\text{op}} \to \mathcal{C},
   \]
   where:
   - For each open set \( U \subseteq X \), \( \mathcal{F}(U) \) is an object in \( \mathcal{C} \).
   - For each inclusion \( V \subseteq U \), there is a restriction morphism \( \rho_{VU}: \mathcal{F}(U) \to \mathcal{F}(V) \).

** smt

SMT in the context of programming languages typically refers to *Satisfiability Modulo Theories (SMT)*, which is a formal system used in automated reasoning and verification. SMT solvers are tools that determine the satisfiability of logical formulas with respect to various background theories (e.g., arithmetic, arrays, bitvectors). They are widely used in program analysis, formal verification, and automated bug-finding.

** stable

stable means \(D, E \in \mathscr D\) implies \(D \cap E \in \mathscr D\)

** staircase function

\(f\colon I \to E\) is a staircase function on \(I\) if there is partition \(\Omega=\{\alpha_{0} ,\dots , \alpha _{n}\}\), that \(f\) is constant on every interval \((a_{i}, a_{i+1})\)

** substructural logic

The classic one: linear logic

See also bananaspace
See also CMU Course. But I don't like CMU course style though

** subobject

If i : A → B is a monomorphism, then we say that A is a subobject of B.
where \(i\) is the kernel of \(f\colon B\to C\)

** tait's heredity termination aka HT

\(HT_{A}(M)\) means that term \(M\) typed \(A\) has a normal form. This is the usage of HT.

\[HT_{A_1 \times A_2} (M) \Leftrightarrow M \mapsto \langle M_{1}, M_2 \rangle \]
and \(HT_{A_1} (M_1)\) \(HT_{A_2}(M_2)\)

This is the work of tait. Check Bob's introduction on this subject.

** tarski's theorem

Tarski's theorem states that for any monotone function \( f \) on a complete lattice \( L \), the set of fixed points of \( f \) forms a complete lattice. This means there exists a least fixed point and a greatest fixed point. In the context of powerset lattices (where elements are subsets of a set, ordered by inclusion), this theorem implies that any monotone function on the powerset has a well-defined set of fixed points, including a smallest and largest one.

Check Bob's introduction paper.

** tarski's universe

also check this link https://ncatlab.org/nlab/show/Tarski+universe

Tarski's universe, also known as a Tarski-Grothendieck universe, is a set \( U \) that satisfies the following properties:

1. *Transitivity*: If \( x \in U \) and \( y \in x \), then \( y \in U \).
2. *Closure under subsets*: If \( x \in U \), then \( \{y \in U \mid y \subseteq x\} \in U \).
3. *Closure under power sets*: If \( x \in U \), then \( \mathcal{P}(x) \in U \), where \( \mathcal{P}(x) \) is the power set of \( x \).
4. *Closure under unions*: If \( x \in U \), then \( \bigcup x \in U \).
5. *Closure under indexed unions*: If \( I \in U \) and \( \{x_i\}_{i \in I} \subseteq U \), then \( \bigcup_{i \in I} x_i \in U \).

Tarski's universe is used in set theory to provide a framework for handling large sets in a consistent manner. It is particularly useful in category theory and higher-order set theory, where it helps to avoid certain paradoxes associated with very large collections of sets.

[[id:8c4b6fbc-18b6-45ee-a751-fecd2ba6038a][Tarski Universes:]]

the relation between the tarski-grothendieck's universe and the tarski's universe in agda is that

** tensor product according to Bourbaki.

#+begin_quote
My advice is to work with multilinear functions in an appropriate setting, without trying to delve into the category-theoretic framework. This may become necessary in certain cases, but one should not forget that ideas are not to be found in diagrams. (true)
#+end_quote

We seek a space \(A \otimes B\) and bilinear function \(\otimes \colon A, B \mapsto A \otimes B\), with the following universal property: if \(G\colon A, B \mapsto C\) is another solution (bilinear function), then there is a unique linear \(H \colon A \otimes B \mapsto C\) such that \(G(a, b) = H(a \otimes b)\) for all \(a \in A, b  \in B\). We see by the way that the tensor product is of « inductive », positive, style.

ps. the def provided above is a bit vague. We use the def that everyone is using.

A tensor product of two spaces \(A\), \(B\), we find the tensor product of \(A\) and \(B\) \(A \otimes B\), together with a bilinear map \(\varphi\colon A \times B \to A \otimes B\). The object is the initial object.

\[
\begin{tikzcd}
  V \times W \arrow[r, "\varphi"] \arrow[rd, "h"]
    & V \otimes W \arrow[d, dashed, "\bar h"]\\
    & Z
\end{tikzcd}\]

It serves as « the biggest possible » image of a bilinear map.

How to prove the dimension of the tensor product is equal to the product of the dimensions of \(V\) and \(W\), that \(\dim V \times \dim W\). What is a bilinear map actually?

There is a vector in \(V \times W\), namely \((\sum a_{i} v_{i}, \sum b_{i} w_{i})\). It should be that

\[h(\sum a_{i} v_{i}, w) = \sum a_{i}h(v_{i}, w),\]

and also

\[h(v, \sum b_{i} w_{i} ) = \sum b_{i} h(v, w_{i}),\]

We should start from the basis. Let us say that the basis of \(V\) and \(W\) are respectively \(\alpha _{1}\dots \alpha _{n}\) and \(\beta _{1} \dots \beta _{m}\). When fix \(w = \beta _{j}\), \(v = \sum a_{i}\alpha _{i}\)

\[h(v, w) = \sum a_{i}h(\alpha _{i}, \beta _{j})\]

A linear combination of \(\big(h(\alpha _{i}, \beta _{j})\big)_{i \in I}\) with \(j\) fixated. Okay I think we can show that \(h(\alpha_{i} , \beta_{j}) _{i\in I _{n}, j \in I _{m}}\) are linear independent (at most). And that is the set of basis of \(\mathrm{Im}\, Z\), and thus using the fact that

\[h(v, w) = \bar h \cdot \varphi(v, w)\]

We may conclude that the basis of \(V \otimes W\) is of the same number as \(\dim V \times \dim W\).

I don't really know the definition of \(V \otimes W\).

** there is an assignment makes it true

There is an entry in truth table that is true.

** topology
:PROPERTIES:
:ID:       ea16267b-93d3-4fda-b0b4-1731b483b0a6
:END:

We can view a linear map as a functor, which preserving two kinds of computations. What about continuous map?

An open set in topological space suits that

1. if \(A\) and \(B\) is open, then \(A \cup B\) and \(A \cap B\) is open
2. \(X\) is open and \(\emptyset\) is open

** unification algorithm

unification algorithm is an effective algorithm used for search m.g.u.

Using the algorithm, we will find a mgu \(\sigma\) satisfies that for all unifier \(\theta\)
\[
\sigma \theta = \theta
\]
holds.

** unifier \(\theta\)

A unifer \(\theta\) for a set of expressions \(S:=\{E\sb{i}\}\), is such \(\theta\), that

\[
E\sb{1}\theta = \dots = E\sb{n}\theta
\]

** uniform space :analysis:

The definition of uniform space focus one the relation of "closeness". We have a relation on space \(X\), and the *family* of these relations is noted \(\cal U\). This is call entourage, which satisfy:

  1. for every \(U \in \cal U\), \((x , x) \in U\)
  2. for every \(U \in \cal U\), \((x, y)\in U\) implies \((y, x) \in U\)
  3. for every \(U, V \in \mathcal U\), there is a \(W \in \mathcal U\) that \(W \subset U \cap V\)
  4. if \(U \in \mathcal U\), and \(U \subset V\), then \(V \in \cal U\)

The axioms does not seem intuitive. But we can define entourage in metric space. I think a \(U\) in \(\cal U\) is defined by saying two points are in the same ball.
\[x \sim _{U} y \Leftrightarrow x, y \in B(a, \delta(U))\]
where \(a\) is a point in space \(X\).
we can verify the properties above.

** uniformly continuous :analysis:

Def (uniformly continuous). \(f \colon X \to Y\) is continuous. \(f\) is uniformly continuous if for each \(\epsilon\), there is a \(\delta(\epsilon)\) such that
\[d(f(x), f(y)) < \epsilon\]
where \(x, y\) are in a ball \(B(\delta(\epsilon))\) (remember of the definition of the open sets in metric spaces).

Def (uniformly continuous). \(f\colon X \to Y\) where \(X, \cal U\) and \(Y, \cal V\) are uniform spaces.

\(f\) is said to be uniformly continuous if for every  \(V \in \cal V\), there is an entourage \(U\) in \(\cal U\), such that
\[x\sim _{U} y \Rightarrow f(x) \sim _{V} f(y)\]

** characteristics function

A characteristic function \( \phi_X(t) \) of a random variable \( X \) is a
mathematical function that uniquely determines the probability distribution
of \( X \). It is defined as the expected value of \( e^{itX} \), where \( i \)
is the imaginary unit and \( t \) is a real number:

\[
\phi_X(t) = \mathbb{E}[e^{itX}]
\]

A characteristic function does not distinguish between vrs which differs on null sets.

** volume for interval in \(\mathbb R^{n}\)
:PROPERTIES:
:ID:       6ea8307c-098f-4bcf-a62e-6edb68a48e2e
:END:

\[\mathrm{vol}_{n}(a,b) := \begin{cases} \displaystyle \prod\nolimits_{j=1}^{n} (b_{j}-a_{j}),\quad & a \le b, \\
0  & \mathrm{otherwise} \end{cases}.\]

An interval in \(\R^{n}\) is like a cube. We can easily define the volume for a cube.

volume can be used in construing

** cramer's theorem for normal distribution

Theorem 3.15, known as Cramér's Theorem, states that if \( X \) and \( Y \) are independent random variables and their sum \( X + Y \) is normally distributed, then both \( X \) and \( Y \) must themselves be normally distributed.

** brownian motion :probability:

Definition 3.1. A Brownian motion is a continuous process that has stationary independent increments.


连续时间随机过程 \( B(t) \)，满足以下条件

1. *初始值为零*：\( B(0) = 0 \)。

2. *独立增量*：对于任意 \( 0 \leq t_1 < t_2 < \dots < t_n \)，增量 \( B(t_2) - B(t_1) \)，\( B(t_3) - B(t_2) \)，…，\( B(t_n) - B(t_{n-1}) \) 是相互独立的随机变量。

3. *平稳增量*：增量 \( B(t + h) - B(t) \) 的分布只依赖于时间间隔 \( h \)，而不依赖于 \( t \)。换句话说，增量的统计特性在时间上是均匀的。

4. （正态布朗运动）： *正态分布增量* 对于任意 \( t \geq s \geq 0 \)，增量 \( B(t) - B(s) \) 服从均值为 0、方差为 \( t - s \) 的正态分布，即：
   \[
   B(t) - B(s) \sim \mathcal{N}(0, t - s)
   \]

** exercise

*** Lax 3.1

Prove that gauge \(p_{K}\) have that \(p_{K}(x) < 1\) iff \(x\) is an interior. p.22

The proof uses some obvious fact and the definition of convex.

**** Proof (verbose)

"⇒" \(p_{K}(x) = \inf a\) where for \(a\), \(x / a \in K\). For every \(a' \in (a,1)\), we have
\[x / a' \in K\]
Consider convex combination of \(x / a'\) and \(y\)
\[\begin{aligned}& a' \cdot \frac{x}{a'} + (1-a')y \in K \\ \Rightarrow\ {}& x + (1-a')y \in K\end{aligned}\tag{1}\]
where \((1-a') \in (0, 1 - a)\). Because \(0\) is interior in \(K\), \(0 + t y \in K\) holds for \(t \in (-\epsilon, \epsilon)\) where \(\epsilon\)'s value depends on \(y\). Thus we denote that \(-k y \in K\) as the "negative" \(y\) where \(k\) is one of the positive \(t\).

Apply \((1)\) to "negative" \(y\), we have
\[x - k (1 - a') y \in K.\tag{2}\]
To satisfy the definition of interior, we take \(\epsilon = \min\{k(1-a), (1-a)\}\). Combine \((1)\) and \((2)\) to complete the answer.

"⇐" if \(x\) is an interior. Means that for \(y \in K\), there is such \(t\) that \(x + t y \in K\). Okay apparently this \(y\) can be \(x\) itself. Then we have
\[x + t x \in K, \qquad t \in (-\epsilon, \epsilon)\]
let \(t' > 0, t' \in (-\epsilon, \epsilon)\) then we have \(x/ (\frac{1}{1 + t'}) \in K\) where \(\frac{1}{1 + t'}\) is less than \(1\). Thus \(p_{K}(x)\) must be less than \(1\).

Optional: This is because \(\forall x \in A, \inf A \le x\). \(x/(\frac{1}{1 + t'})\) implies \(\frac{1}{1 + t'}\) is in \(A\). Immediately \(\inf A \le \frac{1}{1 + t'} < 1\) holds.

*** Lax 3.2

Theorem 4. Let \(p\) denote a positive homogeneous, subadditive function defined on a linear space \(X\) over the reals.

(i) The set of points \(x\) satisfying  \(p(x) < 1\)  is a convex subset of \(X\), and \(0\) is an interior point ofit.

(ii) The set ofpoints \(x\) satisfying \(p(x) \le 1\) is a convex subset of \(X\).

Check convex def.

**** Proof

(i) use homogeneous and subadditivity
Homo: for a positive \(a\), \(p(ax) = ap(x)\) holds. When \(x = 0\), \(p(0) = p(a \cdot 0) = a p(0)\). thus \(p(0)\) must be \(0\). Thus \(0 \in X'\)
for \(x,y \in X'\), consider convex sum \(w_1 x + w_2 y\).
\[p(w_1 x + w_2 y) \le p(w_1 x) + p(w_2y) = w_1 p(x) + w_2 p(y)\]

and \(w_1 p(x) + w_2p(y) < w_1 + w_2 = 1\). Thus we have \(w_1 x + w_2 y \in X'\)

(ii) is almost identical.

*** Amann IX.3.1

(1) show that \(I,J \in \mathbb J(n)\), then \(I \cap J \in \mathbb J(n)\)
(2) \(\{I_{i}\}_{i\in I}\) belongs to \(\mathbb J(n)\), and \(I\) is an interval such that \(I \subset \bigcup_{i}^{n}I_{i}\)
Then \(\mathrm{vol}_{n}(I)\le\sum^{k}_{j=0}\mathrm{vol}_{n}(I_{j})\).

*** Amann IX.3.2

(a) let \(\mu\)  be a measure on the Borel \(\sigma\)-algebra \(\mathcal B^{1}\) and suppose \(\mu\big((\infty, x)\big)\) is finite for \(x \in \mathbb R\). Further let

\[F_{\mu}(x) := \mu\big((-\infty, x)\big)\]
for \(x \in \mathbb R\)

show that \(F_{\mu}\) is a measure generating function with \(\lim_{x \to -\infty}\) \(F_{\mu}(x) = 0\)
: measure generating function is left continuous function and monotonic function

Proof. Apparently, the left limit \(\lim _{x \to a-}F_{\mu}(x) \) which is \(\lim _{x \to a -}\mu(-\infty, x)\). Because of sigma additivity. left continuity is obvious I mean.

And then we need to prove \(\lim _{x \to -\infty} F_{\mu}(x) = 0\). This is apparent since \(\lim _{x \to -\infty}(-\infty , x)\) is equal to \(\emptyset\). (And the limit can commute with \(F\), this is because \(\sigma\)-additivity) And since \(\mu (\varnothing) = 0\). The proof is done.

(b) Determine \(F_{\delta _{0}}\) where \(\delta _{0}\) denotes the Dirac measure on \((\mathbb R,\mathcal B^{1})\) with support at \(0\).

    +  Dirac measure means \(\delta_{0}(A) = 1\) if \(0 \in A\) otherwise \(\delta _{0} (A) = 0\)

Proof. You already know about this shit I think.

*** Amann IX.3.3

Suppose \(f \colon \R \to [0, \infty)\) in improperly integrable and

\[F_{f}(x) := \int ^{x}_{-\infty} f(\xi)\ d\xi,\]
for \(x \in \R\)

Verify that \(F_{f}\) is a measure-generating function for which \(\mu_{F_{f}}\big([a,b]\big)\)
\(= \int ^{b}_{a}f(\xi)\ d\xi\) when \(- \infty < a < b < \infty\).

Proof.

*** Amann IX.3.4 :#Amann:
:PROPERTIES:
:ID:       c7497fe1-ea96-46f2-a494-66af9d63a675
:END:

Properties of Hausdorff measure.

Suppose \(A \subset \R ^{n}\) Prove:

(a) \(\mathcal H^{s}_{*}(A) = \lim _{\epsilon \to 0+} \inf \big\{\sum ^{\infty} _{k=0}[\mathrm{diam}(A_{k})]^{s} \colon A _{k} \subset \R ^{n}\big\}\) where \(\mathrm{diam}(A_{k})\le \epsilon\), \(k \in \N\), \(A \subset \bigcup _{k} A_{k}\).

(b) If \( f : A \to \mathbb{R}^m \) is Lipschitz continuous with Lipschitz constant \( \lambda \), then
\[
\mathcal{H}^{s}_{*}(f(A)) \leq \lambda^s \mathcal{H}^{s}_{ * }(A).
\]

(c) For every isometry \( \varphi : \mathbb{R}^n \to \mathbb{R}^n \), we have
\[
\mathcal{H}^{s}_{ * }(\varphi(A)) = \mathcal{H}^{s}_{ * }(A).
\]
Thus, the Hausdorff outer measure on \( \mathbb{R}^n \) is invariant under isometries, that is, an invariant of motion.

(d) Suppose \( \bar n > n\) and \( \overline{\mathcal{H}}{}^{s}_{ * } \) is the Hausdorff outer measure on \( \mathbb{R}^{\bar n} \). Then \(\overline {\mathcal H }{}^{s}_{ * } (A) = \mathcal H ^{s} _{ * }(A)\).
That is, the Hausdorff outer measure is independent of the dimension of the ambient \( \mathbb{R}^n \).

* Topic: coherent space and linearity :noexport:
:PROPERTIES:
:CUSTOM_ID: coherence-and-linearity
:END:

** Scott Domain using logic

Scott Domain is initially postulate in the language of topology. However, Scott Domain is of no significance as a topological space. Also Scott Domain can be formulated with CPO (complete partially ordered set). The layer, in which topology or order set is used, is requiring not only a correct but proper language.

Girard defines the Scott Domain using the language of logic, where the relation between elements is formulated with provability. This alternative definition is of course useful, but should be mentioned later after you know about the "standard" definition of Scott Domain.

** The definition of coherent space: web and coherence

An "element" in a coherent space \(X\), is called a clique, which is a term in graph theory. The description contains two part for the coherent space \(X\): web and coherence

  - web: /web/ is a set, noted as \(|X|\). All the cliques are subsets of \(|X|\).
  - coherence: there is a reflexive and symmetric relation called /coherent/,
    denoted as \(\coh_{X}\) in coherent space \(X\).

A clique in a coherent space \(X\), is a subset of web, whose points are all coherent pair-wise. In space \(X\), \(a\) is a clique is noted as \(a \sqsubset X\).

And we obviously have that if \(a _{0} \subset a \sqsubset X\), then \(a_{0} \sqsubset X\).

** Stable Functions: Definition

A stable function \(F\) from \(X\) to \(Y\) is such function that

  1. \(a \sqsubset X\) \(\Rightarrow\) \(F(a) \sqsubset Y\)
  2. \(a \subset b \sqsubset X\) \(\Rightarrow\) \(F(a) \subset F(b)\)
  3. \(F(\uparrow \bigcup _{i}a_{i}) = {\uparrow} \bigcup _{i}F(a_{i})\)
  4. \(a \cup b \sqsubset X\) \(\Rightarrow\) \(F(a \cap b) = F(a) \cap F(b)\)

** Stable Functions: A categorical intuition

The (1)-(2) is actually saying that a stable function should be a functor. We can define a coherent space as a category where cliques are objects, and morphisms are inclusions. We can say "\(\Hom\)(\(a\), \(b\)) is not empty iff \(a\subset b\)"

There are more than one name for \({\uparrow}\bigcup_{i}a_{i}\), in proofs and types, this is called /filtered colimit/, and it is also called directed union. Stable function preserves this thing.

And also stable function preserves pullback in the coherent space. The pullback is defined given two morphism: one from \(\Hom(a, c)\), the other from \(\Hom(b, c)\). the pull back is defined by such diagram:
\[
\begin{tikzcd}
a \cap b \ar[r] \ar[d] \arrow[dr, phantom, "\lrcorner", very near start]
& a \ar[d]\\
b \ar[r] & a\cup b
\end{tikzcd}\]
The pullback of \(a \subset a \cup b\) and \(b \subset a \cup b\) is \(a \cap b\). The stable function preserves it.

** Coherent Space: A relation with topological space

Coherent space is originally described with topological space. Although coherent space as topology has no good property in general, we can use an idea of basis.

Okay, the idea is that all open sets can be expressed as the union of topological basis where all the basis are finite
\[O = \bigcup _{i} \alpha_{i}\]
The same is almost true for coherent space: a clique can be expressed by the union of finite cliques.

** Coherent Space: Minimal Approximant and Coding of Stable Function

The term approximant is used in proofs and types. Anyway, the general idea can be expressed as:

Let \(X\), \(Y\) be two coherent space, and \(F\) is a stable function from \(X\) to \(Y\). Let \(A \sqsubset X\) and \(y \in F(A)\). Then we have

  1. There exists a finite subset \(a \subset A\), such that \(y \in F(a)\)
  2. If \(a\) is chosen minimal, \(a\) is unique (that is for every \(a' \subsetneq  a\) \(y \notin F(a')\))

This is the proposition \(10\) in The Blind Spot. Such \(a\) can be called minimal approximant. What is it used for?

Well, the property is important, because it is firstly about finding the value of \(F(a)\). So I want to know if \(y\) is in \(F(a)\), we only need to check if the minimal approximant is in \(a\).

Secondly, it is about the coding of a stable function. For a value \(y\) in \(F(A)\), there is a unique minimal approximant \(a\). If we list all the "minimal approximant, value" pair, then we have the coding of the stable function \(F\)!
It is especially important because from now on, we can find a *one-one* correspondence between stable function space and \(X_{\mathrm{fin}} \times |Y|\) whose elements are composed of a finite clique in \(X\) and a point in \(Y\).

** Stable Functions: Skeleton and Trace

/Trace/ is the old name for upcoming concept. We use skeleton nowadays.

Given a stable function \(F\) from \(X\) and \(Y\), we can define the skeleton \(\mathrm{Sk}(F)\) of \(F\).

\[\mathrm{Sk}(F) := \{(a, y) \semicolon y \in F(a)\land \forall b\subsetneq a, y \notin F(b)\}.\]

The condition \(\forall b \subsetneq a, y \notin F(b)\) is /exactly/ saying that \(a\) is the minimal choice. The Sk of \(F\) is important because it is the coding of a stable function \(F\).

** Stable Functions: Skeleton and Berry Order

There is a definition for Berry Order in The Blind Spot:

\(F \sqsubset G\) iff for all \(a \subset b \sqsubset X\), \(F(a) = F(b) \cap G(a)\).

But this is uncomprehensible. We can use use Sk for the definition of Berry Order. \(F \sqsubset G\) iff \(\mathrm{Sk}(F) \subset \mathrm{Sk}(G)\). The proof can be seen in Proofs and Types page 64.

The Berry Order is said to preserve the pullback:

\[
\begin{tikzcd}
 & (G,a) \\
 (G ,a ') \ar[ru] && (F, a) \ar[lu] \\
 & (F, a')\ar[lu] \ar[ru]
\end{tikzcd}\]
For evaluation on \(( A \to B) \with A\).

** Coherent Space: Of CC and CCC

We can use coherent space to construct a CC and further more CCC.

What we need is two kinds of connectives: in this case, \(\with\) and \(\Rightarrow\). The former is called direct product (which supposed to be \(+\), but we use \(\with\) for notation in linear logic). The latter uses \(\mathrm{Sk}\).

** Coherent Space: Direct Product \(\with\)

This part can be checked on section 8.4 in Proofs and Types and section 8.2.7 in The Blind Spot. This part can be viewed as one step in constructing CC

We use the definition in The Blind Spot:

  - web: \(| X \with Y | := |X| + |Y|\)
  - coherence: if two points are from the same space, then they coherent iff
    they coherent in the original space. If two points are not from the same
    space, then they are coherent.

Proposition: cliques from \(X \with Y\) can be explained as the disjoint union of cliques from \(X\) and cliques from \(Y\). Note: a disjoint union of two sets.

** Coherent Space: the well-definedness of direct product

The well-definedness can be checked through a categorical view point.

A direct product is defined as
\[
\begin{tikzcd}
Z \ar[rrd, bend left, "F"] \ar[rdd, bend right, "G"] \ar[rd, "H"]\\
  & X \with Y \ar[r, "\pi_{1}"]
              \ar[d, "\pi_{2}"]
      &  X \\
  & Y
\end{tikzcd}\]
So we have to give definition for \(H\) and \(\pi_{1}, \pi_{2}\). The construct of \(H\) is extremely obvious.

** Coherent Space: As a CCC

We already have \(\with\) as the cartesian product, what about exponential?

We shall use \(\mathrm{Sk}\) for a stable map \(F\colon X \to Y\), since we already know that a Sk is a clique in space \(X _{\mathrm{fin}} \times |Y|\), we shall start from here to create coherent space \(X \Rightarrow Y\):

  - web: \(|X \Rightarrow Y| := X_{\mathrm{fin}} \times |Y|\), \(X_{\mathrm{fin}}\) being he finite cliques of \(X\).
  - coherence: for \((a,y)\) and \((a',y')\) if \(a \cup a' \sqsubset X\), then \(y  \coh y'\), if additionaly \(a \neq a'\), then \(y \scoh y'\).

The idea that Sk has a one-one correspondence with stable function, gives the idea that a stable function can be represented with a clique in space \(X \Rightarrow Y\).

** Stable Functions: Representation Theorem of Sk

Sk defiens a bijection between the stable functions from \(X\) into \(Y\) and the cliques of \(X \Rightarrow Y\). The reciprocal bijection associates to a clique \(C \sqsubset X \Rightarrow Y\) the stable function \((C) \cdot\) defined by

\[(C)A := \{ y \semicolon \exists a \subset A (a, y) \in C\}.\]

Moreover the bijection exchanges the Berry Order and Inclusion (by exchange it means Berry Order \(F \sqsubset G\) iff Sk(\(F\)) \(\subset\) Sk(\(G\)), we use such way to define Berry Order, so it might seem weird here).

After this we do admit that we can construct a CCC with coherent space.

** Linearity: The Easy Definition

Proofs and Types presents an easy definition of linearity that:

A stable function \(F\) from \(X\) to \(Y\) is /linear/ iff \(\mathrm{Sk}(F)\) consists of pair \((a, y)\) where \(a\) \(\in\) \(X_{\mathrm{fin}}\), but \(a\) is a singleton, that is, of form \((\{x\}, y)\) where \(x \in |X|\).

** Linearity: Linear implication

A linear function \(F\) whose Sk consists of pair \((a, y)\) where \(a\) is a singleton. We know that the first argument in \(\mathrm{Sk}\) can be an element in \(|X|\) rather than \(X_{\mathrm{fin}}\) since \(a\) is always a singleton in \(X\).

This notation is introduced in Proofs and Types as \(\mathcal T\kern-2pt\it rlin\), which remove the bracket of the singleton in Sk.

\[{\mathcal T \kern-2pt \it rlin} (F) = \{ (a , y)\semicolon a \in |X|, y \in F(\{a\})\}\]

With the similiar idea like \(X \Rightarrow Y\), we can now define a coherent space \(X \multimap Y\) for linear function:

  - web: \(|X \multimap Y | = |X | \times |Y|\)
  - coherence: \((x, y)\coh_{X \multimap Y} (x' , y')\) iff \(x \coh x' \Rightarrow y\coh y'\) and
  - \(\beta \incoh \beta '\Rightarrow a \incoh a'\)

Or alternatively, in The Blind Spot, we define the coherence as

\[
\begin{aligned}
(x , y) \coh _{X \multimap Y} (x ' , y') \Leftrightarrow {} & ( x \coh x ' \Rightarrow y \coh y') \\
& \land (x \scoh x' \Rightarrow y \scoh y').
\end{aligned}\]
It can also be written in single line:

\[(x, y) \scoh _{X \multimap Y} (x',y') \Leftrightarrow (x \coh x'\Rightarrow y\scoh y').\]

The \(\scoh\) is called strict coherent if you don't remember, \(x \scoh x'\) means that  \(x \coh x'\) and \(x \neq x'\).

** Linearity: The Symmetry Provided by Linear Implication

We first define linear negation as

  - web: \({\sim} A = |A|\)
  - coherence: \(a \coh _{{\sim} A} a'\) iff \(a \incoh _{A} a'\)

After we have linear negation, we can find a symmetrical structure for linear implication:

\[A \multimap B \simeq {\sim}B \multimap {\sim} A\]

The isomorphism being \((a, b) \mapsto (b, a)\).

** Linearity: Linearisation or Of Course!

I have no idea why Girard would call this "Of Course". Anyway, "Of Course!" is the linearisation of a space \(X\):

  - web: \(|{!} X| := X_{\mathrm{fin}} = \{a \sqsubset X \semicolon a\ \text{finite}\}\)
  - coherence: \(a \coh_{{!}X} a'\) iff \(a \cup a' \sqsubset X\)

For a clique \(a \subset X\), we can define a mapping from \(X\) to \({!} X\):
\[a \mapsto {!}a := \{a _{0} \semicolon a _{0} \subset a, a_{0}\ \text{finite}\}\]

With linearisation we easily have
\[X \to Y \simeq ({!}X)\multimap Y\]
Thus we can know that every stable function can be linearised to a linear function.

** Linearity: Why Not?

/Why Not?/ is considered as the dual of /Of Course!/. Here is the definition of \(?X\)

  - web: \(|{?} X |  = {!}({\sim} X)\)
  - coherence: \(a \scoh a' \Leftrightarrow a \cup a' \not \sqsubset {\sim} X.\)

The definition seems not so important. What matters is the dual:
\[
\begin{aligned}
{\sim} {!} X & {} = {?} {\sim} X,\\
{\sim} {?} X & {} = {!} {\sim} X.
\end{aligned}\]
can written as \(? X = {\sim} {!} {\sim} X\) and \(! X = {\sim} ? {\sim} X\). What is /Why Not?/ used for?

** Linearity: multiplicative connectives

1. web: \(| X \otimes Y | = | X \parr Y | := |X | \times |Y|\)
2. coh (times): \((x,y) \coh _{X \otimes Y} (x', y') \Leftrightarrow x \coh _{X} x' \land y \coh _{Y} y'\)
3. coh (parr) : \((x,y) \scoh _{X \parr Y} (x',y') \Leftrightarrow x \scoh _{X} x' \lor y \scoh _{Y} y'\)

* Topic :noexport:

** index 索引文章

文章连接：
- [[id:eaad00d6-a916-46d8-bfa6-f115db21da9b][国外的量化水平会比国内头部量化公司高很多吗？主要高在什么地方？]]
  -
- [[id:a7819a1a-69b5-4142-bd30-d2a0f9b78fe5][做高频，找一些小的市场，小的标的，测一些小 alpha 的因子做。]]
  - 看不出来到底是在干什么
- [[id:5101c40b-eaf8-4ca6-9923-afb6c51fd0fb][加密货币小白指南（2024 版）]]
  - 基本原则：稳
  - 分为两种交易：链上交易和中心化交易
  - 前者可以称为 DEX，后者称为 CEX
  - [[id:0072b0bf-82c2-4354-bac9-68f7997ccccc][CEX 和 DEX （AI 总结的）]]

** Black-Scholes

Black-Scholes option pricing model, emphasizing its mathematical foundation and practical significance. Key points:

1. *Objective*: Develop the Black-Scholes formula with mathematical rigor.
2. *Prerequisite*: Learn stochastic calculus, essential for option pricing.
3. *Applications*: Use stochastic calculus tools for advanced finance and
   economics problems.
4. *Impact*: The Black-Scholes model revolutionized finance, with trillions of
   dollars in options trades annually relying on it.
5. *Recognition*: Myron Scholes and Robert Merton won the 1997 Nobel Prize in
   Economics for this work (Fischer Black, a co-developer, had passed away in
   1995).

** derivative

Definition 1.2. A /derivative/ is a financial instrument whose value is determined by the value of something else.

  That is, a derivative is a financial object /derived/ from other, usually more basic, financial objects. The basic objects are known as assets. According to Higham [12], the term asset is used to describe any financial object whose value is known at present but is liable to change over time. A /stock/ is an example of an asset.

ref: Lectures on Stochastic Calculus with Applications to Finance

** bond

A bond is used to indicate cash invested in a risk-free savings account earning continuously compounded interest at a known rate.

** example of derivative: mutual fund, options, futures, and swaps

This example illustrates that a mutual fund can be viewed as a derivative because it is a financial product composed of multiple underlying assets (stocks and bonds). Key points:

1. /Derivative Nature/: Mutual funds are derivatives since their value depends on a portfolio of underlying investments.
2. /Risk Management/: Mutual funds help investors diversify risk by spreading investments across various assets.
3. /Accessibility/: They are suitable for investors who lack capital for individual stock investments or prefer not to focus on a single asset.
4. /Market Presence/: Banks like TD Canada Trust offer mutual funds alongside other investment products.

ref: Lectures on Stochastic Calculus with Applications to Finance

** CWB

This example highlights a real-world application of options in agriculture, specifically through the Canadian Wheat Board's Guaranteed Delivery Contract. Key points:

1. /Contract Mechanism/: A farmer and the CWB agree on a fixed price for
   barley delivery in the future (e.g., December).

2. /Farmer's Perspective/: The farmer benefits if the market price in December
   is lower than the contract price, earning more than market value.

3. /CWB's Perspective/: The CWB profits if the market price in December is
   higher, allowing them to sell the barley at a higher price.

4. /Option Nature/: This contract is essentially an option, giving the farmer
   the right to sell barley at a predetermined price.

5. /Valuation Problem/: Determining the fair price for such an option is a
   fundamental financial problem, crucial for both parties.

This example demonstrates how options can be used in non-traditional markets
like agriculture, with implications for pricing and risk management.

** options

The text explains that an option is a financial contract giving the buyer the
right, but not the obligation, to buy (call option) or sell (put option) an
asset at a fixed price (strike price \( E \)) by a future date (expiration
time \( T \)).

- /Call Option/: The buyer profits if the asset's market price at \( T \) is
  higher than \( E \), exercising the option to buy at \( E \) and sell at the
  higher market price. If the market price is lower, the option is not
  exercised.

- /Put Option/: The buyer profits if the asset's market price at \( T \) is
  lower than \( E \), exercising the option to sell at \( E \) and buy at the
  lower market price. If the market price is higher, the option is not
  exercised.

The key question is determining the fair price of the option at time 0, which
is essential for both buyers and sellers to manage risk and maximize returns.

** american/european call and put

Definitions 1.5 and 1.6 describe European call and put options, which grant
the holder the right, but not the obligation, to buy or sell a specific asset
at a predetermined price (strike price) on a future date (expiry date).

- /European Call Option/: The holder can buy the asset from the writer at the
  strike price on the expiry date.
- /European Put Option/: The holder can sell the asset to the writer at the
  strike price on the expiry date.

The term "European" distinguishes these options from "American" options, which
can be exercised at any time between the start and expiry dates. Notably:

- /American Call Options/: Have the same value as European call options, as
  early exercise is rarely optimal.
- /American Put Options/: Are more complex and potentially more valuable than
  European puts, as early exercise can be beneficial.

These distinctions are crucial for understanding option pricing and behavior.

ref: Lectures on Stochastic Calculus with Applications to Finance

** portfolio

A portfolio refers to a combination of:

1. /Assets/: Stocks or other securities.
2. /Options/: Call or put options.
3. /Cash/: Money invested in a bank or bonds.

** asset

Since an asset is defined as a financial object whose value is known at present but is liable to change over time, we see that it is reasonable to model the asset price (i.e., stock price) by a stochastic process \(\{S_{t}, t \ge 0\}\). There will be much to say about this later.

** no arbitrage principle

- /No-Arbitrage Principle/: A fundamental assumption in finance stating that
  there are no risk-free opportunities to earn returns higher than those from
  bank deposits. This ensures markets are efficient and prevents "free
  lunches."

same asset could have different prices at different markets. Exploiting that difference is called arbitrage.

** arbitrage with example


This example illustrates the concept of arbitrage using currency exchange rates. Key points:

1. /Exchange Rate Discrepancy/: If the exchange rate differs between two
   markets (e.g., $1.60 = £1 in Toronto vs. $1.58 = £1 in London), it creates
   an opportunity for arbitrage.

2. /Arbitrage Strategy/: The company could buy pounds in London for $1.58 and
   sell them in Toronto for $1.60, making a risk-free profit of $0.02 per
   pound.

3. /Unlimited Profit/: This strategy could be repeated indefinitely, leading
   to unlimited profits.

4. /Market Correction/: Such discrepancies are rare and short-lived, as market
   forces quickly adjust prices to eliminate arbitrage opportunities.

The example emphasizes the no-arbitrage principle, where markets efficiently
correct price differences to prevent risk-free profits.


text: Suppose that a company has offices in Toronto and London. The exchange rate between the dollar and the pound must be the same in both cities. If the exchange rate were $1.60 = £1 in Toronto but only $1.58 = £1 in London, then the company could instantly sell pounds in Toronto for $1.60 each and buy them back in London for only $1.58 making a risk-free profit of $0.02 per pound. This would lead to unlimited profit for the company. Others would then execute the same trades leading to more unlimited profit and a total collapse of the market! Of course, the market would never allow such an obvious discrepancy to exist for any period of time.

** law of one price

- /Law of One Price/: States that identical goods should have the same price
  in an efficient market, preventing arbitrage opportunities.

- /Example of Violation/: Gasoline prices often differ between gas stations in
  the same area (e.g., Regina), contradicting the law of one price.

- /Challenge/: Exploiting such price discrepancies legally can be complex,
  involving factors like transaction costs, timing, and market dynamics.

** normal distribution :probability:

density function

\[f_{Z}(z) = \frac{1}{\sqrt{2\pi}}e ^{-\frac{z^{2}}{2}}, - \infty < z < +\infty\]

or more generally,

\[f_{X}(x) = \frac{1}{\sigma\sqrt{2\pi}}e ^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}, - \infty < x < +\infty\]

\(X \sim \mathcal N (\mu, \sigma^{2})\).

** standard nomral cumulative dis :probability:

\[\Phi (z) = \int ^{z} _{-\infty} \frac{1}{\sqrt{2\pi}} e ^{-\frac{x^{2}}{2}}dx\]

\(\Phi(z) = \mathbf P\{Z \le z\} = F_{Z}(z)\) is the dis function of a random variable \(Z \sim \mathcal N (0,1)\).

** FMZ

A beginner platform.

** jupyter notebook in emacs

Please use pandoc to transfer from notebook to org mode.
Or org mode to notebook.

Or ein to use jupyter notebook

** ccxt

It is an open library to access to exchange platform.

希望什么时候能够入场

** Limit Order 和 Market Order

Limit Order (限价订单): 限价订单是指交易者指定一个特定的价格，只有当市场价格达到或超过这个价格时，订单才会被执行。这种订单类型允许交易者控制他们愿意支付或接受的价格。

Market Order (市场订单): 市场订单是指交易者不指定价格，而是以当前市场价格立即执行的订单。这种订单类型确保了交易的即时性，但交易者无法控制具体成交价格，可能会高于或低于预期。

** Limit Buy Order 和 Limit Sell Order

Limit Buy Order (限价买单): 指交易者指定一个低于当前市场价格的价格，只有当市场价格下跌到或低于这个指定价格时，买单才会被执行。

Limit Sell Order (限价卖单): 指交易者指定一个高于当前市场价格的价格，只有当市场价格上涨到或高于这个指定价格时，卖单才会被执行。

Market Price (市场价格): 指当前市场上实际交易的价格，是买卖双方在市场上达成的即时成交价格。

/优点/: 限价订单允许交易者控制他们愿意支付或接受的价格，避免以不利价格成交。

/缺点/: 限价订单可能不会立即执行，需要等待市场价格达到指定水平，这可能导致交易延迟或无法成交，特别是在市场价格快速变动的情况下。

** Taker 和 Maker

在高频交易中，交易者通常分为两种角色：Maker（做市商）和 Taker（吃单者），每种角色都有其独特的策略和盈利模式。

*Maker策略*：
Maker策略的核心是通过放置限价订单（limit order）来提供市场流动性，从而成为市场的做市商。做市商在市场上同时提供买入和卖出的报价，确保市场上有足够的买卖双方，从而促进交易的进行。做市商的主要盈利来源有两个：
1. *Spread（买卖价差）*：做市商通过设置略高于市场价格的卖价（ask）和略低于市场价格的买价（bid），从中赚取买卖价差。这个价差是做市商的主要收入来源之一。
2. *Rebate（回扣）*：许多交易所为了鼓励做市商提供流动性，会向做市商提供交易费用的回扣。做市商通过提供流动性，可以获得这部分额外的收入。

Maker策略的优点在于，它能够稳定地从市场流动性中获利，风险相对较低。然而，缺点是做市商需要承担库存风险，即持有的股票可能会因为市场波动而贬值。

*Taker策略*：
Taker策略则依赖于快速的市场预测能力（alpha），通过使用市场订单（market order）立即执行交易，迅速吃掉市场上的限价订单（limit order）。Taker的目标是利用短期市场趋势或价格波动，快速获取利润。

例如，如果Taker预测某只股票的价格即将上涨，他们会立即使用市场订单买入市场上的最佳卖价（best ask），以便在价格上涨后迅速卖出获利。Taker策略的关键在于快速执行交易，因为他们的alpha（市场预测能力）往往会在短时间内衰减（decay），如果不及时执行，可能会错失获利机会。

Taker策略的优点在于，它能够快速捕捉市场机会，尤其是在市场波动较大时，能够迅速实现利润。然而，缺点是Taker需要具备非常强大的市场预测能力，并且由于市场订单的即时性，可能会以不利的价格成交，尤其是在市场流动性不足的情况下。

** Limit Order Book 和 Order 的来源

在高频交易和金融市场运作中，Quotes（报价）和 Trades（交易）是两个核心概念，它们共同构成了市场的基础。为了更好地理解这些概念，我们可以从Limit Order Book (LOB) 和 Market Order 的角度进行深入分析。

Limit Order Book (LOB)：
LOB是所有限价订单（limit order）的集合，它记录了市场上所有未成交的买卖订单。每个订单都包含两个关键信息：价格和数量。LOB将这些订单按照价格优先、时间优先的原则进行排序，形成一个买卖双方的报价列表。买方报价（bid）通常显示在左侧，卖方报价（ask）显示在右侧，中间的价差（spread）则是市场流动性的体现。

LOB的核心作用是为市场提供透明度和流动性。通过LOB，交易者可以清楚地看到当前市场上所有买卖双方的报价，从而做出更明智的交易决策。做市商（Maker）通常会通过在LOB中放置限价订单（Limit）来提供流动性，而吃单者（Taker）则通过市场订单（Market）来立即执行交易。

/Market Order/：
市场订单（market order）是一种不指定价格、以当前市场价格立即执行的订单。市场订单的本质是驱动市场交易的主要力量，因为只有当市场订单出现时，LOB中的限价订单才会被撮合成交，形成实际的交易（trade）。换句话说，市场订单是推动市场价格变动和交易发生的关键因素。

/Trade 的生成/：
所有的交易（trade）都是由市场订单在LOB中撮合限价订单的结果。当一个市场订单进入市场时，它会与LOB中最优的限价订单进行匹配，从而生成一笔交易。例如，一个市场买单会与LOB中最优的卖价（best ask）进行匹配，而一个市场卖单则会与LOB中最优的买价（best bid）进行匹配。

需要注意的是，虽然限价订单可以设置优于市场价格的价格（即高于当前买价或低于当前卖价），但这些订单本质上仍然是通过市场订单来执行的。为了简化理解，我们可以将这些优于市场价格的限价订单视为市场订单，因为它们一旦进入市场，就会立即被执行，形成交易。

/市场驱动的本质/：
市场的本质是由市场订单驱动的。市场订单的即时性和执行力使得市场价格能够根据供需关系实时调整。限价订单虽然为市场提供了流动性，但它们本身并不会直接导致交易的发生，只有当市场订单出现时，限价订单才会被撮合成交，市场价格才会发生变化。

** Limit Order Book (LOB) and Microstructure

*Limit Order Book (LOB)* ，即限价订单簿，是金融市场中最基础的 microstructure 之一。它是一个集中展示所有未成交限价订单（Limit Order）的信息系统，包含了买入和卖出的限价订单，并按照价格和时间优先级进行排序。简单来说，LOB就像一个“账本”，记录了市场上所有买卖双方在不同价格点上的挂单情况。

*** LOB的结构与功能

LOB的核心功能是为市场提供透明度和流动性。通过LOB，交易者可以清楚地看到当前市场上所有买卖双方的报价，从而做出更明智的交易决策。LOB的结构通常如下：

- *横轴*：表示买卖双方的报价（bid和ask）。
- *纵轴*：表示在每个价格点上的挂单数量和顺序，即该价格上的队列（queue）。

LOB中的买单和卖单统称为*quotes（订单）*。每个订单都包含两个关键信息：价格和数量。买单（bid）通常显示在左侧，卖单（ask）显示在右侧，中间的价差（spread）则是市场流动性的体现。

*** Best Quotes与Spread

在LOB中，最简单的分析通常从 *第一档best quotes* 开始。*best bid*是市场中可用于出售该股票的最高价格，而*best ask*是任何特定时间内买入该股票的最低价格。例如，如果best bid是$20.07，best ask是$20.09，那么买卖双方之间存在一个$0.02的价差（spread）。

*Spread*是衡量市场流动性的重要指标。较小的spread通常意味着更高的流动性，因为买卖双方更容易在接近的价格点上达成交易。反之，较大的spread则表明市场流动性较低，交易者可能需要支付更高的成本才能完成交易。

*** 为什么买单价格比卖单价格低？

买单价格比卖单价格低的原因很简单：买方希望以尽可能低的价格买入，而卖方希望以尽可能高的价格卖出。这种供需之间的博弈导致了买卖双方在价格上的差异。如果bid order的价格高于ask order的价格，那么这两个订单会立即成交（bid/ask crossed），这种情况通常不会显示在LOB上，因为它们已经变成了实际的交易。

*** Crossed Market与Uncross

在金融市场中，术语*"crossed market"*和*"uncross"*有特定的含义：

- *Crossed Market*：当买入价（bid price）高于卖出价（ask price）时，市场被称为"crossed market"。这种情况通常表明市场处于不稳定状态，可能会很快发生交易。

- *Uncross*：当买入价和卖出价之间的差距被消除并且订单被匹配成交时，这个过程称为"uncross"。此时，市场恢复到一个非跨价状态（non-crossed state）。

*** LOB的工作原理

LOB的工作原理可以通过一个简单的例子来解释。假设市场上有以下LOB结构：

- *Best bid*：$20.07，挂单数量为1200股。
- *Best ask*：$20.09，挂单数量为1000股。

如果此时有一个大的*market sell order*进入市场，它将首先匹配best bid上的所有限价订单。假设market sell order的数量为1500股，那么它将首先吃掉best bid上的1200股，然后继续吃掉次一档的bid订单。此时，best bid的价格会下降到下一个价格档位，spread也会相应变宽。

相反，如果此时有一个新的*limit buy order*进入市场，且其价格高于当前的best bid，那么这个limit buy order将成为新的best bid，spread也会变窄，表明市场流动性得到了增强。

*** 流动性的定义与衡量

*流动性* 是金融市场中的一个核心概念，尽管每个人都在谈论它，但很少有人能给出明确的定义。根据这本书的解释，流动性是指“在特定交易场所中，所有限价订单在任何给定时间能够满足新进市场订单的累计交易量”。简单来说，流动性衡量的是交易者在特定价格点上买卖股票的容易程度。

流动性的衡量标准包括：

- *Bid和Ask的数量*：在各个价格点上的挂单数量越多，流动性越高。
- *Spread的大小*：较小的spread通常意味着更高的流动性。
- *交易活跃度*：交易频率越高，流动性越好。
- *Market Impact*：下单对市场价格的影响越小，流动性越高。

*** 总结

LOB是金融市场的微观基础，它通过集中展示买卖双方的限价订单，为市场提供了透明度和流动性。通过分析LOB中的best quotes、spread、cross和uncross等概念，交易者可以更好地理解市场的运作机制，并做出更有效的交易决策。流动性的高低直接影响市场的效率和交易成本，因此，深入理解LOB和微观结构对于任何参与金融市场的交易者来说都是至关重要的。

** tick
:PROPERTIES:
:CUSTOM_ID: 介绍
:END:

高频的数据分为 tick 逐笔数据 (比如美股) 和 snapshot 快照数据 (比如 A 股)。tick 指的其实就是一个 update 的 [[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=data+feed&zhida_source=entity][data feed]]，这个 update 可以是最新变动的 quotes 或者 trade 的价格，订单大小和成交量信息，然后高频频数据就是一系列连续 "ticks" 的集合。Tick 数据通常具有以下属性：

- 时间戳
- 股票代码
- 买价
- 卖价
- 可用买单数量
- 可用卖单数量
- 最后交易价格
- 最后交易数量

时间戳记录了报价产生的日期和时间。它可能是交易所或经纪商发布报价的时间，也可能是[[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=%E4%BA%A4%E6%98%93%E7%B3%BB%E7%BB%9F&zhida_source=entity][交易系统]]收到报价的时间。书里说客户下单到交易所再返回客户并确认订单接收的标准 "往返" 时间在纽约为 15 毫秒或更少。如果经纪商无法以这种现在的标准速度处理订单，客户可能会解雇他们。因此，复杂的报价系统会在时间戳中包括毫秒甚至微秒。

最后交易价格显示了股票最后成交的价格。最后交易价格可能与买价和卖价不同。当客户发布一个有利的 limit order 并立即被经纪商匹配，而不广播客户的报价时，可能会产生差异。最后交易数量显示了最后执行交易的实际数量。

Tick 数据也包含了每次 quote 或者 trade 更新时的 order book 的信息，其实也就是 Level1 和 Level2 数据。Level1 的数据只包括了 quotes 第一档所有的变化信息，比如 best bid price&size， best ask price&size 以及 last trade price&size。Level2 的数据除了包括前面 Level1 里 order book 第一档的所有变化，还包括了远离市场价格 (第二档，第三档以至于第十档) 的 ask 和 bid 的信息。

就比如下面一开始的那张图里，Level1 只包含 best bid@$20.07 和 best ask@$20.09 这两个 queue 的信息，而 Level2 就可以看到整张图，比 Level1 的信息量更多，能做的分析和建模也更多，但自然 Level2 比 Level1 的数据也是贵很多。

[[https://pic4.zhimg.com/v2-f0bca347f70568261d4a19b3df03f207_1440w.jpg]]

其实还有更贵的 Level3 的数据，包括的市场信息会更多。除了包含 Level1 和 Level2 的数据以外，Level3 还会包括比如订单 ID；提交订单的市场参与者的标识，是[[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=%E5%81%9A%E5%B8%82%E5%95%86&zhida_source=entity][做市商]]、流动性提供者、散户还是机构投资者；订单被提交的准确时间戳；订单是买单还是卖单，[[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=%E9%99%90%E4%BB%B7%E5%8D%95&zhida_source=entity][限价单]]还是市价单等；订单当前的状态，如未执行、部分执行、完全执行或已取消；订单的指定价格和指定数量等等。这些信息几乎可以让整个市场的交易变得透明，这样拥有这些数据的人就有了巨大的优势。

我们来看一个书里的例子 tick 数据长什么样子：

[[https://pic4.zhimg.com/v2-a7279537860750465801b3082a716eab_1440w.jpg]]

图 4.1 的 A 和 B 展示了纽约证券交易所 (NYSE) 在 2009 年 11 月 9 日 14:00:16:400 至 14:02:00:000 GMT 期间为 S&P 500 [[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=ETF&zhida_source=entity][ETF]] [[https://zhida.zhihu.com/search?content_id=244130853&content_type=Article&match_order=1&q=SPY&zhida_source=entity][SPY]] 记录的 30 秒一级高频数据日志。A 图显示了报价数据：最佳买价、最佳卖价和最后交易信息，而图 B 显示了相应的持仓规模 (最佳买单数量、最佳卖单数量和最后交易数量)。我们可以看到每次的 bid/ask/trade 的 price 或者 size 有任何变化的时候，都会产生一个 tick 的 update，然后我们收到的 tick 数据就是所有的这些更新的信息。


1.2.1 介绍

高频交易的数据主要分为两种类型：/tick 逐笔数据/（如美股）和 /snapshot快照数据/（如A股）。Tick数据指的是每次市场更新时产生的数据流（data feed），这些更新可能是最新变动的报价（quotes）或交易（trades）的价格、订单大小和成交量信息。高频数据就是一系列连续“ticks”的集合，每个tick记录了市场在某一时刻的状态。

*** Tick数据的属性

Tick数据通常包含以下属性：

- /时间戳/：记录报价产生的日期和时间。时间戳可以是交易所发布报价的时间，也可以是交易系统收到报价的时间。在纽约，客户下单到交易所再返回客户并确认订单接收的标准“往返”时间通常为15毫秒或更少。如果经纪商无法以这种速度处理订单，客户可能会解雇他们。因此，复杂的报价系统会在时间戳中包括毫秒甚至微秒。
- /股票代码/：标识具体的股票。
- /买价（bid price）*：当前市场上最高的买入报价。
- /卖价（ask price）*：当前市场上最低的卖出报价。
- /可用买单数量（bid size）*：在买价上可用的订单数量。
- /可用卖单数量（ask size）*：在卖价上可用的订单数量。
- /最后交易价格（last trade price）*：股票最后成交的价格。这个价格可能与买价和卖价不同，尤其是在客户发布有利的限价订单并立即被经纪商匹配时。
- /最后交易数量（last trade size）*：最后执行交易的实际数量。

*** Tick数据与Order Book

Tick数据还包含了每次报价或交易更新时的订单簿（order book）信息，通常分为*Level1*和*Level2*数据。

- /Level1数据/：只包含订单簿第一档的所有变化信息，如最佳买价（best bid price）、最佳卖价（best ask price）以及最后交易价格（last trade price）和数量（last trade size）。
- /Level2数据/：除了包含Level1中的信息外，还包括远离市场价格的第二档、第三档甚至第十档的ask和bid信息。Level2数据提供了更详细的市场深度，使得交易者能够更全面地分析市场。

例如，在下面的图中，Level1数据只包含best bid@$20.07和best ask@$20.09这两个队列的信息，而Level2数据则可以看到整张图，信息量更大，能够进行更复杂的分析和建模。当然，Level2数据的价格也比Level1数据贵得多。

[[https://pic4.zhimg.com/v2-f0bca347f70568261d4a19b3df03f207_1440w.jpg]]

*** Level3数据

除了Level1和Level2数据，还有更昂贵的*Level3数据/。Level3数据不仅包含Level1和Level2的所有信息，还包括以下额外信息：

- /订单ID/：每个订单的唯一标识符。
- /市场参与者标识/：提交订单的市场参与者的身份，如做市商、流动性提供者、散户或机构投资者。
- /订单提交时间/：订单被提交的准确时间戳。
- /订单类型/：是买单还是卖单，是限价单还是市价单。
- /订单状态/：如未执行、部分执行、完全执行或已取消。
- /指定价格和数量/：订单的指定价格和指定数量。

这些信息几乎可以让整个市场的交易变得透明，拥有这些数据的人可以获得巨大的优势。

*** 示例：Tick数据

我们来看一个书中的例子，展示tick数据的具体内容：

[[https://pic4.zhimg.com/v2-a7279537860750465801b3082a716eab_1440w.jpg]]

图4.1的A和B展示了纽约证券交易所（NYSE）在2009年11月9日14:00:16:400至14:02:00:000 GMT期间为S&P 500 ETF（SPY）记录的30秒一级高频数据日志。A图显示了报价数据：最佳买价、最佳卖价和最后交易信息，而图B显示了相应的持仓规模（最佳买单数量、最佳卖单数量和最后交易数量）。我们可以看到，每次bid/ask/trade的price或size有任何变化时，都会产生一个tick的update，我们收到的tick数据就是所有这些更新的信息。

*** 总结

Tick数据是高频交易的核心数据源，它提供了市场在每个微小时间点的详细状态。通过分析tick数据，交易者可以捕捉到市场的细微变化，从而做出更精确的交易决策。无论是Level1、Level2还是Level3数据，它们都为交易者提供了不同层次的市场透明度和流动性信息，帮助他们在高频交易中占据优势。

** CEX 和 DEX （AI 总结的）
:PROPERTIES:
:ID:       0072b0bf-82c2-4354-bac9-68f7997ccccc
:END:

**DEX（去中心化交易所）和CEX（中心化交易所）的总结*

*1. /定义和特点/：*

- *DEX（去中心化交易所）*：
  - /定义/：直接在区块链上运行的交易所，无需中介。
  - /特点/：
    - 去中心化：用户掌握私钥，无需信任第三方。
    - 透明：所有交易记录公开，可验证。
    - 自主：用户直接与智能合约交互，交易自动执行。
    - 低成本：在某些区块链（如BSC）上，交易费用较低。

- *CEX（中心化交易所）*：
  - /定义/：由中心化机构运营的交易所，用户通过平台进行交易。
  - /特点/：
    - 中心化：平台托管用户资金，用户需信任平台。
    - 高效：交易速度快，流动性高。
    - 用户友好：界面简单，适合新手。
    - 多样性：支持多种加密货币和法币交易。

*2. /常见平台/：*

- *DEX*：
  - /Uniswap/：基于以太坊的DEX，支持ERC-20代币交易。
  - /PancakeSwap/：基于币安智能链（BSC）的DEX，手续费较低。
  - /SushiSwap/：另一个基于以太坊的DEX，提供流动性挖矿和代币交易。

- *CEX*：
  - /Binance/：全球最大的加密货币交易所，支持多种加密货币交易。
  - /Coinbase/：美国知名交易所，安全性高，适合初学者。
  - /Kraken/：提供多种交易对和高级交易功能，适合有一定经验的用户。

*3. /操作流程/：*

- *DEX*：
  - 设置钱包（如MetaMask、Trust Wallet）。
  - 连接钱包到DEX平台。
  - 存入资金到钱包。
  - 选择交易对，执行交易。
  - 支付矿工费（Gas Fee）。

- *CEX*：
  - 注册账户并完成身份验证（KYC）。
  - 存入资金（银行转账、信用卡或加密货币转账）。
  - 选择交易对，执行交易。
  - 查看交易记录和账户余额。

*4. /优缺点/：*

- *DEX*：
  - /优点/：自主控制、透明、安全、创新机会。
  - /缺点/：Gas Fee较高、技术门槛较高、市场波动大。

- *CEX*：
  - /优点/：高效、用户友好、多样性、流动性高。
  - /缺点/：中心化风险、需信任平台、可能存在监管风险。

*5. /适用人群/：*

- *DEX*：适合有一定技术基础、追求自主控制和透明交易的用户。
- *CEX*：适合新手、追求高效交易和多样化选择的用户。

总结来说，DEX和CEX各有优缺点，选择哪种平台取决于你的技术水平、风险偏好和交易需求。DEX提供去中心化和透明的交易体验，而CEX则提供高效和用户友好的交易环境。

* Topic ：初探线性逻辑

** Binary Connectives

Girard 认为 classical 和 intuitionistic 都没有很好的处理 situation 这个概念。我们可以从资源的角度出发，situation 指的就是一些“资源”，而 action 能够对这些“资源”进行变化。可以使用化学方程式的类比：
2H₂ + O₂ = 2H₂O
从资源管理的角度出发，等式两边需要配平。如果使用 Linear Logic 的 connectives 就有：
\[\mathrm H_{2} \otimes \mathrm H_{2} \otimes \mathrm O _{2} \multimap \mathrm H _{2} \mathrm O \otimes \mathrm H _{2} \mathrm O\]
在这里 \(\multimap\) 的左边的资源被消耗了，得到的右边的资源。可以认为 \(\multimap\) 是资源的转化。

这里先介绍 Linear Connectives
1. Linear And （或者说 Tensor Product ，或者说 times ，写作 \(\otimes\)）
2. Classic And （或者说 Direct Product ，或者说 with ，写作 \(\with\)）
3. Linear Or （或者说 Tensor Sum ，或者说 parr ，写作 \(\parr\)）
4. Classic Or （或者说 Direct Sum 或者说 plus ，写作 \(\oplus\)）

我们使用的是上面最后一个称呼（times 、with...）。我们需要注意到资源的问题，对于 linear 类型的 connective 来说 \(A \star B\) 来说，需要 \(A\) 和 \(B\) 的资源同时可用。而对于 classic 的就不是如此。比如说这样的 rule 是允许的 \(\displaystyle \frac{\vdash \Gamma , A\quad \vdash \Gamma , B}{\vdash \Gamma, A \mathbin{\&} B}\) （注：因为不是默认使用 intuitionistic 的，所以我们可以使用 one side rule），而对于 \(\otimes\) 上面是不行的，得使用 \(\displaystyle\frac{\vdash \Gamma , A\quad \vdash \Delta , B}{\vdash \Gamma, \Delta, A \otimes B}\).

在这种情况之下 structural rules （weakening rule 和 contraction rule） 就是对“资源守恒”的明显的破坏：
1. weakening 相当于无中生有创造了资源
2. contraction 相当于将资源销毁了
比如说在 contraction 允许的情况下，我们会有：

\begin{prooftree}
  \AxiomC{$A \vdash A$}
  \UnaryInfC{$A \mathbin{\&} B \vdash A$}
  \AxiomC{$B \vdash B$}
  \UnaryInfC{$A \mathbin{\&} B \vdash B$}
  \BinaryInfC{$A \mathbin{\&} B, A \mathbin{\&} B \vdash A \otimes B $}
  \UnaryInfC{$A \mathbin{\&} B \vdash A \otimes B$}
\end{prooftree}

** Exponential: Of Course! 和 Why Not?

Girard 着实是命名鬼才，Of Course! 和 Why Not? 是什么鬼名字。总之我们有两个 Unary Connectives ：\(!\) 和 \(?\) 。

我们说标记了 \(!\) 的公式就是可以无限使用的：假设 \(A\) 是有 10 块钱，\(B\) 是有 pizza （价值 10 块钱），\(C\) 是有 burger （价值 10 块钱），那么 \(A \vdash B \otimes C\) 是不行的，但是 \(!A \vdash B \otimes C\) （也就是 \(\vdash !A \multimap B \otimes C\)）是可行的，在标记了 \(!\) 之后就相当于“从有着无限钱的账户里面取出 10 块钱了”，是可以复制的。有了这个之后就能表示 classical 的 implication 了 \(A \Rightarrow B \equiv (!A) \multimap B\) ，因为在 \(A \Rightarrow B\) 里的 \(A\) 是可以无限使用的，也就是能够使用 weakening 和 contraction 了。
\[\frac{\Gamma, A \vdash}{\Gamma, {!} A \vdash}, \quad \frac{\Gamma, !A , !A \vdash}{\Gamma, !A \vdash}\]
（注：这里用的是单侧的 rule）
而 \(?\) 是 \(!\) 的对偶：左侧的 \(!\) 到了右侧就是 \(?\) 了
\[(!A) ^{\bot} = {?} (A ^{\bot})\]
所以用右手侧的 rule 就有：
\[\frac{\vdash \Gamma}{\vdash \Gamma, ? A}(\mathrm{weakening}), \quad \frac{\vdash \Gamma, ?A, ?A}{\vdash \Gamma, ?A}(\mathrm{contraction})\]
* COMMENT text1 :noexport:

** prelude

In Geometry as well as in Physics, one has often to use the tools of differential and integral calculus on topological spaces which are locally like open subsets of the Euclidean space \(\mathbb{R}^n\), but do not admit coordinates valid everywhere. For example, the sphere \(\{(x, y, z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\), or more generally, the space \(\{(x_1, \ldots, x_n) \in \mathbb{R}^n : \sum x_i^2 = 1\}\) are clearly of geometric interest.

On the other hand, constrained motion has to do with dynamics on surfaces in \(\mathbb{R}^3\). In general relativity, one studies 'space-time' which combines the space on which motion takes place and the time parameter in one abstract space and allows reformulation of problems of Physics in terms of a 4-dimensional object. All these necessitate a framework in which one can work with the tools of analysis, like differentiation, integration, differential equations and the like, on fairly abstract objects.

This would enable one to study Differential Geometry in its appropriate setting on the one hand, and to state mathematically the equations of Physics in the required generality, on the other. The basic objects which accomplish this are called differential manifolds. These are geometric objects which are locally like domains in the Euclidean space, so that the classical machinery of calculus, available in \(\mathbb{R}^n\), can be transferred, first, to small open sets and then patched together. The main tool in the patching up is the notion of a sheaf. We will first define sheaves and discuss about basic notions related to them, before taking up differential manifolds.

** 1. Sheaves and Presheaves

At the outset, it is clear that functions of interest to us belong to a class such as continuous functions, infinitely differentiable (or \( C^\infty \)) functions, real analytic functions, holomorphic functions of complex variables, and so on. Some properties common to all these classes of functions are that

  (i) they are all continuous, and

  (ii) the condition for a continuous function to belong to the class is of a local nature.

By this we mean that for a continuous function to be differentiable, real analytic or holomorphic, it is necessary and sufficient for it to be so in the neighbourhood of every point in its domain of definition. We will start with an axiomatisation of the local nature of the classes of functions we seek to study.


Here's the text from the image:

** 1.1. Definition: presheaf

Let \( X \) be a topological space. An assignment to every open subset \( U \) of \( X \), of a set \( \mathcal{F}(U) \) and to every pair of open sets \( U, V \) with \( V \subseteq U \), of a map (to be called restriction map) \( \text{res}_{UV} : \mathcal{F}(U) \to \mathcal{F}(V) \) satisfying:
\[ \text{res}_{VW} \circ \text{res}_{UV} = \text{res}_{UW} \]
for every triple \( W \subseteq V \subseteq U \) of open sets, is called a presheaf of sets. If \( \mathcal{F}(U) \) are all abelian groups, rings, vector spaces, etc., and the restriction maps are homomorphisms of the respective structures, then we say that \( \mathcal{F} \) is a presheaf of abelian groups, rings, vector spaces, etc.

\[\begin{tikzcd}
  & U\ar[rr, dashed] & & \mathcal F(U)\ar[d]\ar[ddl]\\
  & V\ar[u]\ar[rr, dashed] & & \mathcal F(V)\ar[dl] \\
  W\ar[ur]\ar[uur]\ar[rr, dashed] & & \mathcal F(W)
\end{tikzcd}\]

(supplement) alternative description (because the author's English is kinda tricky): *Presheaf*:

A presheaf on a topological space \( X \) is a collection of sets (or groups, rings, etc.) \( \mathcal{F}(U) \) for each open set \( U \subseteq X \), along with restriction maps \( \text{res}_{UV}: \mathcal{F}(U) \to \mathcal{F}(V) \) for \( V \subseteq U \), satisfying:

   - \( \text{res}_{UU} = \text{id}_{\mathcal{F}(U)} \),
   - \( \text{res}_{VW} \circ \text{res}_{UV} = \text{res}_{UW} \) for \( W \subseteq V \subseteq U \).

** 1.2. Definition: sheaf

A presheaf is said to be a sheaf if it satisfies the following additional conditions. Let \( U = \bigcup_{i \in I} U_i \) be any open covering of an open set \( U \). Then:

*S1*: Two elements \( s, t \in \mathcal{F}(U) \) are equal if \( \text{res}_{UU_i} s = \text{res}_{UU_i} t \) for all \( i \in I \).

*S2*: If \( s_i \in \mathcal{F}(U_i) \) satisfy \( \text{res}_{U_{i}U_i \cap U_j} s_i = \text{res}_{U_{j}U_i \cap U_j} s_j \) for all \( i, j \in I \), then there exists an element \( s \in \mathcal{F}(U) \) with \( \text{res}_{UU_i} s = s_i \) for all \( i \). We will also assume that \( \mathcal{F}(\emptyset) \) consists of a single point.

If there is \(\{s_{i}\in \mathcal F(U_{i})\}\), satisfy \(\text{res}_{U_{i}U_{i}\cap U_{j}}s_{i} = \text{res}_{U_{j}U_{i}\cap U_{j}} s_{j}\) for all \(i,j \in I\), then there is an element in \(s \in \mathcal F (U)\) that \(\mathrm{res}_{UU_{i}}s = s_{i}\) for all \(i\).

(supplement from editor) The description for two conditions:

- *Locality*: If \( s \in \mathcal{F}(U) \) and \( s \) restricts to the zero element on each \( U_i \cap U \), then \( s = 0 \).
- *Gluing*: If \( s_i \in \mathcal{F}(U_i), s_{j} \in \mathcal{F}(U_{j}) \) satisfy \( \text{res}_{U_{i}U_i \cap U_j} s_i = \text{res}_{U_{j}U_i \cap U_j} s_j \) for all \( i, j \), then there exists a unique \( s \in \mathcal{F}(U) \) with \( \text{res}_{UU_i} s = s_i \) for all \( i \).

Intuitions:

- *Locality* ensures that sections are determined by their local behavior.
- *Gluing* ensures that compatible local sections can be "glued" together to
  form a global section.

Gluing allows us to construct global objects from local data. For example, in a topological space, we can define functions or sections on open sets and then "glue" them together to define a function on the entire space.

** 1.3. Examples

*** 1.3.1. sheaf of Differential Functions

As indicated above, the concepts of differentiability, real analyticity, ... are all local in nature, so that it is no surprise that if \(X\) is an open subspace of \(\mathbb{R}^n\), then the assignment to every open subset \(U\) of \(X\), of the set \(\mathcal{A}(U)\) of differentiable functions with the natural restriction of functions as restriction maps, gives rise to a sheaf.

This sheaf will be called the sheaf of differentiable functions on \(X\). Obviously this sheaf is not merely a sheaf of sets, but of \(\R\) algebras.

*** 1.3.2. bounded functions

The assignment of the set of bounded functions to every open set \( U \) of \( X \), and the natural restriction maps define a presheaf on \( X \).
# of means target; to means source
# U is mapped to bounded functions

However, if we are given as in \( S_2 \) any compatible set of bounded functions \( s_i \), then while such data does define a unique function \( s \) on \( U \), there is no guarantee that it will be bounded. Thus this presheaf satisfies \( S_1 \) but not \( S_2 \) and so is not a sheaf. (Can one modify this in order to obtain a sheaf?)

** 1.4. Sheaf associated to a presheaf. germ and stalk

In the theory of holomorphic functions, one talks of a *germ* of a function at a point, when one wishes to study its properties in an (unspecified) neighbourhood of a point. This means the following.

  Consider pairs \((U, f)\) consisting of open sets \(U\) containing the given point \(x\) and holomorphic functions \(f\) defined on \(U\). Introduce an equivalence relation in this set by declaring two such pairs \((U, f)\), \((V, g)\) to be *equivalent* if \(f\) and \(g\) coincide in some neighbourhood of \(x\) which is contained in \(U \cap V\). An equivalence class is called a *germ*. This procedure can be imitated in the case of a presheaf and this leads to the concept of a *stalk* of a presheaf at a point.

** 1.5. Definition: stalk and germ

Let \(\cal F\) be a presheaf on a topo space \(X\). Then the stalk \(\mathcal F_{x}\) of \(\cal F\) at point \(x \in X\) is the quotient set of the set consisting of all \((U,s)\) where \(U\) is an open neightbourhood of \(x\) and \(s\) is an element of \(\mathcal F(U)\) under the *equaivalence relation*:

  \((U,s)\) is equivalent to \((V,t)\) if and only if there exists an open neighbourhood \(W\) of \(x\) contained in \(U \cap V\) (i.e. \(W \subset U \cap V\)) such that the restrictions of \(s\) and t to \(W\) are the same.

  If \(s \in \mathcal F(X)\) then, for any \(x \in X\), the pair \((X, s)\) has an image in the talk \(\mathcal F_{x}\), namely the quivalence class containing it. It is called the /germ of \(s\)/ at \(x\). We will denote it by \(s _{x}\).

(supplement from editor)
Let \( \mathcal{F} \) be a sheaf on a topological space \( X \), and let \( x \in X \) be a point. The *stalk* \( \mathcal{F}_x \) of \( \mathcal{F} \) at \( x \) is defined as:
\[
\mathcal{F}_x = \lim_{\longrightarrow} \mathcal{F}(U)
\]
where the direct limit is taken over all open neighborhoods \( U \) of \( x \), ordered by inclusion \( U_1 \subseteq U_2 \).

- In the sheaf of smooth functions on a manifold, the stalk at a point \( x \) consists of all germs of smooth functions defined in some neighborhood of \( x \).
- In the sheaf of holomorphic functions on a complex manifold, the stalk at a point \( x \) consists of all germs of holomorphic functions near \( x \).

** interlude: The topology on the set of all germs

Let \(E = E(\mathcal F)\) be the germs of all elements at all points of \(X\), that is to say, the disjoint union of all the stalks \(\mathcal F_{x}\) \(x \in X\). What we intend to do now is to provide the set \(E\) with a topology such taht

  - (a) the map \(\pi \colon E \to X\) which maps all of \(\mathcal F_{x}\) to \(x\), is continuous;
  - (b) if \(s \in \mathcal F(U)\) then the section \(\tilde s\) of \(E\) over \(U\) (i.e. \(\tilde s \colon U \to E\)), defined by setting \(\tilde s(x) = s_{x}\), is continuous.

  We achieve this by associating to each pair \((U,s)\) where \(U\) is an open subset of \(X\) and \(s \in \mathcal F(U)\), the set \(\tilde s (U)\), and defining a topology on \(E\) whose open sets are generated by sets of the form \(\tilde s (U)\). In order to check that with this topology, the maps \(\tilde s\) are continuous, we have only to show that \(\tilde s^{-1}(\tilde t(V))\) is open for every open subset \(V\) of \(X\) and \(t \in \mathcal F(V)\). This is equivalent to the following.

** 1.6. Lemma.

** 1.7. Definition: etale space

todo
