#    -*- mode: org -*-


Archived entries from file /home/one/org/note.org


* Categorical Logic and Type Theory
:PROPERTIES:
:CUSTOM_ID: categoricallogic
:ARCHIVE_TIME: 2024-12-04 Wed 15:56
:ARCHIVE_FILE: ~/org/note.org
:ARCHIVE_OLPATH: some books
:ARCHIVE_CATEGORY: note
:ARCHIVE_ITAGS: noexport
:END:

** -1 preliminaries

*** Recursion theory

The categories of PERs and of \(\omega\)-sets (and also the effective topos) will occur in many examples. They involve some basic recusion theory. We assume some coding \(( \varphi _{n})_{n \in \N}\) of the partial recursive functions, and use it to describe what is called Kleene application on natural numbers:

\[n \cdot m =
\begin{cases}
  \varphi _{n}(m ),\quad & \mathrm{if}\ \varphi_{n}(m) \downarrow \\
  \uparrow,\quad         & \mathrm{otherwise}
\end{cases}\]
here \(\varphi_{n}(m)\downarrow\) means \(\varphi_{n}(m)\) is defined.

For a partial recusive function \(f\colon \N ^{n}\times \N \to \N\) we let \(\vec x \mapsto \Lambda y. f(\vec x , y)\) be the partial recursive function \(s_{1}^{n}( e, -)\colon \N ^{n} \to \N\) that is obtained from the "\(s\)-\(m\)-\(n\)-theorem" by writing

\[f( \vec x , y ) = \varphi _{e} (\vec x , y) = \varphi _{s ^{n}_{1}} (e, \vec x )(y).\]

Then \((\Lambda y . f(\vec x , y)) \cdot z \sim f(\vec x, z)\), where \(\sim\) is Kleen equality; it expresses that the left hand side is defined if and only if the right hand side is defined, and in that case both sides are equal. We furthere use a recursive bijection \(\langle - , - \rangle\colon \N \times \N \to \N\) with recursive projection functions \(\mathbf{p}, \mathbf{p'}\colon \N \rightrightarrows \N\). See /e.g./ [66. 294, 236] for more information.

*** Category theory

*Terminal object*

We generally use \(1\) for a terminal object (also called final object or *empty product*) in a category.

*Cartesian*

Binary Cartesian productes are written as \(X \times Y\) with porjections \(\pi \colon X \times Y \to X\), \(\pi '\colon X \times Y \to Y\) and tuples \(\langle f, g \rangle\colon X \to X\times Y\) for \(f\colon X \to X\) and \(g\colon Z \to Y\).

*Diagonals*

As sepcial case of tupleing, we often write \(\delta\) or \(\delta (X)\) for the diagonal \(\langle \mathrm{id} , \mathrm{id} \rangle \colon X \to X \times X\) on \(X\), and \(\delta\) or \(\delta (I, X)\) for the "parametrised" diagonal \(\langle \mathrm{id}, \pi' \rangle \colon I \times X \to (I \times X) \times X\), which duplicates \(X\), with parameter \(I\).

*Evaluation in CCC*

Associated with the abovementioned exponent object \(Y ^{X}\) in a CCC there are evaluation and abstraction maps, which will be written as \(\mathrm{ev}\colon Y ^{X} \times X \to Y\) and \(\Lambda (f)\colon Z \to Y ^{X}\), for \(f \colon Z \times X \to Y\).

(ps. In Girard's book, \(\mathrm{ev}\) is written as \(\epsilon\), and \(\Lambda(f)\) is written as \(\lambda (f)\). As you can see, the reason why we use \(\lambda\) as this unique homomorphism is that this \(\lambda\) is associated with the \(\lambda\) in \(\lambda\)-calculus.)

*Initial object*

An initial object (or emtpy coproduct) is usually written as \(0\). For binary coproduct we write \(X + Y\) with coprojections \(\kappa \colon X \to X + Y\), \(\kappa ' \colon Y \to X + Y\) and cotuples \([f, g] \colon X + Y \to Z\), where \(f\colon X \to Z\) and \(g \colon Y \to Z\). The codiagonal \(\nabla = [\mathrm{id}, \mathrm{id}] \colon X + X \to X\) is an example of a cotuple.

*Adjoint*

For functors \(F\colon \mathbb{A} \to \mathbb{B}\) and \(G \colon \mathbb{B} \to \mathbb{A}\) in an adjunction \(F \dashv G\) the homset isomorphism \(\B (F X , Y) \simeq \mathbb{A}(X, GY)\) is often written as a bijective correspondence between morphisms \(f\colon F X \to Y\) and \(g \colon X \to G Y\)via double lines:

\[\frac{ F X \longrightarrow _{f} Y}{ X \longrightarrow _{g} GY},\quad \textit{e.g.}\text{ for exponents:}\quad \frac{Z \times X \longrightarrow Y}{{Z \longrightarrow Y ^{X} }= X \Rightarrow Y\hidewidth}\]

In such a situation, transpostion is sometimes written as \((f: F X \to Y)\mapsto (f ^{\lor} \colon X \to GY)\) and \((g \colon X \to GY) \mapsto (g ^{\lor} \colon F X \to Y)\), or more ambiguoously, as \(f \mapsto \bar f\), \(g \mapsto \bar g\).

\(\eta\) *and* \(\varepsilon\)

We reserve the symbols \(\eta\) for the unit natural franformation \(\mathrm{id} \Rightarrow GF\), and \(\varepsilon\) for the counit natural tranformation \(FG \Rightarrow \mathrm{id}\) of an adjunction \((F \dashv G)\). We recall that these natural tranformations have componets \(\eta _{X} = (\mathrm{id}_{F X}) ^{\lor}\) and \(\varepsilon _{Y} = (\mathrm{id} _{ G Y})^{\land}\). In case both \(\eta\) and \(\varepsilon\) are (natural) isomorphisms, the categories \(\mathbb{A}\) and \(\mathbb B\) are called equivalent. This is written as \(\mathbb A \simeq \B\).

For the rest, we generally follow usual categoriacal notation, \textit{e.g.} as in the standard reference [187] (ps. Mac Lane). Another (more recent) reference text is [36] (ps. Handbook of Categorical Algebra). And [186, 19, 61] may be used as introductions. (ps. [186]: S. Mac Lane and R. Pare. Coherence for bicategories and indexed categories. [19]: M. Barr and Ch. Wells. Category Theory for Computing Science. [61]: R.L. Crole. Categories for Types.)

*** Type theory

Mostly, standard type theoretical notation will be used. For example, exponent types are written as \(\sigma \to \tau\) and (dependent) product types as \(\Pi x \colon \sigma . \tau\).

*Lambda*

The associated introduction and elimination operations are lambda-abstraction \(\lambda x\colon \sigma. M\) (ps. i.e. variable \(x\) is typed) and application \(M  \cdot N\), or simply \(MN\). (Sometimes we also use "meta-lambda-abstraction" \(\lambda\kern-4.5pt \raisebox{1.3pt}{$ \lambda $} x . f(x)\) for the actual function \(x \mapsto f(x)\)), not in some formal calculus.

Bonus: the meta-lambda is written as
#+begin_example
\(\lambda\kern-4.5pt \raisebox{1.3pt}{$ \lambda $} x . f(x)\)
#+end_example

*Limit and colimit*

We standardly describe besides "limit types" also "colimit types" like coproduct (disjoint union) \(\sigma + \tau\), dependent sum \(\Sigma x \colon \sigma . \tau\), equality \(\mathrm{Eq}_{\sigma}(x , x ')\) and quotient \(\sigma / R\). There is no established notation for the introduction and elimination operations associated with these type formers. The notation that we shall use is given in Figure 0.1. The precise rules will be given later. For these "colimit" type formers there are typical "commutation conversions" (involving substitution of elimination terms) and "Frobenius properties" (describing commutation with products).

*Substitution*

We write (e,g. in the above table) \(M [ N / x]\) for the result of substituting \(N\) for all free occurrences of \(x\) in \(M\). This applies to terms, types or kinds \(M, N\).

*Equality and "defined as"*

In a type theoretic context, an equation \(M = N\) between terms usually describes convertibility. We shall use \(\equiv\) to denote syntactic equality (following [13]).

*The rest*

Familiarity with the propostions-as-types correspondence (between derivability in logic and inhabitation in type theory) will be convenient, but not necessary. For basic information on type theory we refer to [14, 98]. Also the standard textbook [13] on the untyped lambda calculus is relevant, since many of the typed nottions stem from the untyped setting.

** 0 Prospectus
:PROPERTIES:
:CUSTOM_ID: clog00
:END:

This introductory chapter is divided into two parts. It first discusses some generalities concerning logic, type theory and category theory, and describes some themes that will be developed in this book. It then continues with a description of the (standard) logic and type theory of ordinary sets, from the perspective of fibred category theory--typical of this book. This description focuses on the fundamental adjunctions that govern the various logical and type theoretic operations.

** 0.1 Logic, type theory, and fibred category theory
:PROPERTIES:
:CUSTOM_ID: clog001
:END:

*** part one: logic over a type theory

A logic is always a logic over a type theory. This statement sums up our approach to logic and type theory, and forms an appropriate starting point. It describes a type theory as a “theory of sorts”, providing a domain of reason-ing for a logic. Roughly, types are used to classify values, so that one can distinguish between zero as a natural number \(0\colon \mathbb{N}\) and zero as a real number \(0\colon\mathbb{R}\), and between addition \(+ \colon \mathbb{N}\times\mathbb{N}\to\mathbb{N}\) on natural numbers and addition \( + \colon \mathbb{R}\times\mathbb{R}\to \mathbb{R}\) on real numbers. In these examples we use atomic types \(\mathbb{N}\) and \(\mathbb{R}\) and composite types \(\mathbb{N}\times\mathbb{N}\to\mathbb{N}\) and \(\R \times \R \to \R\) obtained with the type constructors \(\times\) for Cartesian product, and \(\to\) for exponent (or function space). The relation \(:\) as in \(0\colon \N\), is the inhabitation relation of type theory. It expresses that \(0\) is of type \(\mathbb{N}\), i.e. that \(0\) inhabits \(\mathbb{N}\). It is like membership \(\in\) in set theory, except that \(\in\) is untyped, since everything is a set. But a string is something which does not inhabit the type of natural numbers. Hence we shall have to deal with rules regulating inhabitation, like

\[
\prftree[r]{}
        {}
        {0 \colon \N}\quad
\prftree[r]{}
        {n \colon \N}
        {\texttt{succ}(n)\colon \N}
\]

The first rule is unconditional: it has no premises and simply expresses that the term O inhabits the type \(\N\). The second rule tells that if we know that \(n\) inhabits \(\N\), then we may conclude that \(\texttt{succ}(n)\) also inhabits \(\N\), where \(\texttt{succ}(-)\) may be read as successor operation. In this way one can generate terms, like \(\succ(\succ(0)): \N\) inhabiting the type \(\N\).

In predicate logic one reasons about such terms in a type theory, like in

\[\forall x \colon \N. \exists y\colon \N . y > \succ(x)\]

This gives an example of a proposition. The fact that this expression is a proposition may also be seen as an inhabitation statement, so we can write

\[(\forall x \colon \N . \exists y \colon \N . y > \succ(x))\colon \mathsf{Prop}\]


using a type Prop of propositions. In this particular proposition there are no free variables, but in predicate logic an arbitrary proposition \(y\colon \Prop\) may contain free variables. These variables range over types, like in:

\[x > 5 \colon \mathsf{Prop}, \text{ where }x \colon \N \quad \text{or} \quad
x > 5 \colon \mathsf{Prop}, \text{ where } x \colon \R\]

We usually write these free variables in a “context", which is a sequence of variable declarations. In the examples the sequence is a singleton, so we write

\[x \colon \N \vdash x > 5 \colon \mathsf{Prop}\quad \text{and} \quad
x \colon \R \vdash x > 5 \colon \mathsf{Prop}\]

The turnstile symbol \(\vdash\) separates the context from the conclusion: we read the sequent \(x\colon \N \vdash x > 5 \colon \mathsf{Prop}\) as: in the context where the variable \(x\) is of type \(\N\), the expression \(x > 5\) is a proposition. Well-typedness is of importance, since if \(x\) is a string, then the expression \(x > 5\) does not make sense (unless one has a different operation \(>\) on strings, and one reads '\(5\)' as a string).

This explains what we mean with: a logic is always a logic over a type theory. Underlying a logic there is always a calculus of typed terms that one reasons about. But one may ask: what about single-sorted logic (i.e. single-typed, or untyped, logic) in which variables are thought of as ranging over a single domain, so that types do not really play a role? Then one still has a type theory, albeit a very primitive one with only one type (namely the type of the domain), and no type constructors. In such situations one often omits the (sole) type, since it has no role. But formally, it is there. And what about propositional logic? It is included as a border case: it can be seen as a degenerate predicate logic in which all predicates are closed (i.e. do not contain term variables), so one can see propositional logic as a logic over the empty type theory.

*** part two: type theory

We distinguish three basic kinds of type theory:

- simple type theory (STT);
- dependent type theory (DTT);
- polymorphic type theory (PTT).

In simple type theory there are types built up from atomic types (like \(\N\), above) using type constructors like exponent \(\to\), Cartesian product \(\times\) or coproduct (disjoint union) \(+\). Term variables \(x\colon \sigma\) are used to build up terms, using atomic terms and introduction and elimination operations associated with the type constructors (like tuples and projections for products \(\times\)). Types in simple type theory may be seen as sets, and (closed) terms inhabiting types as elements of these sets. In /dependent/ type theory, one allows a term variable \(x\colon \sigma\) to occur in another type \(\tau (x)\colon \mathsf{Type}\). This increases the expressive power, for example because one can use in DTT the type \(\mathsf{Matrix}(n, m)\) of \(n \times m\) matrices (say over some fixed field), for \(n\colon \N\) and \(m\colon \N\) terms of type \(\N\). If one thinks of types as sets, this type dependency is like having for each element \(i \in I\) of a set \(I\), another set \(X(i)\). One usually writes \(X_{i} = X(i)\) and sees \((X_{i})_{i \in I}\) as an \(I\)-indexed family of sets. Thus, in dependent type theory one allows type-indexed-types, in analogy with set-indexed-sets. Finally, in /polymorphic/ type theory, one may use additional type variables \(\alpha\) to build up types. So type variables \(\alpha\) may occur inside a type \(\sigma(\alpha)\), like in the type \(\mathsf{list}(\alpha)\) of lists of type \(\alpha\). This means that one has types, indexed by (or parametrised by) the universe \(\mathsf{Type}\) of all types. In a set theoretic picture this involves a set \(X_{A}=X(A)\) for each set \(A\). One gets indexed collections \((X_{A})_{A \in \mathbf{Sets}}\) of sets \(X_{A}\).

These three type theories are thus distinguished by different forms of in-dexing of types: no indexing in simple type theory, indexing by term variables \(x\colon \sigma\) in dependent type theory, and indexing by type variables \(\alpha \colon \mathsf{Type}\) in polymorphic type theory. One can also combine dependent and polymorphic type theory, into more complicated type theories, for example, into what we call polymorphic dependent type theory (PDTT) or full higher order dependent type theory (FhoDTT).

What we have sketched in the beginning of this section is predicate logic over simple type theory. We shall call this simple predicate logic (SPL). An obvious extension is to consider predicate logic over dependent type theory, so that one can reason about terms in a dependent type theory. Another extension is logic over polymorphic type theory. This leads to dependent predicate logic(DPL) and to polymorphic predicate logic (PPL). If one sees a typed calculus as a (rudimentary) programming language, then these logics may be used as program logics to reason about programs written in simple, dependent, or polymorphic type theory. This describes logic as a “module” that one can plug on to a type theory.

*** part three: fibred category

This book focuses on such structural aspects of logic and type theory. The Language and techniques of category theory will be essential. For example, we talked about a logic /over/ a type theory. Categorically this will correspond to one ("total") category, capturing the logic, being /fibred/ over another ("base") category, capturing the type theory. Indeed, we shall make special use of tools from fibred category theory. This is a special part of category theory, stemming from the work of Grothendieck in algebraic geometry, in which (continuous) indexing of categories is studied. As we already mentioned, the various forms of type theoretic indexing distinguish varieties of type theory. And also, putting a logic on top of some type theory (in order to reason about it) will be described by putting a fibration on top of the categorical structure corresponding to the type theory. In this way we can put together complicated structures in a modular way.

Fibred category theory is ordinary category theory with respect to a base category. Also, one can say, it is ordinary category theory /over/ a base category. Such a base category is like a universe. For example, several concepts in category theory are defined in terms of sets. One says that a category \(\C\) has arbitrary products if for each /set/ I and each \(I\)-indexed collection \((X_{i})_{i\in I}\) of objects \(X_{i}\in \C\) there is a product object \(\prod_{i\in I}X_{i}\in \C\) together with projection morphisms \(\pi_{j} \colon (\prod _{i\in I}X_{i}) \to X_{j}\), which are suitably universal. In category theory one is not very happy with this privileged position of sets and so the question arises: is there a way to make sense of such products with respect to an object I of a ‘universe’ or ‘base category’ \(\mathbb{B}\), more general than the category \(\mathbf{Sets}\) of sets and functions? This kind of generality is needed to interpret logical products \(\forall x \colon \sigma . \varphi\) or type theoretic products \(\Pi x\colon \sigma . \tau\) when the domain of quantification \(\sigma\) is not interpreted as a set (but as some ordered set, or algebra, for example).

Another example is local smallness. A category \(\C\) is locally small if for each pair of objects \(X,  Y \in \C\) the morphisms \(X\to Y\) in \(\C\) form a /set/ (as opposed to a proper class). That is, if one has homsets \(\C(X,Y) \in \mathbf{Sets}\) as objects in the category of sets. Again the question arises whether there is a way of saying that \(\C\) is locally small with respect to an arbitrary universe or base category \(\mathbb{B}\) and not just with respect to \(\mathbf{Sets}\).

Fibred category theory provides answers to such questions. It tells what it means for a category \(\mathbb{E}\) to be “fibred over' a base category \(\mathbb{B}\). In that case we write \(
\begin{gathered}
  \scriptstyle \mathbb{E}\\[-8pt]
  \scriptstyle\downarrow\\[-7pt]
  \scriptstyle \mathbb{B}
\end{gathered}\), where the arrow \(\mathbb{E}\to \mathbb{B}\) is a functor which has a certain property that makes it into a fibration. And in such a situation one can answer the above questions: one can define quantification with respect to objects \(I \in \mathbb{E}\) and say when one has appropriate hom-objects \(\underline{\Hom}(X, Y)\in \mathbb B\) for \(X, Y \in \mathbb{E}\). The ways of doing this will be explained in this book. And for a category \(\C\) there is always a ‘family fibration’ \(
\begin{gathered}
  \scriptstyle \mathrm{Fam}(\C)\\[-7pt]
  \scriptstyle \downarrow \\[-7pt]
  \scriptstyle \mathbf{Sets}
\end{gathered}\) of set-indexed families in \(\C\).
The fibred notions of quantification and local smallness, specialised to this family fibration, are the ordinary notions described above. Thus, in the family fibration we have our standard universe (or base category) of sets.

*** part four: categorical phenomena

There are many categorical notions arising naturally in logic and type theory (see the list below). And many arguments in category theory can be formulated conveniently using logic and type theory as “internal” language (sometimes called the “Mitchell-Benabou” language, in the context of topos theory). These fields however, have different origins: category theory arose in the work of Eilenberg and Mac Lane in the 1940s within mathematics, and was in the beginning chiefly used in algebra and topology. Later it found applications in almost all areas of mathematics (and computer science as well, more recently). Type theory is also from this century, but came up earlier in foundational work by Russell in logic (to avoid paradoxes). Recently, type theory has become important in various (notably functional) programming languages, and in computer mathematics: many type theories have been used during the last two decades as a basis for so-called(?) proof-assistants. These are special computer programs which assist in the verification of mathematical statements, expressed in the language of some (typed) logic. The use of types in these areas imposes certain restrictions on what can be expressed, but facilitates the detection of various errors. We think it is in a sense remarkable that two such fundamental fields (of category theory and of type theory)---with their apparent differences and different origins---are so closely related. This close relationship may be beneficial in the use and further development of both these fields.


We shall be especially interested in categorical phenomena arising within logic and type theory. Among these we mention the following.

- (i) Every context of variable declarations (in type theory) or of premises(in logic) is an index. It is an index for a ‘fibre’ category which captures the logic or type theory that takes place within that context--with the declared variables, or under the assumptions. The importance of this categorical role of contexts is our motivation for paying more than usual attention to contexts in our formulations of type theory and logic.

- (ii) Appropriately typed sequences of terms give rise to morphisms be tween contexts. This is the canonical way to produce a category from types and terms. These context morphisms induce substitution functors between fibre categories. The structural operations of weakening (adding a dummy assumption) and contraction (replacing two assumptions of the same kind by a single one) appear as special cases of these substitution functors: weakening is substitution along a projection \(\pi\), and contraction is substitution along a diagonal \(\delta\). These \(\pi\) and \(\delta\) may be Cartesian projections and diagonals in simple and polymorphic type theories, or ‘dependent’ projections and diagonals in dependent type theory. (ps. a cartesian diagonal \(\delta\) is a functor \(\C \to \C \times \C\))

- (iii) The basic operations of logic and type theory can be described as adjoints in category theory. Such operations standardly come with an intro-duction and an elimination operation, which are each other's inverses (via the so-called (\(\beta\))- and (\(\eta\))-conversions). Adjoint correspondences capture such situations. This may be familiar for the (simple) type theoretic constructors \(1\), \(\times\), \(0\), \(+\) and \(\to\) (and for their propositional counterparts \(\top\), \(\land\), \(\bot\), \(\lor\) and \(\supset\)), since these are the operations of bicartesian closed categories (which can be described via standard adjunctions). But also existential \(\exists x \colon\sigma .(-)\) and universal \(\forall x \colon \sigma.(-)\) quantification in predicate logic over a type \(\sigma\), dependent sum \(\Sigma x \colon \sigma . (-)\) and product \(\Pi \alpha\colon \sigma .(-)\) in dependent type theory over a type \(\sigma\), and polymorphic sum \(\Sigma \alpha \colon \mathsf{Type}.(-)\) and product \(\Pi \alpha \colon \mathsf{Type}.(-)\)in polymorphic type theory over the universe Type of types, are characterised as left and right adjoints, namely to the weakening functor which adds an extra dummy assumption \(x\colon\sigma\), or \(\alpha \colon \mathsf{Type}\). Moreover, equality \(=_{\alpha}\) on a type \(\sigma\) is characterised as left adjoint to the contraction functor which replaces two variables \(x, y \colon \sigma\) by a single one (by substituting \(x\) for \(y\)). By ‘being characterised’ we mean that the standard logical and type-theoretical rules for these operations are (equivalent to) the rules that come out by describing these operations as appropriate adjoints.

  The most important adjunctions are:

  \[\begin{aligned}\text{existential }  \exists , \text{ sum } \Sigma & \dashv \text{ weakening}\\ \text{weakening} & \dashv \text{ contraction}\\ \text{equality} & \dashv \text{ comprehension}\\ \text{(but also: equality} & \dashv \text{ comprehension, via a different functor)}\\ \text{quotients} & \dashv \text{ equality} \end{aligned}\]

  The first four of these adjoints were recognised by Lawvere (and the last two are identified in this book). Lawvere first described the quantifiers \(\exists\), \(\forall\) as left and right adjoints to arbitrary substitution functors. The above picture with separate adjoints to weakening and to contraction functors is a refinement, since, as we mentioned in (ii), weakening and contraction functors are special cases of substitution functors. (These operations of weakening and contraction can be suitably organised as a certain comonad; we shall define quantification and equality abstractly with respect to such comonads.)

- (iv) As we mentioned above, the characteristic aspect of dependent type theory is that types may depend on types, in the sense that term variables inhabiting types may occur in other types. And the characteristic aspect of polymorphic type theory is that type variables may occur in types. Later we shall express this as: types may depend on kinds. These dependencies amount to certain forms of indexing. They are described categorically by fibred (or indexed) categories. Thus, if one knows the dependencies in a type theory, then one knows its underlying categorical structure. The additional type theoretic structure may be described via certain adjunctions, as in the previous point.

- (v) Models of logics and type theories are (structure preserving) functors. From a specific system in logic or type theory one can syntactically build a so-called ‘classifying’ (fibred) category, using a term model---or generalised Lindenbaum-Tarski---construction. A model of this system is then a (fibred) functor with this classifying (fibred) category as domain, preserving appropriate structure. We shall make systematic use of this /functorial semantics/. It was introduced by Lawvere for single-typed simple type theories. And it ex-tends to other logics and type theories, and thus gives a systematic description of models of (often complicated) logics and type theories.

- (vi) If \(\sigma = \sigma (\alpha)\) is a type (in polymorphic type theory) in which a free type variable \(\alpha\) occurs,then,under reasonable assumptions about type formation, the operation \(\tau \mapsto \sigma [\tau / \alpha]\) of substituting a type \(\tau\) for \(\alpha\), is functorial. This functoriality is instrumental in describing the rules of (co-)inductively defined data types in terms of (co-)algebras of this functor. And the reasoning principles (or logic) associated with such data types can also be captured in terms of (co-)algebras (but for a different functor, obtained by lifting the original functor to the logical world of predicates and relations).

- (vii) A logical framework is a type theory \(\mathcal T\) which is expressive enough so that one can formulate other systems \(S\) of logic or of type theory inside \(\mathcal T\). Categorically one may then describe (the term model of) \(S\) as an internal category in (the term model of) \(\cal T\). We briefy discuss dependent type theory as a logical framework in Section 10.2, but we refer to [87] for this connection with internal categories.

*** part five: categorical structures

This is not a book properly on logic or on type theory. Many logical and type theoretical calculi are described and some illustrations of their use are given, but there is nothing about specific proof-theoretic properties like cut-elimination, Church-Rosser or strong normalisation. Therefore, see [14]. The emphasis here lies on categorical semantics. This is understood as follows. Category theory provides means to say what a model of, say predicate logic, should look like. It gives a specification, or a hollow structure, which captures the essentials. A proper model is something else, namely an instance of such a structure. We shall describe both these hollow structures, and some instances of these. (But we do not investigate the local structure or theories of the example models, like for example in [197] or in [13, Chapter 19].)

So what, then, is the advantage of knowing what the categorical structures are, corresponding to certain logics and type theories?

Firstly, it enables us to easily and quickly recognise that certain mathematical structures are models of some logical or type theoretical calculus, without having to write out an interpretation in detail. The latter can be given for the ‘hollow categorical structure’, and need not be repeated for the particular instances. One only has to check that the particular structure is an instance of the general categorical structure. For example, knowing that a particular category (of domains, say) is Cartesian closed yields the information that we can interpret simple type theory.

Secondly, once this is realised, we can turn things around, and start using our calculus (suitably incorporating the constants in a signature) to reason directly and conveniently about a (concrete or abstract categorical) model. This is the logician's view of the mathematician's use of language: when reasoning about a particular mathematical structure (say a group \(G\)), one formally adds the elements \(a \in G\) as constants \(\underline{a}\) to the language, and one uses the resulting “internal” language to reason directly about \(G\). The same approach applies to more complex mathematical structures, like a fibred category of domains: one then needs a suitable type theoretic language to reason about such a complex (indexed) structure.

The third advantage is what a clear (categorical) semantics provides a certain syntactic hygiene, and deepens the understanding of the various logical and type theoretical systems. For example, the principle that a (possibly new) operation in logic or type theory should correspond to an adjoint gives certain canonical introduction, elimination and conversion rules for the constructor. Fourthly, models can be used to obtain new results about one's logical or type theoretical system. Consistency, conservativity and independence results are often obtained in this manner. Finally, and maybe most importantly, models provide meaning to one's logical or type theoretical language, resulting in a better understanding of the syntax.

There are so many systems of logic and type theory because there are certain"production rules" which generate new systems from given ones.

- (i) There are three basic type theories: simple type theory (STT), depen-dent type theory (DTT) and polymorphic type theory (PTT).

- (ii) Given a certain type theory, one can construct a logic over this type theory with predicates \(\varphi (\vec x) \colon \Prop\) containing free variables \(\vec x\) inhabiting types. This allows us to reason about (terms in) the given type theory.

- (ili) Given a logic (over some type theory), one can construct a new type theory (extending the given one) by a propositions-as-types upgrade: one considers the propositions \(\varphi\) in the logic as types in the new type theory, and derivations in the logic as terms in the new type theory.

This modularity is reflected categorically in the following three points.

- (i) There are three basic categorical structures: for STT (Cartesian closed categories), for DTT (what we call closed comprehension categories) and for PTT (certain fibred Cartesian closed categories).

- (ii) Putting a logic on a type theory corresponds to putting a preorder fbration on top of the structure describing the type theory. For logic one uses preorder structures, since in logic one is interested in provability and not in explicit proofs (or proof-terms, as in type theory), which are described as non-trivial morphisms.

- (ii) Under a propositions-as-types upgrade one replaces a preorder fibra-tion by an ordinary fibration (with proper fibre categories), thus making room for proof-terms as proper morphisms.

(Both second points are not as unproblematic as they may seem, because one may have complicated type theories, say with two syntactic universes of types and of kinds, in which there are many ways of putting a logic on top of such a type theory: one may wish to reason about types, or about kinds, or about both in the same logic. Categorically, there are similarly different ways in which a preorder fibration can be imposed.)

By the very nature of its contents, this book is rather descriptive. It contains few theorems with deep mathematical content. The influence of computer science may be felt here, in which much emphasis is put on the description of various languages and formalisms.

*** part six: what this book is not

Also, it is important to stress that this is not a book properly on fibred category theory. And it is not intended as such. It does contain the basic concepts and results from fibred category theory, but only as far as they are directly useful in logic or type theory (and not in topology, for example). Some of these basic results have not been published previously, but have been folklore for some time already. They have been discovered and rediscovered by various people, and the precise fow of ideas is hard to track in detail. What we present in this book is not a detailed historical account, and we therefore apologise in advance for any misrepresentation of history.

We sketch what we see as the main lines. In the development of fibred category and categorical logic one can distinguish an initial French period starting in the 1960s with Grothendieck's definition of a fibration (i.e. a fibred category), published in [107]. It was introduced in order to study descent. The ensuing theory was further developed by Grothendieck and (among others) Giraud [100] and Benabou. The latter's work is more logical and foundational in spirit than Grothendieck's (involving for example suitable fibred notions of local smallness and definability), and is thus closest to the current work. Many of the basic notions and results stem from this period.

In the late 1960s Lawvere first applied indexed categories in the study of logic. Especially, he described quantification and equality in terms of adjoints to substitution functors,and showed that also comprehension involves an adjunction. This may be seen as the start of categorical logic (explicitly, in his influential “Perugia Lecture Notes” and also in [192, 193]). At about the same time, the notion of elementary topos was formulated, by Lawvere and Tierney. This resulted in renewed attention for indexed (and internal) categories, to study phenomena over (and inside) toposes. See for example [173, 169] and the references there.

Then, in the 1980s there is the start of a type theoretic boom, in which indexed and fibred categories are used in the semantics of polymorphic and dependent type theories, see the basic papers 306, 307, 148] and the series of PhD theses [45,330,75,185,318,252,260,7,154,89,217,86,60,289, 125, 4, 198, 133]. This book collects much material from this third phase. Explicitly, the connection between simple type theory and Cartesian closed categories was first established by Lawvere and Lambek. Later, dependent type theory was related to locally Cartesian closed categories by Seely, and to the more general “display map categories” by Taylor. The relation between polymorphic type theory and certain fibred (or indexed, or internal) Cartesian closed categories is due to Seely, Lamarche and Moggi. Finally, more compli-cated systems combining polymorphic and dependent systems (like the calcu-lus of constructions) were described categorically by Hyland, Pitts, Streicher, Ehrhard, Curien, Pavlovic, Jacobs and Dybjer. This led to the (surprising) discovery of complete internal categories by Moggi and Hyland (and to the subsequent development of ‘synthetic’ domain theory in abstract universes).

Interestingly, fibred categories are becoming more and more important in various other areas of (theoretical) computer science, precisely because the aspects of indexing and substitution (also called renaming, or relabelling) are so fundamental.Among these areas we mention(without pretension to be in any sense complete): database theory [295, 151, 9], rewriting [12], automata theory [175, 10], abstract environments [279], data flow networks [310], constraint programming [219], concurrency theory [345, 131], program analysis [230, 25], abstract domain theory [146] and specification [152, 327, 48, 159].

Many topics in the field of categorical logic and type theory are not discussed in this book. Sometimes because the available material is too recent (and un-settled), sometimes because the topic deviates too much from the main line, but mostly simply because of lack of space. Among these topics we mention(with a few references): inductively and co-inductively defined types in depen-dent type theory [70, 71], categorical combinators [63, 290, 116], categorical normalisation proofs [147, 238, 5], fixed points [16], rewriting and 2-categorical] structure [308, 278], modal logic [93], μ-calculi [313], synthetic domain theory [144, 331, 264], a fibred Giraud theorem [229], a fibred adjoint functor theorem [47, 246], descent theory [168] (especially with its links to Beth de-finability [208]], fbrations in bi-categories [315, 317], 2-fbrations [127], and the theory of stacks [100].

The choice has been made to present details of interpretation functions for simple type theory in full detail in Chapter 2, together with the equivalent functorial interpretation. In later chapters interpretations will occur mostly in the more convenient functorial form. For detailed information about interpre-tation functions in polymorphic and (higher order) dependent type theories we refer to [319, 61]. As we proceed we will be increasingly blurring the distinction between certain type theories and certain fibred categories, thus decreasing the need for explicit interpretations

** 0.2 The logic and type theory of sets
:PROPERTIES:
:CUSTOM_ID: clog002
:END:

This section is manually partitioned by editor (me). This section is highly focused on the fibration over sets and family of sets using the language of adjoint. If you don't know about adjoint, don't worry---me neither.

*** Describing a fibred category: Pred

We shall now try to make the fibred perspective more concrete by describing the (familiar) logic and type theory of ordinary sets in fibred form. Therefore we shall use the fibrations of predicates over sets and of families of sets over sets, without assuming knowledge of what precisely constitutes a fibration. In a well-known situation we thus describe some of the structures that will be investigated in more abstract form in the course of this book. We shall write \(\Sets\) for the category of (small) sets and ordinary functions between them

Predicates on sets can be organised in a category, that will be called \(\mathbf{Pred}\), as follows.

\textbf{Objects}: pairs \(I, X\) where \(X \subset I\) is a subset of a set \(I\); in this situation we consider \(X\) as a predicate on a type \(I\), write \(X(i)\) for \(i \in X\) emphasise that an element \(i \in I\) may be understood as a free variable in \(X\). When \(I\) is clear from the context, we sometimes write \(X\) for the object \(X \subset I\).

\textbf{Morphisms}: \((I, X)\to (J, Y)\) are functions \(u\colon I \to J\) between the underlying sets satisfying

\[X(i) \Rightarrow Y(u (i)), \quad \text{for each } i \in I\]

Diagrammatically, this condition on such a function \(u\colon I \to J\) amounts to the existence of a necessarily unique (dashed) map:

\[
\begin{tikzcd}
  X \arrow[rr, dashed]
    \arrow[d, hook]    && Y \arrow[d, hook] \\
  I \arrow[rr, "u"]    && J
\end{tikzcd}\]

indicating that \(u\) restricts appropriately.

There is an obvious forgetful functor \(\mathbf{Pred}\to \mathbf{Sets}\) sending a predicate to its underlying set (or type): \((I, X)\mapsto I\). This functor is a "fibration". And although it plays a crucial role in this situation, we do not give it a name, but simply write it vertically as \(\begin{gathered} \scriptstyle\mathbf{Pred}\\[-7pt] \scriptstyle\downarrow\\[-7pt] \scriptstyle\mathbf{Sets} \end{gathered}\) to emphasise that it describes predicates as living over sets. (ps. from one: Is this fibration? Isn't that fibration need that a \(\Hom\) should be an object in \(\B\)? Anyway, here the fibration is presented as a forgetful functor)

For a specific set \(I\), the “fibre” category \(\mathbf{Pred}_{I}\) is defined as the subcategory of \(\mathbf{Pred}\) of predicates \((X\subset I)\) on \(I\) and of morphisms that are mapped to the identity function on \(I\). This category \(\mathbf{Pred}_{I}\) may be identified with the poset category \(\langle P(I),\subset\rangle\) of subsets of \(I\), ordered by inclusion. For a function \(u\colon I \to J\) there is “substitution” functor \(u^{*}\colon P(J)\to P(I)\) in the reverse direction, by

\[(Y\subset J)\mapsto (\{i \mid u (i)\in Y\}\subset I)\]

Clearly we have \(Y\subset Y' \Rightarrow u^{ * }(Y)\subset u^{ * }(Y')\), so that \(u^{*}\) is indeed a functor.

ps. Here we need to draw a commutative diagram for you to show the relation between \(u\) and \(u^{ * }\):

\[\begin{tikzcd} \C _{I} & \C_{J} \arrow[l, "u^{ * }"] \\ I \arrow[r, "u"] & J \end{tikzcd}\]
In the context, \(\C _{I}\) should be \(\Pred _{I}\). And for example, \(P(I)\) means the power sets of \(I\).

\[\begin{tikzcd} P(I) & P(J) \arrow[l, "u^{ * }"] \\ I \arrow[r, "u"] & J \end{tikzcd}\]

*** Weakening and contraction

Two special cases of substitution are _weakening_ and _contraction_. Weakening is substitution along a Cartesian projection \(\pi \colon I \times J \to I\). It consists of a functor

\[P(I)\longrightarrow_{\pi^{ * }} P(I\times J)\qquad \text{sending}\qquad X\mapsto \{(i,j)\mid i \in X \land j \in J\}\]

by adding a dummy variable \(j \in J\) to a predicate \(X\). Contraction is substitution along a Cartesian diagonal \(\delta \colon I \to I \times I\). It is a functor

\[P(I\times I) \longrightarrow _{\delta ^{ * }} P(I), \quad \text{given by}\quad Y \mapsto \{i\in I \mid (i, i )\in Y\}\]

It replaces two variables of type \(I\) by a single variable.

ps. The (right handside) weakening rule and contraction rule in (classic) sequent calculus are respectively
\[\prftree[r]{$(\vdash W)$}
          {\Gamma \vdash \Delta}
          {\Gamma \vdash A, \Delta},\quad
  \prftree[r]{$(\vdash C)$}
          {\Gamma \vdash A, A, \Delta}
          {\Gamma \vdash A, \Delta}\]

*** Forall and Exists

Each fibre category \(P(I)\) is a *Boolean algebra*, with the usual set theoretic operations of intersection \(\cap\), top element \((I \subset I)\), union \(\cup\), bottom element \((\emptyset \subset I)\), and complement \(I\, \char`\\ (-)\) These operations correspond to the propositional connectives \(\land, \top, \lor, \bot, \neg\) in (Boolean) logic. They are *preserved* by substitution functors \(u^{ * }\) between fibre categories.

The categorical description of the quantifiers \(\exists\), \(\forall\) is less standard (than the propositional structure of subsets). These quantifiers are given by operations between the fibres—and not inside the fibres, like the propositional connectives—since they bind free variables in predicates (and thus change the underlying types). They turn out to be *adjoints* to weakening, as expressed by the fundamental formula:

\[\exists \dashv \pi ^{*} \dashv \forall.\]
(ps. remember \(\pi ^{ * }\) is weakening substitution, and \(\pi\) is projection \(I \times J \to I\). And \(\dashv\) is notation for adjoint.)

In more detail, we define for a predicate \(T \subset I \times J\),
(ps. remember a substitution \(u^{ * }\) is respective to a function \(u \colon X \to Y\))

\[\exists ( Y) = \{ i\in I\mid \exists j \in J.(i,j) \in Y \}\]
\[\forall(Y) = \{ i \in I \mid \forall j \in J. (i, j )\in Y \}\]

These assignments \(Y\mapsto \exists(Y)\) and \(Y\mapsto \forall(Y)\) are functorial \(P(I \times J) \rightrightarrows P(I)\). And they are left and right adjoints to the above weakening functor \(\pi ^{ * }\colon P(I) \to P(I\times J)\) because there are the following basic adjoint correspondences.

\[\frac{Y \subset \pi ^{ * }(X) \quad \mathrm{over} \quad I \times J}{\exists (Y)\subset X \quad \mathrm{over} \quad I},\quad \text{and}\quad \frac{\pi^{ * }(X) \subset Y \quad \mathrm{over}\quad I \times J}{X \subset \forall (Y) \quad \mathrm{over} \quad I}\]
(ps. here the line should be double line but I don't know how to typeset that, and double line means if and only if. Because of that a \(\Hom\) in poset category is presented as a relation, the original definition of adjoint that "there is a one-one correspondence between \(\Hom(FX , Y)\) and \(\Hom( X, GY)\)" becomes "if and only if". For the definition of adjointing see the preliminary of this book.)

\[
\begin{tikzcd}
  \Pred_{I \times J}
  \arrow[rr, bend left, "\exists"]
  \arrow[rr, bend right, "\forall"]
    && \Pred_{I}
       \arrow[ll, "\pi ^{*}"]\\
  I \times J \arrow[rr, "\pi"]
    && I
\end{tikzcd}\]

*** Equality Functor

For a set (or type) \(I\), euqality \(i = i '\) for elements \(i, i' \in I\) forms a predicate on \(I \times I\). Such equality can also be captured categorially, namely as left adjoint to the contraction functor \(\delta ^{ * }\colon P(I \times I )\to P(I)\) . One defines for a predciate \(X \subset I \) the predicate \(\mathrm{Eq}(X)\) on \(I\times I\) by

\[\mathrm{Eq}(X)= \{ (i , i') \in I\times I \mid i = i ', i \in X \}\]
(ps. It seems that equality can be captured well with relation. Why then did Girard in his book say that "equality" is something that is always in-perfect?)

Then there are adjoint correspondence

\[\frac{\mathrm{Eq}(X) \subset Y \quad \mathrm{over}\quad I \times I}{X \subset \delta ^{ * } (Y) \quad \mathrm{\over} \quad I}\]
(ps. again it should be double line here.)

Notice that the predicate \(\mathrm{Eq}(X)\) is equality on \(I\) for the special case where \(X\) is the top element \(I\). See also Exercise 0.2.2 below for a description of a right adjoint to contraction, in terms of inequality.

The operations of predicate logic can thus be identified as certain structure in this fibration \(\begin{gathered} \scriptstyle \mathbf{Pred}\\[-7pt] \scriptstyle \downarrow \\[-7pt] \scriptstyle \mathbf{Sets} \end{gathered}\), namely as structure in and between its fibres. Moreover, it is a property of the fibration that this logical structure exists, since it can be characterised in a universal way --- via adjoints --- and is thus given uniquely up-to-isomorphism. The same holds for the other logical and type theoratical operations that we identify below.\\
(ps. unique up-to-isomorphism means that there is only one such object, and if there are others, they are isomorphic. This term is used widely when talking about universal properties.)\\
(ps. What is “fibration” in your opinion now?)

*** Comprehension: The adjointing between Pred and Sets

Comprehension is the assignment of a set to a predicate, or, as we shall say more generally later on, of a type to a predicate.
This assignment takes a prediate to the set of elements for which the predicate holds. It also has a universal property. Thererfore we first need the "truth" functor \(1 \colon \mathbf{Sets}\to \mathbf{Pred}\), which assigns to a set \(I\) the truth predicate \(1(I) = (I \subset I)\) on \(I\); it is the terminal object in the fibre over \(I\) (ps. Can you write out the definition of Fibration Category?). Comprehension (or subset types, as we shall also say) is then given by a functor \(\{ - \}\colon \mathbf{Pred}\to \mathbf{Sets}\), namely

\[\{(Y \subset J)\} = \{ j \in J \mid Y(j) \} = Y.\]

Hence \(\{ - \}\colon \mathbf{Pred}\to\mathbf{Sets}\) is simply \(( Y \subset J) \mapsto Y\). It is right adjoint to the truth functor \(1\colon \mathbf{Sets} \to \mathbf{Pred}\) since there is a bijective correspondence between functions \(u\) and \(v\) in a situation:

\[\frac{1 (I) \longrightarrow _{u} (Y \subset J)\quad \mathrm{in}\ \mathbf{Pred}}{I \longrightarrow _{v} \{ (Y\subset J) \}\quad \mathrm{in}\ \mathbf{Sets}}\]
(ps. Here also it should be double line. And also, \(u\) and \(v\) are morphism from \(\Hom(1 (I) , (Y \subset J))\) and \(\Hom(I , \{ (Y \subset J)\})\). The correspondence here means the two homsets are isomorphic. So \(u \in \Hom((I \subset I ) , (Y \subset J))\), and \(v \in \Hom (I , Y)\). Remember that \(u\colon X \subset I \to Y \subset J\) is such function \(u\) that if \(i \in X\) then \(u(i) \in Y\). For \(u\) when \(X \equiv I\), such \(u\) is absolutely identical to function \(I \to Y\), where \(Y \subset J\))

In essence this correspondence tells us that \(Y(j)\) holds if and only if \(j \in \{ (Y \subset J)\}\).
(ps. the purpose is that we formulate \(\{-\}\) as a functor, and also use it as an example.)

*** Quotient and Relation

Quotient sets can also be described using the firbation of predicates over sets. We first form the category \(\mathbf{Rel}\) of (binary) relations on sets by pullback.

\[
\begin{tikzcd}
  \mathbf{Rel} \arrow[rr] \arrow[d] \arrow[drr, phantom, "\lrcorner", very near start]
    && \mathbf{Pred} \arrow[d]\\
  \mathbf{Sets} \arrow[rr, "I \mapsto I \times I"]
    && \mathbf{Sets}
\end{tikzcd}\]
(the pullback is hard to draw)

Via this pullback we restrict ourselves to predicates with underlying sets of the form \(I \times I\). Explicitly, the category \(\mathbf{Rel}\) has

\textbf{Objects}: pairs \((I, R)\) where \(R \subset I \times I\) is a (binary) relation on \(I \in \mathbf{Sets}\). \\
(ps. We can write \(R\) for an object in \(\mathbf{Rel}\) if the context is clear.)

\textbf{Morphisms}: \((I, R)\to (J , S)\) are functions \(u\colon I\to J\) between the underlying sets with the proeperty: \(R(i, i')\) implies \(S(u(i), u (i'))\), for all \(i, i' \in I\).

The functor \(\mathbf{Rel}\to \mathbf{Sets}\) in the diagram is then \((I, R) \mapsto I\). It will turn out to be a fibration by construction. The above-mentioned equality predicate yields an equality functor \(\mathrm{Eq}\colon \mathbf{Sets}\to \mathbf{Rel}\) , namely

\[J \mapsto \mathrm{Eq}(J) = \{ (j, j) \mid j \in J\}.\]
(ps. Read the previous section if you forget what \(\mathrm{Eq}\) is.)

Quotients in set theory can then be described in terms of a left adjoint \(\cal Q\) to this equality functor \(\mathrm{Eq}\): a relation \(R \subset I \times I\) is mapped to the quotient set \(I / \bar R\) where \(\bar R \subset I \times I \) is the least equivalence relation containing \(R\). Indeed there is an adjoint correspondence between functions \(v\) and \(u\) in:

\[\frac{ \mathcal Q (I , R) = I / \bar R \longrightarrow _{v} J \quad \mathrm{in}\ \mathbf{Sets}}{R\longrightarrow _{u} \mathrm{Eq}(J)\quad \mathrm{in}\ \mathbf{Rel}}\]

This correspondence can be reformulated as: for each function \(u \colon I \to J\) with \(u (i) = u (i')\) for all \(i, i ' \in I\) for withc \(R(i, i')\) holds, there is a unique function \(v\colon I / \bar R \to J\) in a commuting triangle

\[\begin{tikzcd}
  I \arrow[rr, "\mathrm{quotient}"] \arrow [drr, "u"]
    && I / \bar R \arrow[d, dashed, "v"]  \\
    && J
\end{tikzcd}\]

*** Fibration of Fam(Sets) over Sets

Finally we mention that predicates over sets give us higher order logic. There is a distinguished set \(2 = \{ 0,1 \}\) of propositions, with special predicate \( (\{1\} \subset 2 )\) for truth: for every preciate \((X \subset I)\) on a set \(I\), there is a unique function char\((X \subset I)\colon I \to 2\) with

\[(X \subset I ) = \mathrm{char} (X \subset I )^{ * }(\{1\} \subset 2 ).\]

This existence of "characteristic morphisms" (ps. \textit{i.e.} \(\mathrm{char}\)) is what makes the category of sets a topos. It allows us to quantify via this set \(2\) over propostions.

This completes our first glance at the fibred structure of the logic of sets. In the remainder of this section we sketch some of the type theoretic structure of sets, agian in terms of fibration, namely interms of the "family" fibration \(\Fibre{\Fam(\Sets)}{\Sets}\) of set-indexed-sets. It captures the dependent type theory (with type-indexed-types) of sets.

The category \(\Fam(\Sets)\) of families of sets has:

\textbf{Objects}: pairs \((I, X)\) consisting of an index set \(I\) and a family \(X = (X _{i}) _{i \in I}\) of \(I\)-indexed sets \(X_{i}\).

\textbf{Morphisms}: \((I, X) \to (J, Y)\) are pairs \((u, f)\) consisting of functions

\[I \longrightarrow _{u} J \quad \mathrm{and} \quad f = (X_{i} \longrightarrow _{f_{i}} Y_{u(i)}) _{i \in I}\]
(ps. it is better written as a set function \(u \colon I \to J\), and a family of morphisms, indexed by \(I\), \(f = (f_{i})_{i \in I}\), where \(f_{i}\colon X_{i} \to Y_{u(i)}\))

There is a projection functor \(\Fam(\Sets) \to \Sets\) sending an indexed family to its underlying set index set: \((I , X) \mapsto I\). It will turn out to be a fibration. Essentially this will mean that there are (appropriate) substitution or reindexing functors: for a function \(u\colon I \to J\) between index sets, we can map a family \(Y = (Y_{j})_{j \in J}\) over \(J\) to a family over \(I\) via:

\[(Y _{j})_{j \in J} \mapsto (Y_{u(i)})_{i\in I}\]

We shall write \(u^{ * }\) for this operation. It extends to a functor between "fibre" categories: for an arbitrary set \(K\), let \(\Fam(\Sets)_{K}\) be the "fibre" subcategory of \(\Fam(\Sets)\) of those families \((K ,X)\) with \(K\) as index set, and with morphisms \((\mathrm{id}_{K}, K)\) with the identity on \(K\) as underlying function. Then \(u\colon I \to J\) yields a substitution functor \(u ^{ * } \colon \Fam(\Sets)_{J}\to \Fam(\Sets)_{I}\).

Notice that there is an inclusion functor \(\Pred \to \Fam(\Sets)\) (it should be inclusion \(\to\) with hook) of predicates into families, since every predicate \((X \subset I)\) yields an \(I\)-indexed family \((X_{i})_{i \in I}\) with

\[X_{i} = \begin{cases} \{ * \},   & i \in X\\ \emptyset, & \mathrm{otherwise} \end{cases}\]

It is not hard to see that this yields a full and faithful functor \(\Pred \to \Fam(\Sets)\), which commutes with substitution. It is a 'morphism of fibrations'. \\
(ps. I don't really know what a faithful functor is.)

Our aim is to describe the dependent coproduct \(\coprod\) and product \(\prod\) of families of sets as adjoints to weakening functors, in analogy with the situation for existential \(\exists\) and universal \(\forall\) quantification in the logic of sets. But in this situation of families of sets we have weakening functors \(\pi ^{ * }\) induced not by Cartesian projections \(\pi\): \(I \times J \to I\), but by "dependent" projections \(\pi \colon \) \(\{ I \mid X \} \to I\), with domain \(\{ I \mid X\}\) given by the disjoint union

\[\{ I \mid X \} = \{ (i, x ) \mid i \in I , x \in X_{i}\}\]

which generalises the Cartesian product. The weakening functor \(\pi ^{ * }\) associated with this dependent projection \(\pi \colon \) \(\{ I \mid X\} \to I\) sends a family \(Y = (Y_{i})_{i \in I}\) over \(I\) to a family \(\pi ^{ * }(Y)\) over \(\{ I \mid X \}\) by vacuously adding an extra index \(x\), as in:

\[\pi ^{ * }(Y) = (Y_{i}) _{(i\in I , x \in X_{i})}.\]

(As we shall see later, the projection \(\pi\colon \{ I \mid X\} \to I
\) arises in a canonical way, since the assignment \((I, X)\mapsto \) \(\{ I \mid X\}\) yields a functor \(\mathrm{Fam}(\mathbf{Sets})\) \(\to\) \(\mathbf{Sets}\), which is right adjoint to the terminal object functor \(1\colon \mathbf{Sets} \to \mathrm{Fam}(\mathbf{Sets})\), sending a set \(J\) to the \(J\)-indexed collection \((\{ * \})_{j \in J}\) of singletons. The count of this adjunction has the projection \(\pi\) as underlying map. Thus, the operation \((I,X)\)\(\mapsto \{ I \mid X\}\) is like comprehension for predicates, as described above.)

(ps. Have you read the chapter 8: Coherent Space in \textit{The Blind Spot}, where a \textit{Directed System} in category \(\C\) is defined as a category where all the objects are indexed by directed partially ordered set \(I\)? If we use the notation here, it should be \(\Fam(\C)_{I}\) where \(I\) is directed.)

*** Coproduct and Product in Fam(Sets)

The claim is that the dependent coproduct \(\coprod\) and product \(\prod\) for set-indexed sets are left and right adjoints to the weakening functor \(\pi^{ * }\) . Therefore we have to define coproduct and product as functors

\[
\begin{tikzcd}
  \mathrm{Fam}(\mathbf{Sets}) _{\{I \mid X\}}
  \arrow[rr, bend left, "\coprod"]
  \arrow[rr, bend right, "\prod"]
    && \mathrm{Fam}(\mathbf{Sets})_{I}
    \arrow[ll, "\pi ^{*}"]\\
  \{I \mid X\} \arrow[rr, "\pi"]
    && I
\end{tikzcd}\]


acting on an \(\{I \mid X\}\) indexed family \(Z = (Z _{(i , x)}) _{i \in I , x \in X}\), and producing an indexed family. These functors are given by

\[\coprod (Z)_{i} = \{ (x , z) \mid x \in X_{i} , z \in Z_{(i , x)} \}\]
\[\prod (Z)_{i} = \{ \varphi \colon X_{i} \to \bigcup _{x \in X _{i}} (Z_{(i,x)}) \mid \forall x \in X_{i}, \varphi (x) \in Z_{(i , x)}\}.\]

We then get the fundamental relation

\[\coprod \dashv \pi ^{ * } \dashv \prod\]

Since there are bijective adjoint correspondences between families of functions \(f\) and \(g\) in:

\[\frac{Z \longrightarrow_{f} \pi ^{*} (Y)\quad \mathrm{over}\ \{ I \mid X\}}{ \coprod (Z) \longrightarrow _{g} Y \quad \mathrm{over}\ I}\]

\[\frac{\pi ^{ * } (Y) \longrightarrow _{f} Z \quad \mathrm{over}\ \{ I \mid X \}}{Y \longrightarrow _{g} \prod (X)\quad \mathrm{over}\ I}\]

Also in this situation, there are adjoints to contraction functors \(S^{ * }\) (induced by dependent diagonals), given by equality and inequality. But we do not further pursue this matter, and conclude our introduction at this point.

*** Conclusion

What we have sketched is that families of sets behave like dependent types, and that subsets behave like predicates, yielding a logic over (dependent) type theory. We have shown that the basic operations of this logic and of this type theory can be described by adjunctions, in a fibred setting. In the course of this book we shall (among many other things) be more precise about what it means to have such a logic over a type theory and we shall axiomatise all of the structure found above, and identify it in many other situations.

Finally, the next few exercises may help the reader to become more familiar with the structure described above.

** 0.2 Exercises
*** 0.2.1

Define a left adjoint \(F\colon \Fam (\Sets) \to \Pred\) to the inclusion functor

\[
\begin{tikzcd}
  \Pred \arrow[rr, hook] \arrow[rd]
    && \Fam(\Sets) \arrow[ll, bend right, "F"] \arrow[ld] \\
    & \Sets
\end{tikzcd}\]
Such that (1) \(F\) makes the triangle commute (so it does not change t he index set), and (2) \(F\) commutes with substitution.

*** 0.2.2 not equal

Define for a subset \(X \subset J\) the relation \(\mathrm{nEq}(X) \subset I \times I\) by

\[\mathrm{nEq}(X) = \{(i, i') \mid i \ne i', \text{ or } i \in X\}\]

and show that the assignment \(X \mapsto \mathrm{nEq}(X)\) is right adjoint to contraction \(\delta ^{ * } \colon P ( I \times I) \to P(I)\). Notice that \(\mathrm{nEq}(X)\) at the bottom element \(X = \emptyset\) is inequality on \(I\).

*** 0.2.3: the right adjoint of Eq

Show that the equality functor \(\mathrm{Eq}\colon \Sets \to \mathbf{Rel}\) also has a right adjoint.

*** 0.2.4 Comprehension

Check that the operation \((I , X) \mapsto \{I \mid X\}\) yields a functor \(\Fam(\Sets)\to \Sets\), and show that it is right adjoint to the terminal object functor \(\Sets \to \Fam(\Sets)\), mapping a set \(J\) to the family of singletons \((\{ * \})_{j \in J}\). Describe the unit and counit of the adjunction explicitly.

** 1 Introduction to fibred category theory

This first proper chapter starts with the basics of fibred category theory; it provides the foundation for much of the rest of this book. A fibration, or fibred category, is designed to capture collections \((\C _{I})_{I\in \B}\) of categories \(\C_{I}\) varying over a base category \(\B\), generalising for example collections of sets \((X_{i})_{i\in I}\) varying over a base, or index, set \(I\). The main categorical examples are the indexed collections of categories

\[(\B / I)_{I \in \B}, \quad (\text{Sub} (I))_{I\in \B}, \quad (\B  /\kern-3pt / I )_{I\in \B}\]

consisting of slice categories \(\B / I\) over, posets \(\text{Sub}(I)\) of subobjects of \(I\), and what we call 'simple slice categories' \(\B \DoubleSlash I\) over \(I\) The ordinary slice categories will be used for dependent type theory, the posets of subobjects for predicate logic, and the simple slice categories for simple type theory (whence the name). The slice categories \(\B / I\) will be used as leading example in the first section when we introduce fibrations. The other examples \(\mathrm{Sub}(I)\) and \(\B \DoubleSlash I\)will be introduced soon afterwards, in Section 1.3.

In all of these cases, a morphism \(u\colon I \to J\) in the base category \(\B\) induces a substitution functor, commonly written as \(u^{*}\), acting in the reverse direction. That is, there are substitution functors:

\[\B / J \to_{u^{ * }} \B / I,\quad \mathrm{Sub}(J)\to_{u^{ * }} \mathrm{Sub}(I),\quad \B \DoubleSlash J \to_{u^{ * }} \B \DoubleSlash I\]

Weakening functors and contraction functors arise as special cases of substitution functors \(u^{*}\), namely (respectively) as \(\pi^{ * }\), where \(\pi\) is a projection morphism in \(\B\), and as \(\delta^{ * }\) where \(\delta\) is a diagonal morphism in \(\B\).

These two aspects—indexing and substitution—will be studied systematically in this first chapter, in terms of fibrations. The notion of'fibred category', or 'fibration', is due to Grothendieck [107].

This chapter develops the basic theory of fibrations and shows how various notions from ordinary category theory—such as adjunctions, products and coproducts—make sense for fibred categories as well. In the last section 1.10 we describe the notion of 'indexed category', a common alternative formulation of variable category, and explain why an indexed category should be regarded as simply a particular kind of fibrations (namely as a 'cloven' one). Chapter 7 describes internal categories, which also correspond to certain fibrations, namely to so-called 'small' fibrations.

The ten sections which together form this chapter contain the essentially standard, first part of the theory of fibrations, geared towards use in categorical logic and type theory. The main notions are: Cartesian morphism, substitution functor, change-of-base, fibred adjunction, fibred (co)product and indexed category. These will be introduced together with many examples. Sometimes the theory is further developed in exercises, but mostly, the exercises of a section serve to familiarise the reader with the new material in that section. There is a later chapter (Chapter 9) which continues the study of fibrations.

(ps. [107] A. Grothendieck. Categories fibrees et descente (Expose VI). In A. Grothendieck, editor, Revetement Etales et Groupe Fondamental (SGA 1), number 224 in Lect. Notes Math., pages 145-194. Springer, Berlin, 1970.)

** 1.1 Fibrations

Basically, a fibration is a categorical structure which captures \textit{indexing} and substitution. Since the formal definition of a fibration is a bit technical—see Definition 1.1.3 below—we start with the following introductory observations. These focus on the special case of a codomain fibration, and will lead to the general definition of fibration towards the end of this section. The exercises contain many elementary results about fibrations, which should help the reader to get acquainted with the concepts involved.

* COMMENT text1 :noexport:
:PROPERTIES:
:ARCHIVE_TIME: 2025-01-01 Wed 22:06
:ARCHIVE_FILE: ~/org/note.org
:ARCHIVE_CATEGORY: note
:END:

** prelude

In Geometry as well as in Physics, one has often to use the tools of differential and integral calculus on topological spaces which are locally like open subsets of the Euclidean space \(\mathbb{R}^n\), but do not admit coordinates valid everywhere. For example, the sphere \(\{(x, y, z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}\), or more generally, the space \(\{(x_1, \ldots, x_n) \in \mathbb{R}^n : \sum x_i^2 = 1\}\) are clearly of geometric interest.

On the other hand, constrained motion has to do with dynamics on surfaces in \(\mathbb{R}^3\). In general relativity, one studies 'space-time' which combines the space on which motion takes place and the time parameter in one abstract space and allows reformulation of problems of Physics in terms of a 4-dimensional object. All these necessitate a framework in which one can work with the tools of analysis, like differentiation, integration, differential equations and the like, on fairly abstract objects.

This would enable one to study Differential Geometry in its appropriate setting on the one hand, and to state mathematically the equations of Physics in the required generality, on the other. The basic objects which accomplish this are called differential manifolds. These are geometric objects which are locally like domains in the Euclidean space, so that the classical machinery of calculus, available in \(\mathbb{R}^n\), can be transferred, first, to small open sets and then patched together. The main tool in the patching up is the notion of a sheaf. We will first define sheaves and discuss about basic notions related to them, before taking up differential manifolds.

** 1. Sheaves and Presheaves

At the outset, it is clear that functions of interest to us belong to a class such as continuous functions, infinitely differentiable (or \( C^\infty \)) functions, real analytic functions, holomorphic functions of complex variables, and so on. Some properties common to all these classes of functions are that

  (i) they are all continuous, and

  (ii) the condition for a continuous function to belong to the class is of a local nature.

By this we mean that for a continuous function to be differentiable, real analytic or holomorphic, it is necessary and sufficient for it to be so in the neighbourhood of every point in its domain of definition. We will start with an axiomatisation of the local nature of the classes of functions we seek to study.


Here's the text from the image:

** 1.1. Definition: presheaf

Let \( X \) be a topological space. An assignment to every open subset \( U \) of \( X \), of a set \( \mathcal{F}(U) \) and to every pair of open sets \( U, V \) with \( V \subseteq U \), of a map (to be called restriction map) \( \text{res}_{UV} : \mathcal{F}(U) \to \mathcal{F}(V) \) satisfying:
\[ \text{res}_{VW} \circ \text{res}_{UV} = \text{res}_{UW} \]
for every triple \( W \subseteq V \subseteq U \) of open sets, is called a presheaf of sets. If \( \mathcal{F}(U) \) are all abelian groups, rings, vector spaces, etc., and the restriction maps are homomorphisms of the respective structures, then we say that \( \mathcal{F} \) is a presheaf of abelian groups, rings, vector spaces, etc.

\[\begin{tikzcd}
  & U\ar[rr, dashed] & & \mathcal F(U)\ar[d]\ar[ddl]\\
  & V\ar[u]\ar[rr, dashed] & & \mathcal F(V)\ar[dl] \\
  W\ar[ur]\ar[uur]\ar[rr, dashed] & & \mathcal F(W)
\end{tikzcd}\]

(supplement) alternative description (because the author's English is kinda tricky): *Presheaf*:

A presheaf on a topological space \( X \) is a collection of sets (or groups, rings, etc.) \( \mathcal{F}(U) \) for each open set \( U \subseteq X \), along with restriction maps \( \text{res}_{UV}: \mathcal{F}(U) \to \mathcal{F}(V) \) for \( V \subseteq U \), satisfying:

   - \( \text{res}_{UU} = \text{id}_{\mathcal{F}(U)} \),
   - \( \text{res}_{VW} \circ \text{res}_{UV} = \text{res}_{UW} \) for \( W \subseteq V \subseteq U \).

** 1.2. Definition: sheaf

A presheaf is said to be a sheaf if it satisfies the following additional conditions. Let \( U = \bigcup_{i \in I} U_i \) be any open covering of an open set \( U \). Then:

*S1*: Two elements \( s, t \in \mathcal{F}(U) \) are equal if \( \text{res}_{UU_i} s = \text{res}_{UU_i} t \) for all \( i \in I \).

*S2*: If \( s_i \in \mathcal{F}(U_i) \) satisfy \( \text{res}_{U_{i}U_i \cap U_j} s_i = \text{res}_{U_{j}U_i \cap U_j} s_j \) for all \( i, j \in I \), then there exists an element \( s \in \mathcal{F}(U) \) with \( \text{res}_{UU_i} s = s_i \) for all \( i \). We will also assume that \( \mathcal{F}(\emptyset) \) consists of a single point.

If there is \(\{s_{i}\in \mathcal F(U_{i})\}\), satisfy \(\text{res}_{U_{i}U_{i}\cap U_{j}}s_{i} = \text{res}_{U_{j}U_{i}\cap U_{j}} s_{j}\) for all \(i,j \in I\), then there is an element in \(s \in \mathcal F (U)\) that \(\mathrm{res}_{UU_{i}}s = s_{i}\) for all \(i\).

(supplement from editor) The description for two conditions:

- *Locality*: If \( s \in \mathcal{F}(U) \) and \( s \) restricts to the zero element on each \( U_i \cap U \), then \( s = 0 \).
- *Gluing*: If \( s_i \in \mathcal{F}(U_i), s_{j} \in \mathcal{F}(U_{j}) \) satisfy \( \text{res}_{U_{i}U_i \cap U_j} s_i = \text{res}_{U_{j}U_i \cap U_j} s_j \) for all \( i, j \), then there exists a unique \( s \in \mathcal{F}(U) \) with \( \text{res}_{UU_i} s = s_i \) for all \( i \).

Intuitions:

- *Locality* ensures that sections are determined by their local behavior.
- *Gluing* ensures that compatible local sections can be "glued" together to
  form a global section.

Gluing allows us to construct global objects from local data. For example, in a topological space, we can define functions or sections on open sets and then "glue" them together to define a function on the entire space.

** 1.3. Examples

*** 1.3.1. sheaf of Differential Functions

As indicated above, the concepts of differentiability, real analyticity, ... are all local in nature, so that it is no surprise that if \(X\) is an open subspace of \(\mathbb{R}^n\), then the assignment to every open subset \(U\) of \(X\), of the set \(\mathcal{A}(U)\) of differentiable functions with the natural restriction of functions as restriction maps, gives rise to a sheaf.

This sheaf will be called the sheaf of differentiable functions on \(X\). Obviously this sheaf is not merely a sheaf of sets, but of \(\R\) algebras.

*** 1.3.2. bounded functions

The assignment of the set of bounded functions to every open set \( U \) of \( X \), and the natural restriction maps define a presheaf on \( X \).
# of means target; to means source
# U is mapped to bounded functions

However, if we are given as in \( S_2 \) any compatible set of bounded functions \( s_i \), then while such data does define a unique function \( s \) on \( U \), there is no guarantee that it will be bounded. Thus this presheaf satisfies \( S_1 \) but not \( S_2 \) and so is not a sheaf. (Can one modify this in order to obtain a sheaf?)

** 1.4. Sheaf associated to a presheaf. germ and stalk

In the theory of holomorphic functions, one talks of a *germ* of a function at a point, when one wishes to study its properties in an (unspecified) neighbourhood of a point. This means the following.

  Consider pairs \((U, f)\) consisting of open sets \(U\) containing the given point \(x\) and holomorphic functions \(f\) defined on \(U\). Introduce an equivalence relation in this set by declaring two such pairs \((U, f)\), \((V, g)\) to be *equivalent* if \(f\) and \(g\) coincide in some neighbourhood of \(x\) which is contained in \(U \cap V\). An equivalence class is called a *germ*. This procedure can be imitated in the case of a presheaf and this leads to the concept of a *stalk* of a presheaf at a point.

** 1.5. Definition: stalk and germ

Let \(\cal F\) be a presheaf on a topo space \(X\). Then the stalk \(\mathcal F_{x}\) of \(\cal F\) at point \(x \in X\) is the quotient set of the set consisting of all \((U,s)\) where \(U\) is an open neightbourhood of \(x\) and \(s\) is an element of \(\mathcal F(U)\) under the *equaivalence relation*:

  \((U,s)\) is equivalent to \((V,t)\) if and only if there exists an open neighbourhood \(W\) of \(x\) contained in \(U \cap V\) (i.e. \(W \subset U \cap V\)) such that the restrictions of \(s\) and t to \(W\) are the same.

  If \(s \in \mathcal F(X)\) then, for any \(x \in X\), the pair \((X, s)\) has an image in the talk \(\mathcal F_{x}\), namely the quivalence class containing it. It is called the /germ of \(s\)/ at \(x\). We will denote it by \(s _{x}\).

(supplement from editor)
Let \( \mathcal{F} \) be a sheaf on a topological space \( X \), and let \( x \in X \) be a point. The *stalk* \( \mathcal{F}_x \) of \( \mathcal{F} \) at \( x \) is defined as:
\[
\mathcal{F}_x = \lim_{\longrightarrow} \mathcal{F}(U)
\]
where the direct limit is taken over all open neighborhoods \( U \) of \( x \), ordered by inclusion \( U_1 \subseteq U_2 \).

- In the sheaf of smooth functions on a manifold, the stalk at a point \( x \) consists of all germs of smooth functions defined in some neighborhood of \( x \).
- In the sheaf of holomorphic functions on a complex manifold, the stalk at a point \( x \) consists of all germs of holomorphic functions near \( x \).

** interlude: The topology on the set of all germs

Let \(E = E(\mathcal F)\) be the germs of all elements at all points of \(X\), that is to say, the disjoint union of all the stalks \(\mathcal F_{x}\) \(x \in X\). What we intend to do now is to provide the set \(E\) with a topology such taht

  - (a) the map \(\pi \colon E \to X\) which maps all of \(\mathcal F_{x}\) to \(x\), is continuous;
  - (b) if \(s \in \mathcal F(U)\) then the section \(\tilde s\) of \(E\) over \(U\) (i.e. \(\tilde s \colon U \to E\)), defined by setting \(\tilde s(x) = s_{x}\), is continuous.

  We achieve this by associating to each pair \((U,s)\) where \(U\) is an open subset of \(X\) and \(s \in \mathcal F(U)\), the set \(\tilde s (U)\), and defining a topology on \(E\) whose open sets are generated by sets of the form \(\tilde s (U)\). In order to check that with this topology, the maps \(\tilde s\) are continuous, we have only to show that \(\tilde s^{-1}(\tilde t(V))\) is open for every open subset \(V\) of \(X\) and \(t \in \mathcal F(V)\). This is equivalent to the following.

** 1.6. Lemma.

** 1.7. Definition: etale space

todo
